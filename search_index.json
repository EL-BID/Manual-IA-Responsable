[
["index.html", "IA Responsable (Ciclo de vida de IA) Manual Técnico Iniciativa fAIr LAC ¿Por qué este manual? ¿Para quién es este manual?", " IA Responsable (Ciclo de vida de IA) Manual Técnico Ilustración 0.1: Portada Temporal Iniciativa fAIr LAC El Banco Interamericano de Desarrollo (BID), en colaboración con socios y aliados estratégicos, lidera la iniciativa fAIr LAC mediante la cual se busca promover la adopción responsable de la Inteligencia Artificial (IA) y los sistemas de soporte de decisión para mejorar la prestación de servicios sociales y crear oportunidades de desarrollo en aras de atenuar la desigualdad social. Este manual es parte de un grupo de documentos y herramientas para equipos técnicos y responsables de la formulación de políticas públicas para guiarles en la mitigación de los retos de los sistemas de soporte de decisión y promover una adopción responsable de la IA (Cristina Pombo (2020)). ¿Por qué este manual? A pesar de que existe un número importante de principios que buscan una IA ética, solo proporcionan una orientación de alto nivel sobre lo que debe o no hacerse en su desarrollo y existe muy poca claridad sobre cuáles son las mejores prácticas para ponerlas en funcionamiento (Jobin, Ienca, and Vayena 2019). El objetivo de este manual es proveer esas recomendaciones y buenas prácticas técnicas con el fin de evitar resultados contrarios (muchas veces inesperados) a los objetivos de los tomadores de decisiones. Esos fines son variados: pueden referirse a consecuencias no deseables desde el punto de vista de los tomadores de decisiones, desaprovechamiento de recursos debido a focalizaciones inadecuadas o cualquier otro objetivo que el tomador de decisiones esté buscando lograr.1 ¿Para quién es este manual? Este manual está pensado para equipos técnicos trabajando en la aplicación de algoritmos de aprendizaje automático para políticas públicas. Sin embargo, todos los retos que cubre son comunes a cualquier aplicación de esta tecnología. Se asume que el lector cuenta con conocimientos básicos de estadística y programación, aunque cuando se nombran conceptos se incluyen descripciones breves y se comparte bibliografía adicional. El manual incluye cuadernillos de trabajo con varios ejemplos de los retos y soluciones explicadas. Se usan distintos tipos de modelos (lineales, basados en árboles y otros) y distintas implementaciones (R, Keras, Xgboost) para mostrar que estos problemas se presentan independientemente de la elección de herramientas particulares. Aunque los códigos y ejemplos se desarrollaron en R, todos los temas y metodologías aplicadas y descritas en este manual se pueden implementar en cualquier otro lenguaje de programación.2 Banco Interamericano de Desarrollo (BID) - Sector Social El Sector Social (SCL) está conformado por un equipo multidisciplinario que actúa bajo la convicción de que la inversión en las personas permite mejorar sus vidas y superar los desafíos del desarrollo en América Latina y el Caribe. Junto con los países de la región, el Sector Social formula soluciones de política pública para reducir la pobreza y mejorar la prestación de servicios de educación, trabajo, protección social y salud. El objetivo es construir una región más productiva donde predomine la igualdad de oportunidades para hombres y mujeres, y una mayor inclusión de los grupos más vulnerables. www. iadb.org/en/about-us/departments/scl Banco Interamericano de Desarrollo (BID) – BID Lab BID Lab es el laboratorio de innovación del Grupo BID. Allí se movilizan financiamiento, conocimiento y conexiones para catalizar la innovación orientada a la inclusión en América Latina y el Caribe. Para BID Lab, la innovación es una herramienta poderosa que puede transformar a la región creando oportunidades sin precedentes para las poblaciones en situación vulnerable por las condiciones económicas, sociales y ambientales en que se encuentran. https://bidlab.org/ Agradecimientos Por su tiempo y valiosos aportes, expresamos un agradecimiento especial a Cristina Pombo, coordinadora de la iniciativa fAIr LAC del BID y al Prof. Ricardo Baeza-Yates, Director de Ciencia de Datos en Northeastern University, Campus de Silicon Valley, e integrante del Grupo de Expertos y Expertas de fAIr LAC. Agradecemos igualmente el apoyo prestado y los comentarios recibidos de Luis Tejerina, Elena Arias Ortiz, Natalia González Alarcón, Tetsuro Narita, Constanza Gómez-Mont, Daniel Korn, Ulises Cortés, José Antonio Guridi Bustos, Cesar Rosales y Sofia Trejo. Este manual no pretende reglamentar o explicar cuáles deben ser los fines y objetivos de organismos y actores que toman las decisiones.↩ Todo el material de este documento es reproducible según instrucciones en el repositorio https://github.com/EL-BID/Manual-IA-Responsable, que contiene un archivo Dockerfile describiendo las dependencias de infraestructura para su replicación. Se utiliza el lenguaje de programación R, y los siguientes paquetes: tidyverse, recipes, themis, rsample, parsnip, yardstick, workflows, tune, knitr, patchwork.↩ "],
["introducción.html", "Introducción ML y sistemas de toma/soporte de decisiones Componentes de un sistema de IA para políticas públicas Retos del ciclo de vida del ML", " Introducción Los métodos de aprendizaje automático, como un subconjunto de lo que se conoce como inteligencia artificial (que para resumir en este documento llamamos ML, por sus siglas en inglés, Machine Learning) son cada vez más requeridos y utilizados por tomadores de decisiones para informar acciones o intervenciones en varios contextos, desde negocios hasta política pública. En la práctica, estos métodos se han utilizado con diversos grados de éxito, y con esto ha aparecido la preocupación creciente de cómo entender el desempeño e influencia positiva o negativa de estos métodos en la sociedad (Barocas and Selbst (2014)) (Suresh and Guttag (2019)). ML y sistemas de toma/soporte de decisiones La OCDE describe a los sistemas de soporte de decisión como un “sistema computacional que puede, para un determinado conjunto de objetivos definidos por humanos, hacer predicciones y recomendaciones o tomar decisiones que influyen en entornos reales o virtuales.” Y que están diseñados para operar con distintos niveles de autonomía (OECD (2019)). En este manual se pretende discutir los problemas más comunes en el uso del aprendizaje automático (ML) como parte de un sistema de toma/soporte de decisión, como detectar errores de implementación, sesgos, y evaluar la posibilidad de resultados indeseables para la sociedad, una compañía o institución particular. Aunque los métodos de aprendizaje automático no son el único tipo de algoritmos que pueden utilizar los sistemas de IA, sí son los que han tenido más crecimiento de los últimos años. Se trata de un conjunto de técnicas para permitir que un sistema aprenda comportamientos de manera automatizada a través de patrones e inferencias en lugar de instrucciones explícitas o simbólicas introducidas por un humano (OECD (2019)). Se consideran dos arquetipos de inclusión de aprendizaje automático en el proceso de toma de decisiones:3 Sistemas de soporte de decisión: Relacionado al concepto de inteligencia asistida o aumentada, se utiliza para describir los sistemas en donde la información generada por los modelos de aprendizaje automático se utiliza como insumo para la toma de decisiones por un humano. Sistemas de toma de decisión: Relacionada con el concepto de inteligencia automatizada y autónoma. Las decisiones finales y su consecuente acción se toman sin intervención humana directa. Es decir, el sistema pasa a realizar tareas previamente desarrolladas por un humano. En muchos contextos se usa ADM para denominar estos sistemas por su sigla en inglés: Automated Decision Making. Para el desarrollo de un sistema de toma/soporte de decisión exitoso basado en aprendizaje automático se debe considerar que existe una gran variedad de técnicas, conocimiento experto del tema y de modelación en general. En este manual no se pretende discutir métodos particulares de aprendizaje automático ni de procesos específicos de ajuste de hiper-parámetros, ver por ejemplo (Hastie, Tibshirani, and Friedman (2017)), (Kuhn and Johnson (2013)), (Gelman and Hill (2006)), sino concentrarse en su evaluación y en los retos más importantes que los sistemas comparten sin importar el tipo de algoritmo o tecnología utilizada. Por otra parte, la evaluación de un sistema de aprendizaje automatizado no tiene sentido fuera de su contexto, preguntas como: ¿cuál es una tasa de error apropiado? o ¿cuáles son sesgos poco aceptables?, entre otras, sólo pueden considerarse y ser respondidas dentro del contexto específico de su aplicación, de los propósitos y motivaciones de los tomadores de decisiones, así como por el riesgo que se presenta en los usuarios finales. Es decir, muchos de los criterios técnicos tienen que ser entendidos bajo la luz del problema específico, los sistemas de toma/soporte de decisión nunca son perfectos, pero si se conocen sus sesgos y sus limitaciones incluso un sistema con una precisión baja podría ser útil y ser utilizado responsablemente. En el caso contrario, el tener un sistema con métricas de evaluación altas no elimina el riesgo de un uso irresponsable si no se entienden sus limitaciones. Principio guía Este manual se concentra en el subconjunto de retos que están relacionados a procesos técnicos a lo largo del ciclo de vida de la IA como sistema de toma/soporte de decisión. Su objetivo es describir cómo distintos sesgos y deficiencias pueden ser causadas por los datos de entrenamiento, problemas y decisiones en el desarrollo del modelo, en el proceso de validación o monitoreo que pueden producir resultados indeseables e inequidad en la toma de decisiones. La evaluación sólo tiene sentido en términos del contexto de la decisión, y de los resultados que son deseables para los tomadores de decisiones, instituciones o compañías involucradas. Componentes de un sistema de IA para políticas públicas Ciclo de vida de la política pública con IA La IA no sustituye a la política pública ya que por sí misma no soluciona el problema social, su función es asistir proveyendo información para la toma o soporte de decisiones. El ciclo de política pública asistido por IA está compuesto por las siguientes etapas: Identificación del problema: Todo proyecto de IA debe iniciar identificando correctamente el problema social en el que la política pública busca impactar, identificando sus posibles causas y consecuencias. Formulación de intervención: Se formula la intervención o política que se está considerando aplicar a ciertas personas, unidades o procesos. Supondremos generalmente que se tiene evidencia del beneficio de esa política cuando se aplica a la población objetivo. Sistema de toma/soporte de decisión: Una vez definida la intervención, se inicia el ciclo de la IA con el diseño y desarrollo del sistema de toma/soporte de decisión, cuyo resultado será utilizado para focalizar u orientar la intervención elegida en el punto anterior.4 Implementación de política: Se implementa la política pública ya sea como proyecto piloto y/o con una escala mayor. Evaluación de política: Se evalúa la eficacia, la fiabilidad, el costo, las consecuencias previstas y no previstas y otras características pertinentes de la medida de política en cuestión, si sus resultados son positivos se escala o continúa la intervención. Ilustración 0.2: Ciclo de diseño de política pública. Construcción propia. Paralelamente al ciclo de política pública, el desarrollo de un sistema de IA tiene su propio ciclo que normalmente incluye las siguientes fases: i) Planeación y diseño, ii) recolección, conocimiento y preparación de datos, iii) modelado, iv) evaluación e implementación y vi) monitoreo. Estas fases suelen tener lugar de manera iterativa y no son necesariamente secuenciales. En la interrelación de estos dos ciclos se genera un importante grupo de retos para obtener una IA robusta y responsable que tienen que ser evaluados y considerados durante el desarrollo y uso de estos sistemas (Ilustración 0.3): Existen retos transversales (rendición de cuentas, gobernanza y seguridad) como son la transparencia, el consentimiento y la protección de datos personales; retos relacionados con el diseño de política pública, la definición de la intervención, así como criterios de necesidad y proporcionalidad en el uso de la IA; retos que se dan durante el ciclo de vida la IA, por sesgos en los datos, errores de implementación en el modelado, validación y monitoreo. . Ilustración 0.3: Retos de los sistemas de toma/soporte de decisión. Fuente: Cristina Pombo (2020) Uno de los conceptos más importantes para los retos del ciclo de vida de la IA es el de sesgos, ya que muchas de las medidas de mitigación y retos que se tienen que contemplar durante el desarrollo de los modelos depende de su correcto entendimiento y tratamiento. Para abordar tempranamente este problema es conveniente tener revisiones específicas en las distintas etapas del ciclo de vida. En cada revisión se debe invitar a los expertos y usuarios finales del sistema que corresponda, para verificar y defender las hipótesis realizadas durante cada etapa. Esto permite enriquecer los puntos de vista, encontrar suposiciones erradas y agregar aspectos no considerados. El error del sistema es la diferencia entre el valor predicho resultado del modelo y el valor real de la variable que se está estimando. Si el error es sistemático en una dirección o en un subconjunto específico de los datos, se llama sesgo5. Por ejemplo, si una balanza siempre pesa un kilo más, está sesgada; o si un valor es siempre menor, como el salario de las mujeres para un trabajo equivalente con respecto a los hombres, la variable salario está sesgada. Por otro lado, si el error es aleatorio, se llama ruido. El sesgo de un sistema de IA puede tener implicaciones éticas al utilizar sus resultados para tomar decisiones de política pública que lleven a acciones que puedan considerarse injustas o prejuiciosas para subgrupos de la población objetivo. Esta evaluación del sesgo está sujeta a una definición de justicia algorítmica específica que debe decidir el tomador de decisiones de política pública. Por ejemplo, en algunos casos el objetivo de un sistema puede estar orientado a satisfacer criterios de paridad demográfica, equidad de posibilidades, tener representatividad por cuotas, entre muchas otras. En algunas ocasiones el cumplimiento de una definición de justicia algorítmica6 imposibilita el cumplimiento de otra, es decir pueden ser parcial o totalmente excluyentes. Existen distintas fuentes de sesgo, algunos causados por problemas intrínsecos a los datos: Por ejemplo: Sesgos históricos o estados indeseable, cuando existen patrones en el mundo que no se quieren replicar o propagar en el modelo; sesgo de representación, se produce cuando existe información incompleta, ya sea por atributos faltantes, diseño de la muestra o ausencia total o parcial de subpoblaciones; Y sesgos de medición, que surgen por el uso u omisión de variables a ser utilizadas en los modelos. (Suresh and Guttag (2019)) Ilustración 0.4: Fuentes de sesgo en un sistema de IA. Fuente: Suresh and Guttag (2019) Otros sesgos aparecen por errores metodológicos: Por ejemplo, sesgos durante el entrenamiento por errores en procesos de validación, definición de métricas y evaluación de resultados (sesgo de evaluación), sesgos por tener supuestos erróneos sobre la población objetivo que puedan afectar a la definición del modelo, sesgos por el mal de uso y monitoreo de los modelos ya sea por interpretaciones inapropiadas de sus resultados o cambios temporales de los patrones en el mundo real o en los métodos de captura de datos, entre otros. Retos del ciclo de vida del ML Para la construcción de sistemas de toma/soporte de decisión robustos y responsables es necesario considerar las posibles fuentes de sesgo y deficiencias que pueden ser causadas por los datos de entrenamiento, problemas y decisiones en el desarrollo del modelo, definir de forma clara los objetivos de los sistemas y los criterios de justicia que se buscarán cumplir, entender las limitantes y errores en el contexto del proyecto específico y establecer medidas de monitoreo de los sistemas para evitar producir resultados indeseables e inequidad en la toma de decisiones. Para lograrlo, este manual presenta los retos y errores usuales en la construcción y aplicación de métodos de aprendizaje automático durante el ciclo de vida de la IA. Se describe en cinco secciones los problemas más comunes que se pueden encontrar, diagnósticos para detectarlos y sugerencias para mitigarlos: Conceptualización y diseño: que ser refiere a la información y criterios necesarios a obtener de parte del tomador de decisiones de política pública para iniciar un proyecto de IA. Fuente y manejo de datos: que se refieren principalmente a deficiencias, sesgos (por ejemplo, en etiquetado y medición, recolección de datos, autoselección, representatividad) y al proceso que genera los datos utilizados. Desarrollo del modelo: que se refiere a métodos y principios importantes para construir modelos robustos y validados correctamente. Uso y monitoreo: que se refiere a la interpretación de los resultados del modelo (cómo se construyen las predicciones, qué variables son importantes), evaluación una vez puesto en producción y principios de monitoreo para evitar consecuencias inesperadas. Rendición de cuentas: que se refiere a las medidas de explicabilidad y transparencia implementadas para fomentar la comprensión de un sistema de IA. Para acompañar su desarrollo se proponen tres herramientas que deberán contestarse a lo largo del desarrollo del sistema: Lista de verificación (Checklist): Herramienta que consolida las principales preocupaciones por dimensión de riesgo del ciclo de vida de IA. El checklist debe revisarse de forma continua por el equipo técnico acompañado por el tomador de decisiones. Perfil de Datos: El perfil de datos es un análisis exploratorio inicial durante la fase de Conocimiento y preparación de datos del ciclo de vida de IA. Brinda información para evaluar la calidad, integridad, temporalidad, consistencia y posibles sesgos, daños potenciales y las implicaciones de su uso. Perfil de Modelo: Descripción final de un sistema de IA, reporta los principales supuestos, las características más importantes del sistema y las medidas de mitigación implementadas. Ilustración 0.5: Retos técnicos del ciclo de vida del ML. Construcción propia. Estos dos tipos de sistemas son genéricos, es decir no necesariamente usan aprendizaje automático. También estos sistemas pueden ser interactivos y aprender en forma dinámica usando técnicas de aprendizaje por refuerzo, pero en este documento sólo consideramos sistemas no interactivos.↩ La IA se puede utilizar de distintas maneras, algunas de ellas pueden ser: i) Sistemas de alerta temprana o detección de anomalías: predicción de deserción escolar o alertas de fenómenos hidrometeorológicos; ii) Sistemas de recomendación o personalización: recomendación para vacantes laborales o personalización de materiales educativos; y iii) Sistemas de reconocimiento, diagnóstico de enfermedades, detección de objetos o reconocimiento biométrico.↩ Un modelo que presenta sesgo no necesariamente terminará en decisiones prejuiciosas, en ocasiones incluso puede ser deseable incrementar el sesgo de un sistema. En modelos de predicción existe una compensación entre la varianza y el sesgo que captura el modelo y su objetivo de generalización de aprendizaje. Por un lado un modelo con sesgo alto puede crear sistemas que subajustan y aprenden muy poco de los datos observados pero modelos con alta varianza pueden tener el efecto contrario y sobreajustar aprendiendo perfectamente los datos de entrenamiento. En la sección de ‘Desarrollo de Modelos’ de este manual se describe estos fenómenos con mayor detalle y medidas para mitigar sus riesgos.↩ La definición de justicia algorítmica es una representación matemática de este objetivo de política pública que se incorpora en el proceso de ajuste y selección de modelo. En la sección 2 de este manual se discutirá a profundidad distintas definiciones de justicia algorítmica y sus implicaciones.↩ "],
["conceptualización-y-diseño.html", "Sección 1 Conceptualización y diseño 1.1 Definición correcta del problema y de la respuesta de política pública 1.2 Necesidad y Proporcionalidad", " Sección 1 Conceptualización y diseño La implementación de una solución de IA no puede ir separada del ciclo de vida de la política pública con IA7 La IA es una herramienta, que debe estar condicionada a un buen diseño de la intervención o acción que se tomará con los resultados del sistema. La IA en ningún momento sustituye a la política pública. Esto implica que cualquier proyecto de IA robusto y responsable debe partir del problema y no desde la tecnología. 1.1 Definición correcta del problema y de la respuesta de política pública En este manual se asume que existen, al menos, dos actores involucrados en el desarrollo de los sistemas, el tomador de decisiones de políticas públicas y el equipo técnico que lo implementará. La definición de la intervención siempre debe ser responsabilidad del tomador de decisiones quien es quien tiene un conocimiento del problema social. Sin embargo, el equipo técnico debe poder entender el problema para que pueda vincular los resultados del modelo a la intervención deseada. Así mismo, es responsable de guiar y orientar en el diseño del sistema explicando lo que es viable y definiendo claramente las limitantes y riesgos del sistema, por lo que se requiere una comunicación constante entre ambos actores. Un caso de esto es la definición de la población donde se aplicará el sistema, la definición de grupos protegidos, así como las medidas de justicia algorítmica a aplicarse8 . Estas definiciones tienen un impacto directo a la forma en la que se puede evaluar la calidad y cobertura de los datos o el posible sesgo en los resultados del modelo. 1.2 Necesidad y Proporcionalidad Una vez que se tiene identificado el problema y la intervención es necesario replantearse el uso de la IA bajo criterios de necesidad y proporcionalidad. Aunque la IA tiene el potencial suficiente para hacer más eficiente algunos procesos y expandir la capacidad del estado, también se debe tener en cuenta que no es la respuesta para todo. En este punto es importante considerar también estándares y leyes que pueden regular el caso de uso donde se quiere implementar el sistema. Por ejemplo, requisitos de explicabilidad en las predicciones podrían limitar el uso de algunos algoritmos para los que es muy difícil dar interpretabilidad a los resultados.9 Medidas: Conceptualización y diseño de políticas públicas Definición correcta del problema y de la respuesta de política pública: (Cualitativo) ¿Se definió claramente el problema de política pública que se está buscando resolver? * (Cualitativo) Describa cómo se da respuesta a este problema actualmente, considerando a las instituciones relacionadas y cuál es su propuesta para solucionar dicho problema usando IA. (Cualitativo)¿Se definieron las acciones o intervenciones que se realizarán a partir del resultado del sistema de Inteligencia Artificial? Necesidad y proporcionalidad (Cualitativo) ¿Afectará su proyecto de forma directa o indirecta a la vida de personas o de personas o grupos vulnerables? (Cualitativo) Para la implementación de estas tecnologías, ¿se han revisado casos de proyectos similares anteriores? (Cualitativa) ¿Se identificaron los distintos grupos o atributos protegidos dentro del proyecto? (por ejemplo: edad, género, raza, nivel de marginación, etc.) Medidas: Gobernanza y seguridad10 Consentimiento informado y límites de privacidad: (Cuantitativa) ¿Se ha definido de forma clara la finalidad del tratamiento de los datos que va a recopilar y/o procesar? (Cuantitativa) ¿Su proyecto cuenta con los acuerdos legales de transferencia de información necesarios? Gobernanza y ciberseguridad (Cuantitativa) El Proyecto cuenta con estructuras y mecanismos concretos de gobernanza y ciberseguridad? (Cuantitativa) ¿Se han considerado formas de reducir al mínimo la exposición de información personal identificable? (por ejemplo, mediante la anonimización o la no recopilación de información que no sea pertinente para el análisis) Ver sección “Componentes de un sistema de IA para políticas públicas”.↩ En la sección 2 de este manual se discutirá a profundidad distintas definiciones de justicia algorítmica y sus implicaciones.↩ El punto de explicabilidad e interpretabilidad se describe en la sección de este manual.↩ La dimensión de Gobernanza y seguridad es una dimensión separada de la dimensión de conceptualización y diseño según el marco de retos de la IA de fAIr LAC: (Cristina Pombo (2020)). Por los objetivos de este manual y para mantenerlo lo más conciso posible se incluyen en esta sección. (Cristina Pombo (2020))↩ "],
["fuente-y-manejo-de-datos.html", "Sección 2 Fuente y manejo de datos 2.1 Calidad y relevancia de los datos disponibles 2.2 Información incompleta acerca de la población objetivo 2.3 Comparación causal", " Sección 2 Fuente y manejo de datos Existe un número cada vez mayor de fuentes de datos que pueden ser utilizadas para la toma de decisión en políticas públicas: censos, encuestas, registros administrativos, registros de uso (logs) de páginas web e incluso imágenes satelitales. Estos datos se vuelven información cuando se obtienen indicadores que describen a la población objetivo o al fenómeno que se está buscando entender. Sin embargo, no siempre los datos recolectados tienen una frecuencia, desagregación o cobertura que los haga relevantes, o no tienen la calidad necesaria para ser utilizados para la toma de decisiones. Por ejemplo, las encuestas diseñadas mediante muestreo probabilístico especifican por su diseño el tipo de análisis que se pueden hacer con ellas, pero este tipo de herramientas suelen levantarse con poca frecuencia y pueden ser insuficientes para capturar el movimiento de los patrones a estudiar. Por otro lado, la información proveniente de registros administrativos o datos provenientes de internet (interacción en redes sociales, visitas y otras medidas en páginas web, etc.) y telefonía (llamadas, ubicación por GPS, etc.) suelen tener una frecuencia mucho mayor, pero en pocos casos cubre a la población en su conjunto por lo que no es siempre posible utilizarla para tomar decisiones para toda la población. Ya sea que se esté implementando un modelo supervisado o no supervisado, los datos de entrenamiento son un punto muy importante de cualquier sistema de ML. La calidad de los datos se puede analizar mediante criterios como volumen, completitud, validez, relevancia, precisión, puntualidad, accesibilidad, comparabilidad e interoperabilidad de distintas fuentes. Definir con precisión estos criterios en general es difícil, pues el contexto de cada problema tiene particularidades sutiles. La relevancia y precisión se refieren a calidad de medición y utilidad para informar la decisión, mientras que la puntualidad se refiere a que los datos ocurren con la temporalidad necesaria para informar el problema a decidir. Accesibilidad, comparabilidad e interoperabilidad se refieren a que los datos pueden ser extraídos oportunamente y distintas fuentes de datos tienen la congruencia necesaria para aplicarse conjuntamente en el análisis.11 El primer conjunto de retos tiene que ver con las limitantes que los datos disponibles tienen de forma intrínseca. Se pueden separar en dos grandes grupos: Calidad y relevancia de los datos disponibles Información completa acerca de la población objetivo 2.1 Calidad y relevancia de los datos disponibles Los algoritmos de aprendizaje automático capturan patrones y relaciones observadas de los datos que reciben como entrenamiento, su objetivo es generalizar y poder identificar esos mismos patrones para casos nuevos no observados cuando el modelo fue entrenado. Por esta razón, los datos de entrenamiento condicionan la forma en la que el algoritmo se va a comportar. Sin embargo, no siempre los datos que se tienen disponibles son ideales para el caso de uso y para la decisión. Dos de los principales problemas son: Estados indeseables o subóptimos en datos recolectados. Mala correspondencia entre variables disponibles y variables ideales. 2.1.1 Reto: Estados indeseables o subóptimos en datos recolectados El primer reto es no tomar en cuenta que los datos con los que entrenamos un modelo de ML pueden haber capturado estados indeseables del mundo real. Cuando las variables respuesta son producidas por sistemas en estados indeseables desde el punto de vista de la política de interés. Estos “estados indeseables” pueden ser sesgos e inequidades perjudiciales para subgrupos, pero también puede ser cualquier otro patrón que se considere subóptimo o no deseable desde un punto de vista de política social, y generalmente es difícil tener información detallada acerca de esos defectos o inequidades. Ejemplos Un caso de este reto se dio en 2015 cuando Amazon experimentó con un sistema de recomendación de recursos humanos a partir de técnicas de aprendizaje supervisado. El modelo entrenaba con una base de datos de los procesos de selección de candidatos de la compañía almacenados durante los diez años anteriores. En esa base de datos se identificaba si un candidato había sido aceptado o rechazado para el trabajo por el departamento. El sistema se basaba en la hipótesis de que el algoritmo podría capturar buenos candidatos y reducir el trabajo del departamento de recursos humanos al hacer una primera selección de los candidatos. Lo que el equipo no había tomado en cuenta es que la industria de la tecnología se ha caracterizado por ser predominantemente masculina. Por lo que el sistema recomendaba una mayor proporción de hombres ya que más hombres habían sido aceptados en esos puestos históricamente creando un sesgo que parecía mostrar que los hombres eran más exitosos cuando en realidad estaba capturando una inequidad. En sistemas judiciales que en algunos países tienden detener, procesar y castigar a personas de menores ingresos, grupos sociales, raciales o con menor educación. (Washingtonpost, 2019) Intentar predecir “variables respuesta” producidas por un sistema discriminatorio para tomar decisiones automáticas o como soporte en la decisión puede reproducir esa misma discriminación. Medidas: Estados indeseables o subóptimos en datos recolectados (Cualitativa) Debe establecerse claramente que la utilización de la variable respuesta seleccionada está alineada con los propósitos o políticas de los tomadores de decisiones. (Cuantitativo) Realizar un análisis exploratorio para identificar sesgos históricos o estados indeseables. 2.1.2 Mala correspondencia entre variables disponibles y variables ideales Cuando se toman decisiones de política pública, se toman a partir de la definición de una o varias variables objetivo “ideales” que tiene en mente el tomador de decisiones. Sin embargo, las variables ideales pueden o no estar disponibles en los datos a los que se tiene acceso. En muchas ocasiones es necesario el uso de variables sustitutas o sucedáneas (proxy) que nos ayude a aproximarnos a la variable ideal. Cuando introducimos este tipo de variables dentro de modelos de ML podemos estar aprendiendo sesgos implícitos que pueden no ser deseables. Por ejemplo, una beca escolar que busque beneficiar a los estudiantes más inteligentes (variable ideal) se encontrará con el problema de definir ese concepto y encontrar una variable que pueda describirlo. Un examen de IQ asigna un valor mediante una prueba estandarizada que se describe como una variable proxy de la inteligencia. Sin embargo, el examen mide únicamente algunas dimensiones de la inteligencia por lo que subestimará la inteligencia de algunas personas (J. (2014)). Reto: Mala correspondencia entre variables disponibles y variables ideales Las variables medidas corresponden pobremente a las variables respuesta de interés. Es indeseable desde el punto de vista de la política hacer aprendizaje automático de medidas sesgadas o poco apropiadas para la toma de decisiones. Las variables objetivo deben plantearse claramente, aunque sean ideales. Las variables disponibles deben ser analizadas para entender qué tan adecuadas son para utilizarse como proxy de la variable ideal. Se deben identificar sesgos sistemáticos dentro del contexto de su uso.12 Ejemplos El sistema de salud de los Estados Unidos implementó un algoritmo para predecir las necesidades de cuidado médico que distintos pacientes requerían. En este caso, el tomador de decisiones de política pública quería una herramienta que le indicara de forma preventiva que pacientes tenían un alto riesgo de requerir mayores cuidados médicos utilizando la información histórica de los hospitales. Dado que la variable ideal de riesgo de complicación no estaba disponible, utilizaron como variable proxy el gasto que incurrieron los pacientes durante su enfermedad, bajo la hipótesis de que personas más enfermas terminarían gastando más en tratamientos médicos para superar la enfermedad. (Obermeyer et al. (2019)) demostraron que este sistema tenía un sesgo racial ya que subestimaba el número de pacientes negros con necesidades de atención médica. El sesgo racial se ocasionaba porque esta subpoblación gastaba, en promedio, menos dinero que los pacientes blancos. Al utilizar el gasto como variable proxy de riesgo de complicación los pacientes blancos más saludables parecían requerir más cuidados de salud que pacientes negros más enfermos. Mitigación: Usar gasto en salud como medida sucedánea de necesidad de cuidado médico es poco apropiado en situaciones donde existe desigualdad económica. Otras medidas asociadas a la salud de los individuos serían más apropiadas (por ejemplo evaluaciones médicas). Suponiendo que una política se considera aplicar a personas de ingresos bajos y que en el conjunto de datos de entrenamiento/validación se usa una medición obtenida en una encuesta, según la pregunta, ¿cuánto estima usted que es el ingreso mensual de su familia?, se puede inferir que esta medición está sujeta a sesgos, ya que existen incentivos para sobre o subreportar los ingresos por parte de los participantes. Por esta razón, predictores de ingreso construidos con estos datos tienen el riesgo de replicar el sesgo de las mediciones, afectando negativamente los resultados de la asignación de la intervención. Mitigación: En la estimación de ingreso, generalmente se utilizan fuentes de datos oficiales con metodología bien establecida para estimar el ingreso de un hogar. Tal metodología debe ser sostenida por validaciones de distinto tipo que muestra posibles sesgos en la medición y de ser posible cuantificaciones del error de medición. Imaginando un proyecto de ML busca pronosticar los niveles de demanda de medicamentos, de manera que pueda satisfacerse la demanda adecuadamente sin incurrir en inventario que caduca. Utilizamos el número de unidades que fueron requeridas en el sistema para el medicamento X. Ésta no es una medida exacta de la demanda, porque puede ser que cuando los inventarios se agotan, los involucrados dejan de hacer requerimientos a los abastecedores. Pronosticar la demanda con estos datos puede incurrir en subestimaciónsubestimación, con el resultado de que reforzamos o empeoramos la escasez de medicamentos. Mitigación: Para los pronósticos de demanda, es posible que sea necesario identificar fuentes adicionales de datos que indiquen mejor la demanda, ya sea con estudios indirectos (fuentes de datos de los compradores) oconstruyendo experimentos que nos permitan observar la demanda en ciertos medicamentos. Medidas: Mala correspondencia entre variables disponibles y métricas objetivo. (Cualitativa) Las variables objetivo deben plantearse claramente, aunque sean ideales. Las variables recogidas/disponibles deben ser analizadas para entender qué tan adecuadas son para sustituir la variable objetivo. Se deben identificar sesgos sistemáticos o validez de la métrica sustituto. (Cualitativa) ¿Se ha justificado claramente la utilización de la variable respuesta seleccionada bajo los propósitos de la intervención? (Cuantitativa) Estudios adicionales diseñados para capturar métrica objetivo y métrica seleccionada permiten comparar las dos, y entender si hay sesgos que corregir y con qué variables puede lograrse esto.``` 2.2 Información incompleta acerca de la población objetivo Los modelos de ML pretenden generar información para tomar acciones o políticas para una población objetivo. La mayor parte del tiempo las fuentes de datos no incluyen a toda la población (como sería el caso de un censo), por lo que es usual que sólo se tenga disponible un subconjunto o muestra de la misma (una encuesta, una base de datos administrativa, etc.), a partir de la cual se debe buscar desarrollar extrapolaciones, predicciones o estimaciones que ayuden en la toma de decisiones. 2.2.1 Muestras probabilísticas y naturales En estadística, una muestra es un subconjunto de casos o individuos de una población y existen dos extremos posibles: Muestreo probabilístico: Se le llama así a una muestra en donde los casos son seleccionados a partir de un diseño probabilístico. Por ejemplo, muestra aleatoria simple, estratificada, por conglomerados, etc. En este caso, todas las predicciones y estimaciones que se pretenden aplicar a la población objetivo pueden ser evaluadas en cuanto a su precisión con garantías probabilísticas. Es decir, podemos dar rangos de error para estimaciones de cantidades asociadas a toda la población objetivo. Por ejemplo: Una encuesta nacional de hogares con diseño probabilístico generalmente consiste en una definición de estratificación, unidades de selección aleatoria a distintos niveles (unidades primarias, secundarias, etc.). Cada hogar es seleccionado con una probabilidad conocida. Aunque la muestra se diseñe de manera no representativa (por ejemplo, más hogares en zonas rurales o de ingresos bajos), es posible hacer inferencia para toda la población con ciertas garantías acerca del tamaño de error de estimación. Muestras naturales (no probabilísticas): Por otro lado, una muestra natural o no probabilística se da cuando los casos no son seleccionados de forma aleatoria sino por un proceso natural mal o parcialmente conocido. En este caso, no es posible saber qué va a pasar cuando apliquemos una política resultado de un modelo en la población general y no se pueden construir rangos de error de predicciones y estimaciones mediante métodos estadísticos que tengan garantías probabilísticas. Es decir, las cantidades y predicciones estimadas tienen error desconocido, los modelos y características útiles en la muestra pueden no aplicar en la población objetivo, y la situación puede agravarse para grupos protegidos subrepresentados (ver por ejemplo (Williams (1981)), donde se muestra que valores predictivos de anemia pueden ser distintos para distintos grupos raciales, y predicciones desarrolladas para un grupo pueden tener desempeño pobre para otro). Un caso usual de este tipo de muestras se da cuando por el canal de captura de la información, se excluyen subgrupos particulares de población (sesgo de selección). Por ejemplo, con aplicaciones donde la población que no tenga acceso a Internet o un teléfono inteligente será excluida, este es el caso de la información proveniente de redes sociales, registros de llamadas telefónicas, etc. La situación ideal es la de muestreo probabilístico. En este caso, se puede entender exactamente qué subpoblaciones se muestrearon, a qué tasas, y cómo se relacionan estas tasas con las tasas poblacionales. El diseño de la muestra determina el alcance inferencial. Sin embargo, tener una muestra probabilística no es siempre posible. Esto no quiere decir que las muestras naturales no sean útiles, en muchas ocasiones son la única fuente de datos disponible para la toma de decisiones. Sin embargo, es importante entender de dónde provienen los datos para poder tomar en cuenta sus limitantes e identificar los riesgos implícitos al tomar decisiones para toda la población. Un caso típico son las muestras de datos que provienen de redes sociales y que no son representativas demográficamente. Por ejemplo, en Twitter hay en general más personas jóvenes y más hombres, con lo que es necesario segmentar y ponderar la muestra para poder extrapolar los resultados al resto de la población. Algo importante a tomar en cuenta es que tener muestras equilibradas en términos de características de la población no es tampoco una condición ni necesaria ni suficiente para calificar como apropiada la base de datos para la construcción de modelos de ML. Por ejemplo, en el caso de información recolectada por redes sociales, el que se tenga una muestra que contenga 50% hombres y 50% mujeres no te dice nada sobre el tipo de conclusiones que se pueden tomar con esos datos, ya que la selección de esas observaciones, al no darse mediante un proceso probabilístico, podría presentar un sesgo en alguna otra dimensión y no necesariamente generalizarán a la población total. Reto: muestras naturales Las muestras naturales de datos pueden resultar en: Errores de estimación y de predicción incorrectamente estimados Estructuras predictivas distintas a las que observaríamos en la población objetivo (modelos no válidos). Extrapolaciones que no son soportadas por los datos. Sub o sobrerepresentación de subconjuntos de la población. Medidas: muestras naturales (Cualitativa) ¿Se ha analizado las posibles diferencias entre la base de datos y la población para la que se busca desarrollar el sistema de IA? (Utilizar literatura relacionada con el tema, información de expertos. Estudiar en particular sesgos de selección no medidos) (Cuantitativa) Aunque los modelos pueden construirse con varias fuentes de datos, diseñadas o naturales, la validación debe llevarse a cabo idealmente con una muestra que permita inferencia estadística a la población objetivo. La muestra de validación debe cubrir apropiadamente la población objetivo y subpoblaciones de interés. Términos Población objetivo: son los elementos que se pretende intervenir (personas, hogares, zonas geográficas, etc.). Los modelos se construyen entonces con el fin de aplicarse a la población objetivo. Estructura predictiva: se utiliza para hablar en general del tipo de modelos que se utiliza para hacer predicciones (lineales, bosques aleatorios, redes neuronales), las características que utiliza y cómo las utiliza el modelo (interacciones, transformaciones no lineales). Subpoblaciones de interés o Subpoblaciones protegidas: subpoblaciones de la población objetivo para las cuales queremos tener evaluaciones concretas del desempeño de estimaciones o los modelos. Representatividad: término alternativo a muestra probabilística bajo el caso más simple de muestreo autoponderado. Garantías probabilísticas: bajo muestras diseñadas con aleatorización, es posible, bajo ciertos supuestos, caracterizar comportamiento de estimadores y procedimientos (con alta probabilidad). Por ejemplo, un intervalo de confianza del 95% para las métricas de desempeño que contiene al valor real que será observado. 2.2.2 Atributos faltantes o incompletos Muchos proyectos de aprendizaje automático están destinados a fallar por la poca calidad de los datos con los que se cuenta. Cuando se recolectan datos del mundo real a través de muestras no probabilísticas es muy común que algunas observaciones tengan datos faltantes, es decir observaciones para las que no se tienen todos los atributos. Los atributos faltantes o incompletos son un fenómeno que pueden tener un efecto significativo en las conclusiones que pueden extraerse de los datos. Reto: Atributos faltantes o incompletos Por un lado, cuando información crucial acerca de las unidades es totalmente desconocida, esto puede resultar en modelos de desempeño pobre, con poca utilidad para la toma de decisiones y, por otro lado, la ausencia de información puede también estar asociada a características relevantes de las unidades para las que se quiere predecir. Cuando existen observaciones faltantes se pueden implementar distintos métodos de imputación, pero es importante explorar las razones o el “mecanismo de censura” por el que una observación puede tener valores faltantes. En la literatura existen tres principales supuestos sobre el “mecanismo de censura” (Little and Rubin (2002)): Valores faltantes completamente aleatorios (Missing Completely at Random - MCAR): Se da cuando la probabilidad de faltar es la misma para todas las observaciones. Es decir, la censura o falta se produce totalmente al azar. Valores faltantes aleatorios (Missing at Random - MAR) Se da cuando los valores faltantes no dependen de los valores que toma esa variable, pero sí existe una relación entre los valores faltantes y otros datos observados del individuo. Valores faltantes no aleatorios (Missing Not at Random - MNAR) Se da cuando los valores faltantes dependen de los valores que toma esa variable o de datos no observados. Por ejemplo, es un fenómeno conocido que cuando se levantan encuestas de ingreso auto reportado las personas con mayor ingreso tienden a no revelarlo. Ejemplos Algunos sistemas judiciales tienden detener, procesar y castigar a las personas de menores ingresos, menos educación y menos contactos sociales (ver por ejemplo este reporte de Conapred). Intentar predecir “variables respuesta” producidas por un sistema discriminatorio para tomar decisiones automáticas o como soporte en la decisión puede reproducir esa misma discriminación. Medidas: atributos incompletos (Cualitativa) ¿Se ha realizado un análisis de valores faltantes y de variables omitidas? (Cualitativa) ¿Se ha identificado si existen variables omitidas importantes para las cuales no se tiene mediciones asociadas? (En caso de existir) (Cualitativa) ¿Se ha identificado las razones por las que existen observaciones faltantes? (En caso de existir) (Cuantitativa) Los procesos de imputación tienen que ser evaluados en cuanto a su sensibilidad a supuestos y datos. De preferencia, se deben utilizar métodos de imputación múltiple que permitan evaluar incertidumbre en la imputación (Little and Rubin (2002)) (van Buuren and Groothuis-Oudshoorn (2011)). 2.3 Comparación causal Cuando los humanos racionalizan el mundo lo intentan comprender en términos de causa y efecto - si entendemos por qué ocurrió algo, podemos alterar nuestro comportamiento para cambiar resultados futuros. Un modelo de ML nos puede dar resultados que parecerían describir relaciones causales sin que necesariamente lo sean. Si la política se aplica en función de hallazgos en términos de las variables incluidas en el modelo, la derivación de políticas a partir de esos modelos puede llevar a decisiones erróneas. Técnicas econométricas como los experimentos aleatorios controlados o RCTs (randomized controlled trials), experimentos naturales, diferencia en diferencias y variables instrumentales son utilizadas con estos objetivos para controlar por fenómenos como sesgo por selección o endogénesis por variables omitidas, entre otros. En los últimos años trabajos como (Wager and Athey (2018)) han comenzado en introducir en algoritmos de ML estas técnicas y procesos experimentales tipo A/B testing se han empezado a utilizar de forma masiva en contextos digitales por la facilidad de crear experimentos masivos en Internet. Sin embargo, en la mayoría de los casos los algoritmos de ML no buscan describir relaciones causales y es necesario ser muy cuidadosos con este tipo de uso (Imai, King, and Stuart (2008)). Medidas: comparación causal (Cualitativa) Entender y describir las razones por las que la variable tratamiento está correlacionada con variables conocidas y no conocidas. Describir sesgos posibles basados en análisis y conocimiento experto. (Cuantitativa) En el caso de intentar inferencia causal con modelos, debe describirse cuáles fueron las hipótesis, consideraciones o métodos usados para soportar una interpretación causal. (Cuantitativa) Producir datos diseñados que incluyan la consideración causal. Esto incluye experimentos aleatorios y otras técnicas. En muchos casos, suplir con la modelación de datos creados experimentalmente puede ser muy difícil, y los resultados pueden depender fuertemente de las decisiones del equipo técnico o modelador. Como consecuencia, la incertidumbre es rara vez medida. Observación: este problema es ortogonal al de la representatividad o diseño muestral. Muestras bien diseñadas desde el punto de vista de la población objetivo pueden ser poco apropiadas para hacer inferencia causal, y a la inversa, datos experimentales pueden proveer indicaciones causales correctas en la muestra seleccionada, pero tener dificultades para generalizar a una población objetivo. Por ejemplo, estudios clínicos para medir la efectividad de una medicina realizan tratamientos aleatorizados para estimar adecuadamente el efecto sobre los pacientes que recibieron la medicina en la muestra, pero muchas veces estos efectos no se extienden para la población general (las muestras son autoseleccionadas, con sesgos a cierto tipo de pacientes). Por otra parte, un diseño muestral probabilístico riguroso para estimar ingresos y gastos de hogares no nos permite saber cuál es el efecto causal sobre los ingresos de que un hogar tenga seguro médico. Actividad: * Se recomienda el llenado del Perfil de Datos durante la fase de Conocimiento y preparación de datos del ciclo de vida de IA. * Al terminar esta fase se recomienda el llenado de la sección de Fuente y Manejo de datos del Perfil de Modelo y llevar a cabo una discusión con el tomador de decisiones de políticas públicas. En esta etapa se recomienda el llenado del Perfil de Datos de la sección de herramientas de este manual.↩ Ver ejemplo de cuadernillo.↩ "],
["desarrollo-de-modelos.html", "Sección 3 Desarrollo de modelos 3.1 Ausencia y errores de validación 3.2 Fugas de información 3.3 Clasificación: probabilidades y clases 3.4 Sub y sobreajuste 3.5 Errores no cuantificados y evaluación humana 3.6 Equidad y desempeño diferencial de predictores", " Sección 3 Desarrollo de modelos El proceso de desarrollo de un modelo conlleva muchas decisiones que tienen implicaciones en los resultados de este. Algunas decisiones pueden llevar a cometer errores metodológicos que generen sesgos o que eviten que el sistema generalice de forma adecuada, entre estos encontramos fugas de información, sobreajuste y subajuste. Pero también hay otro grupo de decisiones que no son necesariamente problemas metodológicos que pueden cambiar sustancialmente la forma en la que se comporta el sistema, ¿cómo elegir entre dos modelos?, ¿qué tipo de errores reportar?, ¿qué definición de justicia algorítmica se elegirá? Como se comentó al principio del manual ninguna de estas preguntas tiene sentido fuera del contexto de la aplicación específica. Lo que sí es posible es crear un marco de entendimiento de estos errores para que puedan ser discutidos entre los equipos técnicos y los tomadores de decisiones de política pública. En esta sección del manual se exponen retos que aparecen durante los procesos de entrenamiento y validación de los sistemas de soporte y toma de decisión. En este caso la mayoría de los errores se deben a fallos metodológicos en la evaluación y a no plantear de forma correcta el objetivo de ajuste del sistema o las métricas que se buscan optimizar. 3.1 Ausencia y errores de validación Uno de los primeros errores graves en este proceso es la no consideración de etapas robustas de validación y prueba de los modelos Los modelos de aprendizaje automático se entrenan principalmente para crear predicciones en casos no observados. De nada sirve evaluar un sistema en su desempeño de predicción de las observaciones con las que se entrenó, pues el sistema podría únicamente memorizar cada respuesta13. Su utilidad se encuentra en la medida en la que el sistema logra generalizar un aprendizaje para predecir con datos fuera del conjunto de entrenamiento (out-of-sample). La validación generalmente involucra al menos dos muestras (1 y 2), y de preferencia tres: Datos de entrenamiento: Subconjunto de los datos utilizados para entrenar el modelo. Datos de validación: Subconjunto de los datos con los que se evalúa el entrenamiento de forma iterativa. Datos de prueba: Subconjunto de los datos que se deben mantener ocultos hasta después de seleccionar el modelo y son usados para confirmar los resultados. Para evitar que una partición aleatoria en datos de entrenamiento y validación favorezca o perjudique la evaluación, en general se hace una validación cruzada. Esto consiste en dividir los datos en k pedazos, calculando el promedio de k evaluaciones, donde los datos de validación son cada uno de los pedazos y los k-1 restantes son los datos de entrenamiento. Esto en inglés se llama k-fold evaluation y normalmente se escoge k=5 o k=10. Ilustración 3.1: Etapas de evaluación. Construcción propia. El primer reto es no tener un proceso de validación apropiado o que incluso sea inexistente. En este caso, los resultados del modelo se presentarían únicamente con el desempeño del conjunto de datos de entrenamiento. Las métricas de desempeño de este conjunto no deberían utilizarse como indicador del potencial comportamiento del modelo para casos nuevos ya que podría estar sobreestimando su desempeño. Una validación exitosa está también relacionada a los criterios de calidad tales como la completitud y representatividad de la información que vimos en el capítulo anterior, ya que, si la población objetivo es distinta a la representada por datos utilizados durante el entrenamiento, aunque el proceso de evaluación se haya realizado de forma correcta, se puede tener un comportamiento completamente distinto. Medidas: ausencia de muestras de validación (Cuantitativa) ¿Se construyeron las muestras de validación y prueba adecuadamente? Considerando un tamaño apropiado, cubriendo a subgrupos de interés y protegidos y evitando fugas de información durante su implementación − La construcción de la muestra de validación debe ser producida bajo un diseño muestral que permita inferencia a la población objetivo (Lohr (2009)). − La muestra de validación debe cubrir a subgrupos de interés y protegidos, de manera que sea posible hacer inferencia a sus subpoblaciones. Eso incluye tamaños de muestras adecuados según metodología de muestreo (Lohr (2009)). − Si no está disponible tal muestra, es indispensable un análisis de riesgos y limitaciones de la muestra natural, conducida por expertos y personas que conozcan el proceso que generó esos datos muestrales. 3.2 Fugas de información Las fugas de información (Kaufman, Rosset, and Perlich (2011)) ponen en duda la validación de modelos como manera de estimar el desempeño en producción de los métodos de aprendizaje automático. Esto ocurre de dos maneras: Fugas de entrenamiento-validación: La muestra de entrenamiento recibe fugas de los datos de validación, lo que implica el uso de datos de validación en entrenamiento e invalida la estimación del error de predicción. Fugas de datos no disponibles en la predicción: Muestras de validación y entrenamiento tienen agrupaciones temporales o de otro tipo que no se conservan en el proceso de entrenamiento y validación. En este caso, entrenamiento y validación recibe fugas de información que no estará disponible el momento de hacer predicciones. Reto: fugas entrenamiento-validación Si alguna parte de los datos de validación/prueba se utiliza en la construcción de los modelos durante entrenamiento, la muestra de validación/prueba no cumple su función de dar una estimación realista (out of sample) del error en producción. Otro fenómeno común de fuga de información es el uso iterativo del conjunto de prueba para el ajuste de hiper-parámetros. El uso de este conjunto en más de una ocasión transmite información del conjunto de prueba al modelo que puede llevar a sobreestimar su precisión. Medidas: fugas entrenamiento-validación (Cuantitativa) Cualquier procesamiento y preparación de datos de entrenamiento debe evitar usar los datos de validación o prueba de ninguna manera. Se debe mantener una barrera sólida entre entrenamiento vs validación y prueba. Esto incluye recodificación de datos, normalizaciones, selección de variables, identificación de datos atípicos y cualquier otro tipo de preparación de cualquier variable a ser incluida en los modelos. Esto incluye también ponderaciones o balance de muestras basados en sobre/sub muestreo. 3.2.1 Fugas de datos - Variables no disponbiles en la predicción Reto: Fugas de datos - Variables no disponbiles en la predicción Algunos modelos son riesgosos de poner en producción pues utilizan variables en entrenamiento y validación que no estarán disponibles en la misma forma al momento de poner en producción. Esto generalmente tiene ver con temporalidad de los datos o agrupaciones particulares. En el caso más extremo, aunque quizá más fácil de detectar, existen variables presentes en datos de entrenamiento que no estarán disponibles en producción. En casos más sutiles este error puede ser difícil de detectar ya que la variable está presente pero la información se actualiza de forma retroactiva. Este tipo de error generalmente produce modelos que parecen muy optimistas, y ocurre de muchas maneras. Un ejemplo de esto se puede observar en estadísticas de criminalidad y mortalidad. Los reportes de un robo ante las autoridades pueden tardar en reportarse en las bases de datos (por burocracia o procesos administrativos) y la incidencia observada de un periodo podría incrementarse sistemáticamente conforme pasa más tiempo. Este modelo tendría una fuga de datos no disponibles en la predicción, pues al momento de predecir actividad criminal al tiempo los datos temporales más cercanos presentarían un patrón a la baja. El modelo puede parecer preciso, pero en producción su exactitud se verá considerablemente degradada. Otro ejemplo es que siempre habrá más casos de criminalidad donde hay más control policíaco y menos donde no lo hay, Es decir, el sesgo de las observaciones refuerza el mismo sesgo. Medida: fugas de datos no disponbiles en la predicción El esquema de validación debe replicar tan cerca como sea posible el esquema bajo el cual se aplicarán las predicciones. Esto incluye que hay que replicar: * Ventanas temporales de observación y registro de variables y ventanas de predicción. * Si existen grupos en los datos, considerar si tendremos información disponible de cada grupo cuando hacemos la predicción, o es necesario predecir para nuevos grupos. Ejemplo Supóngase que se quiere predecir, en varias regiones o ciudades, el daño de edificios a partir de fotos aéreas después de un temblor, usando como métrica objetivo peritajes de los edificios seleccionados. En la validación se podría cometer el error de no respetar la agrupación regional, y el modelo podría parecer dar buenas predicciones. En la realidad, se aplicaría para una región sobre la cual no hay información. La validación debe considerar la necesidad de predecir para puntos en regiones enteras sin tener información adicional de tal región (es decir, la validación debe estratificar por región). 3.3 Clasificación: probabilidades y clases Reto: Métricas de evaluación fuera de contexto En problemas de clasificación, los puntos de corte o decisiones de clasificación se toman con criterios vagamente relacionados con el contexto de la decisión. La mayoría se construye mediante el análisis de la matriz de confusión de clasificación. Real Positivos Negativos Predicho Positivo Verdadero Positivo (VP) Falso Positivo (FP) Negativo Falso Negativo (FN) Verdadero Negativo (VN) Exactitud (Accuracy): Una de las métrica más utilizadas para evaluar los modelos de clasificación y es la fracción de las predicciones que el modelo tuvo correctas. Sin embargo, la precisión puede resultar muy engañosa cuando hay desbalance de clases. \\[\\text{Exactitud} = \\frac{\\text{ VP+VN }}{\\text{ VP + VN + FP + FN}}\\] Precisión: Fracción de aquellos clasificados como positivos por el modelo que en realidad eran positivos. \\[\\text{Precision} = \\frac{VP}{VP+FP}\\] Sensibilidad (Recall): Fracción de positivos que el modelo clasificó correctamente. \\[\\text{Sensibilidad} = \\frac{VP}{VP+FN}\\] 3. Especificidad: Fracción de negativos que el modelo clasificó correctamente. \\[\\text{Especificidad} = \\frac{VN}{VN+FP}\\] Sin embargo, estos criterios pueden ser muy engañosos dependiendo de la composición de la base de datos de entrenamiento y evaluación principalmente con datos desbalanceados donde una exactitud de 95% puede en realidad ser un modelo muy malo. Soluciones parciales a este problema incluyen usar medidas que combinan la precisión y la sensibilidad como la medida F o usar la sensibilidad, también llamada exhaustividad, de la clase más importante. Adicionalmente el criterio que se debe de priorizar siempre deberá de ser analizado en función del contexto del problema a resolver. Por ejemplo, si el modelo está clasificando la prevalencia de una enfermedad mortal, el costo de no diagnosticar la enfermedad de una persona enferma es mucho mayor que el coste de enviar a una persona sana a más pruebas. En otras palabras, dependiendo de la aplicación, el costo de los falsos negativos es muy distinto al costo de los falsos positivos. Por esta razón se recomienda el uso de análisis costo beneficio ya que compara el resultado del modelo en el contexto de la toma de decisión. Medida: Métricas de evaluación fuera de contexto (Cualitativa) ¿Se cuestionaron las implicaciones de los diferentes tipos de errores para el caso de uso específico, así como la forma correcta de evaluarlos? (Cualitativa) ¿Se explicó de forma clara las limitantes del modelo? Identificando tanto los falsos positivos como falsos negativos y las implicaciones que una decisión del sistema tendría en la vida de la población beneficiaria. (Cuantitativa) ¿Se implementó un análisis costo-beneficio del sistema contra el status quo u otras estrategias de toma/soporte de decisión? (Cuando es posible) Reto: Puntos de corte arbitrario Muchas veces, en problemas de clasificación, se toma erróneamente un punto de corte de 0,5 para clasificación binaria, que es el valor por defecto de mucho de las técnicas de aprendizaje automático. Esta decisión se toma fuera del contexto de la decisión que se quiere tomar y puede tener implicaciones importantes. Como se mencionó en el punto anterior, para tener una mayor sensibilidad en la clase de interés o darles más valor a los falsos negativos, debemos usar un punto de corte menor a 0,5. Medida: puntos de corte arbitrario (Cuantitativa) En problemas de clasificación ruidosos (no es posible acercarse a tener certidumbre para muchos casos), las probabilidades de clasificación en cada clase son instrumentos más apropiados para la toma de decisiones. (Cuantitativa) Costos y utilidades pueden utilizarse, en combinación con las probabilidades, para tomar mejores decisiones caso por caso. Reto: Datos desbalanceados En problemas de clasificación muchas veces se presenta el fenómeno de que algunas clases tienen representación relativamente baja (por ejemplo, clases con menos de 1% de los casos totales). Estas clases presentan dificultades considerables en los modelos predictivos, pues puede ser que tengamos poca información acerca de esas clases y sea difícil discriminarlas exitosamente de otras clases, aun cuando contemos con la información correcta. En datos con desbalance grande, predictores de clase (el algoritmo determina si una imagen es de un perro o un gato, por ejemplo) pueden tener desempeño malo (por ejemplo, nunca hacen predicciones de la clase minoritaria) aunque las medidas de desempeño sean buenas (si predecimos siempre la clase mayoritaria, la exactitud será igual al porcentaje de elementos en esta clase). Una solución es considerar las probabilidades de clase (el algoritmo da la probabilidad de que una imagen contenga un perro) como salida principal. Medidas: desbalance de clases * (Cuantitativa) Hacer predicciones de probabilidad en lugar de clase. Estas probabilidades pueden ser incorporadas al proceso de decisión posterior como tales. Evitar puntos de corte estándar de probabilidad como 0.5, o predecir según máxima probabilidad. * (Cuantitativa) Cuando el número absoluto de casos minoritarios es muy chico, puede ser muy difícil encontrar información apropiada para discriminar esa clase. Se requiere recolectar más datos de la clase minoritaria. * (Cuantitativa) Submuestrear la clase dominante (ponderando hacia arriba los casos para no perder calibración) puede ser una estrategia exitosa para reducir el tamaño de los datos y tiempo de entrenamiento sin afectar desempeño predictivo. * (Cuantitativa) Replicar la clase minoritaria, para balancear mejor las clases (sobremuestro). * (Cuantitativa) Algunas técnicas de aprendizaje automático permiten ponderar cada clase por un peso distinto para que el peso total de cada clase quede balanceado. Si esto es posible, es preferible a sub o sobremuestreo. Ejemplos Consideremos que tenemos 1 millón de datos, 999 mil negativos y mil positivos. Puede ser buena idea submuestrar los negativos por una fracción dada (por ejemplo 10%) ponderando cada caso muestreado por 10 en el ajuste y el postproceso. Consideremos que tenemos 1 millón de datos, 999.950 mil negativos y 50 positivos. Puede ser imposible discriminar apropiadamente los 50 datos positivos. Construir conjuntos de validación empeora la situación: no es posible validar el desempeño predictivo ni construir un modelo con buen desempeño. Observaciones: Sub y sobremuestreo alteran la proporción natural de positivos y negativos en los datos. Esto quiere decir que las probabilidades obtenidas están mal calibradas y tienen menos utilidad para la toma de decisiones. Es importante recordar que los procesos de sub y sobre muestreo se tienen que realizar de forma posterior a la separación de la muestra en (entrenamiento, evaluación y validación) ya que de otra forma se puede tener fuga de información. 3.4 Sub y sobreajuste Sub y sobreajuste ocurren cuando la información predictiva en los datos es usada de manera poco apropiada para el objetivo final del aprendizaje automatizado que es la generalización del aprendizaje y su uso en conjuntos de datos no observados en el entrenamiento. Sobreajuste: Se da cuando el modelo sobreentrena en las particularidades de los datos de entrenamiento. Un modelo demasiado complejo para los datos disponibles tiende a capturar características no informativas como parte de la estructura predictiva. Esto se refleja muchas veces en una brecha de error grande entre entrenamiento y validación. Estos pueden ser modelos ruidosos difíciles de interpretar, y las predicciones pueden ser inestables dependiendo del conjunto de datos particular que se utilice. Subajuste: Se da cuando agrupamos de más y damos poco peso a características individuales de los casos. Un modelo con subajuste tiende a ignorar patrones sólidos en la estructura predictiva. Esto se refleja en errores sistemáticos e identificables, por ejemplo, sub/sobre predicción sistemática para ciertos grupos o valores de las variables de entrada. Ilustración 3.2: Sub y sobre ajuste Reto: sub y sobreajuste Modelos que presentan sub o sobreajuste son particularmente difíciles de interpretar, y comparaciones predictivas pueden ser malas. Modelos subajustados pueden cometer errores sistemáticos que pueden afectar negativamente, por ejemplo, al tratamiento de grupos protegidos. Modelos sobreajustados pueden tener predicciones inestables que cambian mucho dependiendo de los datos, por ejemplo, con cada actualización. Aunque sub y sobre ajuste puede producir resultados predictivos subóptimos, pueden producir rangos de error aceptables (según el contexto del problema, conocimiento experto, y objetivos) esto sucede sobretodo si no se diseñó una validación de forma correcta o si existe fuga de información entre los subconjuntos. Medidas: sub y sobreajuste (Cuantitativa) Sobreajuste: debe evitarse modelos cuya brecha validación - entrenamiento sea grande (indicios de sobreajuste). De ser necesario, deben afinarse métodos para moderar el sobreajuste como regularización, restricción del espacio funcional de modelos posibles, usar más datos de entrenamiento o perturbar los datos de entrenamiento, entre otros (Hastie, Tibshirani, and Friedman (2017)). (Cuantitativa) Subajuste: deben revisarse subconjuntos importantes de casos (por ejemplo, grupos protegidos) para verificar que no existen errores sistemáticos indeseables. Ejemplo (sesgo en procesamiento de caras): Algunos algoritmos de preprocesamiento o afinación de fotos de caras tienden a producir caras del tipo racial que domina las datos. Este es un ejemplo de subajuste, donde en ciertos grupos raciales con menor representatividad en los datos toman demasiada información de las caras dominantes, lo cual produce sesgo para grupos de menor representatividad: las imágenes procesadas tienden a parecerse más a la del grupo dominante. 3.5 Errores no cuantificados y evaluación humana En muchos casos, existirán aspectos del modelo que no son medidos por las métricas de desempeño que se han escogido, relacionado con algún sesgo particular en las predicciones que no son deseables para la toma de decisiones. Por ejemplo, en un sistema de búsqueda en documento que, aunque tenga buen desempeño de validación en las métricas, seleccione documentos que tiendan a ser demasiado cortos, produzcan resultados poco útiles o imparciales para búsquedas particulares, o prefiera documentos de tipo promocional o propagandístico. Las razones pueden ir desde errores de preprocesamiento (algunos atributos mal calculados) hasta la selección de atributos para hacer las predicciones que consideran sólo una parte del problema. Reto: Fallas no medidas por el modelo Algoritmos o métodos predictivos producen resultados que de mala calidad según aspectos no medidos por las métricas de validación, o que tienen mal desempeño para ciertos subconjuntos de datos no predefinidos. Esto puede ser por varias razones: Errores de preprocesamiento en el momento de calcular predicciones. Tratamiento de los datos que excluyen métricas importantes para hacer predicciones de calidad o no injustas. Ausencia de métricas que midan cierto tipo de errores particulares graves Este puede ser un problema difícil, pues por su naturaleza son errores no visibles o medidos directamente. Es necesario descubrir estos sesgos o errores fuera del contexto técnico de evaluación, y de ser posible incluir métricas adicionales de evaluación que consideren estos problemas. Medidas: errores no medidos y revisión humana (Cualitativa) ¿Se realizó una evaluación humana con expertos del caso de uso para buscar sesgos o errores conocidos? (Por ejemplo, se pueden usar paneles de revisores que examinen predicciones particulares y consideren si son razonables o no. Estos paneles deben ser balanceados en cuanto al tipo de usuarios que se prevén, incluyendo tomadores de decisiones si es necesario). (Cuantitativa) Esquemas de monitoreo de predicciones que permitan identificación de errores o sesgos no medidos. Por ejemplo, se pueden usar paneles de revisores que examinen predicciones particulares y consideren si son razonables o no. Estos paneles deben ser balanceados en cuanto al tipo de usuarios que se prevén, incluyendo tomadores de decisiones, si es necesario. 3.6 Equidad y desempeño diferencial de predictores Métodos basados en aprendizaje automático pueden producir resultados injustos o discriminatorios para subgrupos (Buolamwini and Gebru (2018)), (Barocas and Selbst (2014)), (Bolukbasi et al. (2016)). Estos resultados pueden estar ocasionados por todos los retos antes mencionados, tanto de la fuente y manejo de los datos como de errores en el diseño del modelo. Ejemplos de desempeño diferencial e inequidad pueden encontrarse en las citas de arriba, e incluye, por ejemplo, distintas tasas de aceptación para recibir beneficios en distintos grupos o errores de detección en caras humanas que son diferentes dependiendo de la raza. Es importante recordar que la evaluación de los resultados de un sistema de toma/soporte de decisión se realiza tomando en cuenta los objetivos del tomador de decisiones que pueden ser distintos e incluso contradictorios a los objetivos desde el punto de vista del problema de aprendizaje automático. Por ejemplo, un tomador de decisiones podría sacrificar el desempeño global de un modelo para mejorar el desempeño del modelo en un subgrupo, aunque este subgrupo sea pequeño en comparación a la población en su conjunto (por ejemplo, una acción afirmativa para corregir alguna discriminación social existente). Aunque el análisis de las implicaciones éticas en los modelos de aprendizaje automático y la relación que estas tienen con una definición de justicia es aún un campo de estudio abierto, existe una importante literatura que busca implantar definiciones matemáticas de equidad en los modelos para describir su imparcialidad o discriminación entre subgrupos y tomar decisiones que mitiguen resultados no deseados. Términos Atributo protegido: una característica o variable protegida es aquella en que queremos que se cumpla cierto criterio de equidad en las predicciones. En un conjunto de datos podemos tener más de una variable protegida como edad, género, raza, etc. Reto: Definición de justicia algorítmica y equidad algorítmica Aún conociendo el verdadero valor de la variable que queremos predecir, las predicciones de un método dado dependen fuertemente de una variable protegida. En particular, las tasas de error de distintos grupos de la variable protegida pueden ser muy distintos. Lo que se entiende por “justicia” puede cambiar según la cultura y/o tradición de un grupo de población dado y puede ser también específico para un proyecto o problema de política pública. Por ejemplo, para ciertos casos se pueden buscar crear políticas que busquen equidad mediante acciones afirmativas como cuotas de diversidad e inclusión y políticas de reparación, mientras que en otros casos se puede buscar tomar decisiones equitativas bajo argumentos regionales o territoriales. Estos criterios se deben integrar en el proceso de diseño tanto como en el análisis de los datos de entrenamiento, durante la evaluación de los errores, así como en el resultado de las clasificaciones. La implementación de este proceso se puede separar en dos etapas importantes: Justicia algorítmica: Representación matemática de una definición de justicia específica que se incorpora en el proceso de ajuste y selección de modelo. Es importante tomar en cuenta que estas definiciones pueden ser excluyentes, es decir satisfacer una podría implicar no satisfacer las demás (Verma and Rubin (2018)). Inequidad algorítmica: Fallas técnicas en los modelos que producen disparidad de resultados para grupos protegidos que deben de evaluarse bajo la definición de justicia algorítmica determinada en el punto anterior (podría ser más de una). En el caso de clasificación binaria, cuando una de las alternativas es deseable para los individuos (por ejemplo, calificar para un beneficio, crédito, candidatura de un trabajo, etc.), esta situación de inequidad muchas veces implica que la estructura predictiva dependa (Hardt, Price, and Srebro 2016) de la información que contiene la variable protegida acerca de la variable respuesta, con el riesgo de producir sesgos injustos para este sub grupo. El objetivo del modelador es establecer lineamientos para evitar que deficiencias en los modelos produzcan disparidades indeseables según los distintos subgrupos asociados a una variable protegida (por ejemplo, género, raza o nivel de marginación). Para ello, es necesario seleccionar de antemano una definición de justicia algorítmica. Algunas de las más utilizadas son las siguientes (aunque pueden definirse otras dependiendo del problema particular y los objetivos de los tomadores de decisiones): a) Omisión de variables y Paridad demográfica Dos estrategias no muy útiles para prevenir disparidades entre los grupos de \\(A\\) son: ignorar la variable \\(A\\) y buscar paridad demográfica de predicciones, ya sea en términos proporcionales o absolutos. En el primer caso, se pretende eliminar la posibilidad de disparidad no incluyendo la variable \\(A\\) en el proceso de construcción de predictores. Este enfoque no resuelve el problema ya que: Típicamente existen otros atributos asociados a \\(A\\) que pueden producir resultados similares en el entrenamiento aunque \\(A\\) no se considere (por ejemplo, zona geográfico o código postal y nivel socioeconómico). Puede haber razones importantes para incluir \\(A\\) en los modelos predictivos. Por ejemplo, en el caso de presión arterial, existe variaciones en los grupos raciales (\\(A\\)) en cuanto a predisposición a presión alta (Lackland (2014)), por lo tanto un modelo que evalúe riesgo sería más preciso y adecuado si incluye la variable \\(A\\). En el segundo caso, en paridad demográfica de predicciones se busca que las predicciones de los distintos grupos de \\(A\\) sean similares: en el caso de clasificación, por ejemplo, que la tasa de positivos sea similar. Esto es poco deseable por sí solo: por ejemplo, si quisiéramos construir un clasificador para cierta enfermedad, consideramos que es posible que mujeres y hombres sean afectados de manera distinta. Sin embargo, la paridad demográfica puede ser un objetivo de los tomadores de decisiones, y eso debe tomarse en cuenta al momento de tomar la decisión asociada a la predicción. b) Equidad de posibilidades El concepto de equidad de posibilidades ((Hardt, Price, and Srebro 2016)) es uno menos dependiente de los objetivos de los tomadores de decisiones, y se refiere al desempeño predictivo a lo largo de distintos grupos definidos por \\(A\\). Si \\(Y\\) es la variable que queremos predecir, y \\(\\hat{Y}\\) es nuestra predicción, decimos que nuestra predicción satisface equidad de posibilidades cuando \\(\\hat{Y}\\) y \\(A\\) son independientes dado el valor verdadero \\(Y\\) Esto quiere decir que \\(A\\) no debe influir en la predicción cuando conocemos el valor verdadero \\(Y\\), o dicho de otra manera: la pertenencia o no pertenencia al grupo protegido \\(A\\) no debe influir en la probabilidad de la clasificación. Se considera entonces que predictores que se alejan mucho de este criterio son susceptibles de incluir disparidades asociadas a la variable protegida A. Una implicación de este criterio es: - Bajo el supuesto de equidad de posibilidades, las tasas de error predictivo sobre cada subgrupo de \\(A\\) son similares y para clasificación binaria las tasas de falsos positivos y de falsos negativos son similares Ejemplo Supongamos que se quiere crear un sistema para la selección de beneficiarios a una beca escolar para una universidad reconocida. La institución define como como variable protegida la pertenencia a una comunidad indígena (que supondremos en este caso toma dos valores: se autodenomina indígena o no se autodenomina indígena). El predictor satisface equidad de posibilidades cuando tanto la tasa de falsos positivos como la de falsos negativos son iguales para personas indígenas como personas que no lo son. c) Justicia contrafactual: Esta medida considera que un predictor es “justo” si su resultado sigue siendo el mismo cuando se toma el valor del atributo protegido y se cambia a otro valor posible del atributo protegido (como por ejemplo introducir un cambio de raza, género u otra condición). En la práctica no existe una respuesta única ni una medida de justicia algorítmica que funcione para todos los problemas y en la mayoría de los casos buscar el cumplimiento de una implica no cumplir totalmente con las demás, por lo que su elección se debe de hacer en el contexto del problema y se deben justificar sus razones. Equidad de oportunidad muchas veces es un criterio aceptable, que introduce criterios de justicia algorítmica permitiendo también optimizar otros resultados deseables. Medidas: definición de justicia algorítmica (Cualitativa) Identificar grupos o atributos protegidos. Por ejemplo: edad, género, raza, nivel de marginación, etc. (Cualitativa) ¿Se definió con expertos y tomadores de decisiones la medida de justicia algorítmica a usarse en el proyecto? (Cuantitativo) ¿Se ha comprobado la equidad de los resultados del modelo con respecto a los diferentes grupos de interés? Medidas de mitigación: inequidad algorítmica (Cuantitativa) Cuando existen atributos protegidos, debe evaluarse qué tanto se alejan las predicciones de la definición de justicia algorítmica elegida. (Cuantitativa) Postprocesar adecuadamente las predicciones, si es necesario, para lograr el criterio de justicia algorítmica elegido (e.g., equidad de posibilidades, oportunidad). (Cuantitativa) En el caso de clasificación, puntos de corte para distintos subgrupos pueden ajustarse para lograr equidad de oportunidad. (Cuantitativa) Recolectar información más relevante de subgrupos protegidos (tanto casos como características) para mejorar el desempeño predictivo en grupos minoritarios. Esto en general implica que además de la decisión tomada en función de las predicciones depende de esta métrica adicional de equidad, y no solo del análisis costo-beneficio. Actividad: Al terminar esta fase se recomienda el llenado de las secciones de Desarrollo de Modelo del Perfil de Modelo y llevar a cabo una discusión con el tomador de decisiones de políticas públicas. Este fenómeno está relacionado con el sobreajuste que se verá más adelante.↩ "],
["uso-y-monitoreo.html", "Sección 4 Uso y Monitoreo 4.1 Degradación de desempeño 4.2 Experimentos y recopilación de datos", " Sección 4 Uso y Monitoreo Una vez que los métodos de aprendizaje automático se comienzan utilizar para tomar decisiones, es necesario: Monitorear, en general, desempeño y atributos usadas en el tiempo. Monitorear en particular resultados indeseables que pueden resultar de la interacción de usuarios con algoritmos. Tomar decisiones acerca del proceso generador de datos para mejorar desempeño o evaluar resultados. 4.1 Degradación de desempeño Reto: degradación de predicciones El desempeño de un modelo puede degradarse con el tiempo por múltiples razones. Los modelos de ML que asumen una relación estática entre las variables de entrada y de salida pueden degradar la calidad de sus predicciones por cambios en las relaciones subyacentes del contexto de estudio. También se puede deber a un cambio en la calidad de datos por la forma de recolección o incluso redefiniciones metodológicas utilizadas para recolectar información. Por ejemplo, en registros administrativos un ministerio o secretaría podría cambiar los procesos de recolección de datos, digitalizar sistemas, sistematizar limpieza o procesamiento que haga que el aprendizaje de un sistema ya no sea relevante. También esto ocurre en sistemas interactivos donde el sistema y sus usuarios forman un ciclo de realimentación cerrado, con lo cual el sistema se va degradando ya que los usuarios sólo pueden interactuar con elementos que son decididos por el sistema. Para mitigar estos posibles errores es necesario monitorear el comportamiento de las variables de entrada y actualizar supuestos con tomadores de decisión y conocimiento experto. También se debe vigilar el comportamiento de las métricas de error en el tiempo. Cantidades con tasa total de positivos y negativos (incluyendo desagregaciones por otras variables protegidas o de interés), distribución de predicciones y atributos. Medidas: degradación de predicciones [x] Degradación de desempeño: ¿Existe un plan para monitorear el desempeño del modelo y la recolección de información a lo largo del tiempo? (Cuantitativa) Monitorear varias métricas asociadas a las predicciones, en subgrupos definidos con antelación (incluyendo variables protegidas). (Cuantitativa) Monitorear deriva en distribuciones de características con respecto al conjunto de entrenamiento. (Cuantitativa) Monitorear cambios en la metodología de levantamiento y procesamiento de datos que pueden reducir calidad de las predicciones. (Cuantitativa) Idealmente, planear para recolectar datos de la variable no observada para reajustar modelos y mantener el desempeño. (Cualitativa) Cuando sea aplicable y factible, una fracción de las predicciones deberán ser examinadas por humanos y calificadas según alguna rúbrica o mediciones de las variables que se busca predecir. (Cuantitativa) Reentrenar periódicamente los modelos agregando nuevos datos. ``` 4.2 Experimentos y recopilación de datos La forma y los datos que se recopilan para el mantenimiento de los algoritmos de predicción debe planearse con el objeto de mejorar en lo posible, y entender mejor las consecuencias del uso de los modelos. Reto: evaluación de efectividad Las mejoras que esperamos en el proceso pueden ser dificíles de evaluar sin contrafactuales sólidos. Pruebas con diseño experimental pueden planearse (pruebas de tipo A/B, o ver por ejemplo (Vaver and Koehler 2011)), cuando sea posible, para entender qué cambios particulares, deseables o indeseables, introduce el uso de los modelos. Medidas: evaluación de efectividad (Cuantitativa) Cuando sea posible, planear asignar bajo diseños experimentales tratamientos al azar (o según el status-quo) a algunas unidades. Hacer comparaciones de desempeño y comportamiento entre esta muestra y los resultados bajo el régimen algorítmico. Actividad: Al terminar esta fase se recomienda el llenado de la sección de Uso y monitoreo del Perfil de Modelo y llevar a cabo una discusión con el tomador de decisiones de políticas públicas. "],
["rendición-de-cuentas.html", "Sección 5 Rendición de cuentas 5.1 Interpretabilidad y explicación de predicciones. 5.2 Explicabilidad de predicciones individuales 5.3 Trazabilidad", " Sección 5 Rendición de cuentas 5.1 Interpretabilidad y explicación de predicciones. Es difícil dar una definición técnica de interpretabilidad o explicabilidad, que en general se refieren a hacer inteligible para humanos el funcionamiento de un algoritmo y sus resultados ((Molnar 2019), (Miller 2019)). Hay varias razones por las que tener cierto grado de interpretabilidad en los modelos que se usan para tomar decisiones es importante (Molnar (2019)): Aprendizaje acerca del dominio del problema. Aspectos legales y aceptación social (tomadores de decisiones y afectados). Detección de sesgos potenciales de los algoritmos. Depuración y mejora de modelos. Los puntos 1 y 2 son un campo aún abierto en ML, y que cualquier técnica aplicada a interpretar los modelos con estos dos fines tienen varias dificultades que superar. El aprendizaje automático, como se usa típicamente hoy en día, difícilmente se acerca a explicaciones causales. El punto 3 y 4, que se exploran más abajo, son más susceptibles de análisis técnico. Los sesgos potenciales pueden ocurrir cuando en el proceso de aprendizaje se aprenden de características que son irrelevantes, pero caracterizan a los conjuntos de entrenamiento y validación/prueba que fueron utilizados. Sesgos potenciales pueden ocurrir cuando se usan características o variables de los datos que, aunque válidos para un momento y conjunto de datos dado, son susceptibles de cambiar fácilmente con intervenciones en el proceso generador de datos. Ejemplos pueden ser el uso de variables que están siendo influenciadas activamente por alguna política particular que no continuará en el futuro, o aprendizaje de características particulares de un conjunto de entrenamiento no exhaustivo (por ejemplo, en reconocimiento de imágenes reconocer especies animales por el contexto en el que se recolectó la información: zoológico, trampa cámara, paisaje, etc.). mágenes reconocer especies animales por el contexto en el que se recolectó la información: zoológico, trampa cámara, etc). Reto: sesgos potenciales y depuración Algoritmos o métodos predictivos utilizan atributos poco relevantes, con validez temporal, más adelante pueden dañar el desempeño conforme observamos desplazamiento en datos futuros. Algoritmos o métodos predictivos que usan una gran cantidad de atributos poco importantes tienen más riesgo de fallar tanto explícitamente como de forma silenciosa cuando las fuentes de datos o los procesos generadores de datos cambian. Ambas fallas pueden ser difíciles de diagnosticar y depurar, y las fallas silenciosas representan riesgos adicionales. Este tipo de sesgo es difícil de detectar, pero principios de parsimonia y conocimiento experto pueden mitigar su riesgo: Medidas de mitigación: sesgos potenciales y depuración (Cualitativa)¿Se han comunicado las deficiencias, limitaciones y sesgos del modelo a las partes interesadas de manera que estos se consideren en la toma/soporte de decisiones? (Cualitativa)¿Se han tomado medidas para identificar y prevenir los usos no intencionados y el abuso del modelo y tenemos un plan para monitorear estos una vez que el modelo sea desplegado? (Cuantitativa) ¿Se han definido un plan de respuesta en caso de que algún usuario se vean perjudicado por los resultados? (Cualitativa) Incluir todas las características disponibles para construir modelos aumenta el riesgo de que esto suceda. Las variables por incluirse en el proceso de aprendizaje deben tener algún sustento teórico o explicación de por qué pueden ayudar en la tarea de predicción. (Cuantitativa) Métodos más parsimoniosos, que usan menos características son preferibles a modelos que utilizan muchas características. (Cuantitativa) Métodos como gráficas de dependencia parcial ((Friedman 2001)) o importancia basada en permutaciones ((Breiman 2001), (Molnar 2019)) pueden señalar variables problemáticas que reciben mucho peso en la predicción, en contra de observaciones pasadas o conocimiento experto. 5.2 Explicabilidad de predicciones individuales Reto: Explicaciones individuales Existe en muchos casos la necesidad legal y/o ética de dar explicaciones individuales de cómo fueron tomadas ciertas decisiones (por ejemplo, por qué a una persona no se le otorgó un crédito, o por qué alguien no califica para un programa social).14 En áreas de investigación como visión artificial y procesamiento del lenguaje natural, las implementaciones más exitosas suelen estar desarrolladas con modelos de alta complejidad, como redes neuronales profundas, que son en principio poco transparentes en cuanto a cómo se hacen las predicciones subyacentes (Carrillo (2020)). Existen varias formas de explicar predicciones (Molnar (2019)). Pueden utilizarse métodos como el de explicaciones contrafactuales (Wachter, Mittelstadt, and Russell (2017)), valores de Shapley (Lundberg and Lee (2017)) o gradientes integrados para redes profundas (Sundararajan, Taly, and Yan (2017)). Medidas: explicaciones individuales (Cualitativo) ¿Se analizaron los requerimientos legales y éticos de explicabilidad e interpretabilidad necesarios para el caso de uso? .(Cualitativo) ¿Se han definido un plan de respuesta en caso de que algún usuario se vea perjudicado por los resultados? (Cualitativo) ¿Existe algún proceso para dar explicaciones a un individuo en particular sobre por qué se tomó una decisión? (Cualitativo) ¿Se discutieron los pros y contras de los algoritmos según su nivel de interpretabilidad y explicabilidad para elegir el más apropiado? (Cuantitativo) Para modelos más simples (por ejemplo, lineales o árboles de decisión), pueden construirse explicaciones ad-hoc. (Cuantitativo) Utilizar métodos como: explicaciones contrafactuales, valores de Shapley o gradientes integrados para redes profundas. 5.3 Trazabilidad Un proceso de datos a decisiones que es poco trazable es uno que contiene pasos con documentación deficiente acerca de su ejecución: incluyen procesos manuales o decisiones de operadores pobremente especificados, extraen datos de fuentes no documentadas o no accesibles, omiten códigos o materiales necesarios, o no explican los ambientes de cómputo para garantizar resultados reproducibles. Reto: trazabilidad Cuando existe poca trazabilidad, todos los riesgos mencionados en este documento pueden ser difíciles de identificar, y generalmente se exacerban. Por el contrario, en un proyecto trazable todos los pasos de datos a decisiones están claramente documentados y especificados sin ambigüedad. Medidas: trazabilidad (Cuantitativa) ¿Está bien documentado el proceso de ingesta, transformación, modelado y toma de decisión? (incluyendo fuente de datos, infraestructura y dependencias, código, métricas e interpretación de resultados) Fuentes de datos (localizadas en el tiempo y en su repositorio), procesos de extracción y su preprocesamiento. Código completo y documentado apropiadamente, definiendo librerías necesarias, versiones de las mismas e infraestructura, de forma que un tercero pueda entender el propósito de cada parte de ese código. Cómo debe ejecutarse el código para obtener los insumos de la decisión, incluyendo documentación de parámetros de cada ejecución, y documentación detallada acerca del ambiente de cómputo. Todo esto debe garantizar reproducibilidad de los resultados originales por un tercero. Cómo se utilizaron e incluyeron resultados del proceso computacional en el proceso de toma de decisiones. Documentación acerca del diseño del monitoreo, incluyendo detalles acerca de cuáles son los comportamientos previstos y cuáles requieren acciones de mitigación. Idealmente, estos pasos deberían poder ejecutarlos un tercero con intervención mínima de los creadores y operadores originales. Actividad: Al terminar esta fase se recomienda el llenado de la sección de Rendición de cuentas del Perfil de Modelo y llevar a cabo una discusión con el tomador de decisiones de políticas públicas. En la Unión Europea, por ejemplo, el Artículo 22 de GDPR describe el derecho de una persona a rebatir la decisión de un sistema, especialmente cuando es automática.↩ "],
["herramientas.html", "Herramientas Herramienta 1: Lista de verificación (Checklist) de IA robusta y responsable Herramienta 2: Perfil de Datos Herramienta 3: Perfil del Modelo", " Herramientas Para acompañar el ciclo de vida de IA se proponen las siguientes tres herramientas que deberán contestarse a lo largo del desarrollo del sistema, la lista de verificación (Checklist), el perfil de Datos y el perfil de Modelo. Ilustración 5.1: Retos técnicos del ciclo de vida del ML. Construcción propia. Herramienta 1: Lista de verificación (Checklist) de IA robusta y responsable Herramienta que consolida las principales preocupaciones por dimensión de riesgo del ciclo de vida de IA. El checklist debe revisarse de forma continua por el equipo técnico acompañado por el tomador de decisiones (Fritzler 2015), (DrivenData 2019). Conceptualización y diseño de políticas públicas ☑ Definición correcta del problema y de la respuesta de política pública: (Cualitativo)¿Se definió claramente el problema de política pública que se está buscando resolver? (Cualitativo) Describa cómo se da respuesta a este problema actualmente, considerando a las instituciones relacionadas y cuál es su propuesta para solucionar dicho problema usando IA. (Cualitativo) ¿Se definieron las acciones o intervenciones que se realizarán a partir del resultado del sistema de Inteligencia Artificial? ☑ Necesidad y proporcionalidad (Cualitativo) ¿Afectará su proyecto de forma directa o indirecta a la vida de personas o de personas o grupos vulnerables? (Cualitativo) Para la implementación de estas tecnologías, ¿se han revisado casos de proyectos similares anteriores? (Cualitativa) ¿Se identificaron los distintos grupos o atributos protegidos dentro del proyecto? (por ejemplo: edad, género, raza, nivel de marginación, etc.) Gobernanza y seguridad ☑ Consentimiento informado y límites de privacidad: (Cuantitativa) ¿Se ha definido de forma clara la finalidad del tratamiento de los datos que va a recopilar y/o procesar? (Cuantitativa) ¿Su proyecto cuenta con los acuerdos legales de transferencia de información necesarios? ☑ Gobernanza y ciberseguridad (Cuantitativa) El Proyecto cuenta con estructuras y mecanismos concretos de gobernanza y ciberseguridad? (Cuantitativa) ¿Se han considerado formas de reducir al mínimo la exposición de información personal identificable? (por ejemplo, mediante la anonimización o la no recopilación de información que no sea pertinente para el análisis) Fuente y manejo de datos ☑ Calidad de datos recolectados y relevancia de variable respuesta: (Cualitativa) ¿Se ha justificado claramente la utilización de la variable respuesta seleccionada bajo los propósitos de la intervención? (Cuantitativa) ¿Se han examinado los datos en busca de posibles sesgos históricos o estados indeseables que no queramos replicar? ☑ Muestras naturales y diseñadas: (Cualitativa) ¿Se ha analizado las posibles diferencias entre la base de datos y la población para la que se busca desarrollar el sistema de IA? (Utilizar literatura relacionada con el tema, información de expertos. Estudiar en particular sesgos de selección no medidos) ☑ Atributos incompletos (Cualitativa) ¿Se ha realizado un análisis de valores faltantes y de variables omitidas? (Cualitativa) ¿Se ha identificado si existen variables omitidas importantes para las cuales no se tiene mediciones asociadas? (Cualitativa) ¿Se ha identificado las razones por las que existen observaciones faltantes? (En caso de existir) ☑ Comparación Causal (Cualitativa) ¿Se manifestó de forma explícita las limitaciones de los resultados al tomador de decisiones de política pública? (En caso de que no se haya trabajado en asegurar causalidad en los resultados) Desarrollo de modelos ☑ Ausencia y errores de validación: (Cuantitativa) ¿Se construyeron las muestras de validación y prueba adecuadamente? Considerando un tamaño apropiado, cubriendo a subgrupos de interés y protegidos y evitando fugas de información durante su implementación. ☑ Métricas de evaluación engañosas: (Cualitativa) ¿Se cuestionaron las implicaciones de los diferentes tipos de errores para el caso de uso específico, así como la forma correcta de evaluarlos? (Cualitativa) ¿Se explicó de forma clara las limitantes del modelo? Identificando tanto los falsos positivos como falsos negativos y las implicaciones que una decisión del sistema tendría en la vida de la población beneficiaria. (Cuantitativa) ¿Se implementó un análisis costo-beneficio del sistema contra el status quo u otras estrategias de toma/soporte de decisión? (Cuando es posible) ☑ Errores no cuantificados y evaluación humana: (Cualitativa) Se realizó una evaluación humana con expertos del caso de uso para buscar sesgos o errores conocidos? (Por ejemplo, se pueden usar paneles de revisores que examinen predicciones particulares y consideren si son razonables o no. Estos paneles deben ser balanceados en cuanto al tipo de usuarios que se prevén, incluyendo tomadores de decisiones si es necesario). ☑ Equidad y desempeño diferencial: (Cualitativa) ¿Se definió con expertos y tomadores de decisiones la medida de justicia algorítmica a usarse en el proyecto? (Cuantitativo) ¿Se ha comprobado la equidad de los resultados del modelo con respecto a los diferentes grupos de interés? Uso y monitoreo ☑ Degradación de desempeño: (Cualitativo) ¿Existe un plan para monitorear el desempeño del modelo y la recolección de información a lo largo del tiempo? (Cualitativa) ¿Se han tomado medidas para identificar y prevenir los usos no intencionados y el abuso del modelo y tenemos un plan para monitorear estos una vez que el modelo sea desplegado? Rendición de cuentas ☑ Sesgos potenciales y depuración (Cualitativa) ¿Se han comunicado las deficiencias, limitaciones y sesgos del modelo a las partes interesadas de manera que estos se consideren en la toma/soporte de decisiones? ☑ Interpretabilidad y explicabilidad (Cualitativo) ¿Se analizaron los requerimientos legales y éticos de explicabilidad e interpretabilidad necesarios para el caso de uso? (Cualitativo) ¿Se discutieron los pros y contras de los algoritmos según su nivel de interpretabilidad y explicabilidad para elegir el más apropiado? (Cualitativo) ¿Se han definido un plan de respuesta en caso de que algún usuario se vea perjudicado por los resultados? (Cualitativo) ¿Existe algún proceso para dar explicaciones a un individuo en particular sobre por qué se tomó una decisión? ☑ Transparencia y trazabilidad (Cuantitativa) ¿Está bien documentado el proceso de ingesta, transformación, modelado y toma de decisión? (incluyendo fuente de datos, infraestructura y dependencias, código, métricas e interpretación de resultados) Herramienta 2: Perfil de Datos El perfil de datos es un análisis exploratorio que brinda información para evaluar la calidad, integridad, temporalidad, consistencia, posibles sesgos de un conjunto de datos que será utilizado para entrenar un modelo de aprendizaje automático (Gebru et al. 2020). Descripción general de datos y motivación Nombre de conjunto de datos usados ¿Qué institución creó el dataset? ¿Con qué objetivo la institución creó el dataset utilizado? ¿Qué mecanismos o procedimientos se utilizaron para recopilar los datos (por ejemplo, encuesta en vivienda, sensor, software, API)? ¿Número de individuos presentes en los datos? Frecuencia de captura (semanal, mensual, diaria) o ¿Número de observaciones promedio por individuo? ¿Se actualizará el conjunto de datos (por ejemplo, añadir nuevas instancias, eliminar instancias)? Controles esenciales: Obtener documentación para cada variable dentro del conjunto de datos. breve descripción que incluya su nombre y tipo, lo que representa, cómo se mide su valor, etc. Realizar un análisis exploratorio de los datos calcular estadísticas descriptivas identificando porcentaje de valores faltantes y distribución de cada variable dentro de la base de datos. Analizar cobertura espacial y temporal de los datos. Analizar cobertura de grupos protegidos (sexo, raza, edad, etc.) Describir las dimensiones importantes en las cuales la muestra de datos puede ser diferente a la población en particular sesgos de selección no medidos. Utilizar literatura relacionada con el tema, información de expertos. Identificar posibles “estados indeseables” en los datos, pueden ser sesgos e inequidades perjudiciales para subgrupos, pero también puede ser cualquier otro patrón que se considere subóptimo o no deseable desde un punto de vista de política social. ¿Existen valores faltantes? En caso afirmativo, explique las razones por las que no se tiene esa información. (Esto incluye información eliminada intencionalmente) Identificar razones por las que existen datos faltantes y piense si la falta de datos está asociada con la variable a predecir. Herramienta 3: Perfil del Modelo La rúbrica presentada aquí, es una tarjeta de seguimiento que resume las características principales de un sistema de toma/soporte de decisiones basado en ML y destaca los principales supuestos, las características más importantes del sistema y las medidas de mitigación implementadas (Mitchell et al. 2018). Conceptualización y diseño de política pública Información básica Personas que desarrollaron el modelo, fecha, versión, tipo Casos de uso Antecedentes Población objetivo y horizonte de predicciones Actores y componentes que interactuarán con los resultados Casos de uso considerados durante el desarrollo Usos no considerados y advertencias relacionadas Definición de grupos protegidos Fuente y manejo de datos Datos de entrenamiento Conjunto de datos usados y su etiquetado Pasos de preprocesamiento o preparación de datos. Sesgos y deficiencias potenciales según el caso de uso (2) Desarrollo de Modelo Modelación Algoritmos que se usaron para entrenar, parámetros supuestos o restricciones Métricas de desempeño Métricas técnicas usadas para seleccionar y evaluar modelos Análisis costo-beneficio del modelo para su caso de uso según (2) Definición de grupos protegidos y medidas de equidad seleccionadas Datos de validación Conjuntos de datos usados y su etiquetado Pasos de preprocesamiento Evaluación de adaptación de datos de validación según el caso de uso (2) Sesgos y deficiencias potenciales según el caso de uso (2) Resumen de análisis cuantitativo Error de validación reportado Resumen de análisis costo-beneficio Reporte de medidas de equidad para grupos protegidos Uso y monitoreo Recomendaciones de monitoreo Estrategia de monitoreo y mejora en producción Estrategias de monitoreo humano de predicciones (si aplica) Rendición de cuentas (Opcional) Explicabilidad de predicciones Estrategia para explicar predicciones particulares (si es necesario) Estrategia para entender la importancia de distintos atributos Otras consideraciones éticas, recomendaciones y advertencias La siguiente página muestra estas herramientas completadas para un caso de uso ejemplo. "],
["cuadernillos-de-trabajo.html", "Cuadernillos de trabajo Mala correspondencia de métrica y objetivos Muestras naturales y sesgo Desarrollo de los modelos Fugas Entrenamiento Validación Fugas en implementación Evaluación de punto de corte Desbalance de clases Equidad con atributos protegidos Interpretabilidad Explicación de predicciones", " Cuadernillos de trabajo En esta sección se muestran varios ejemplos de los retos y soluciones explicadas en el documento principal. Se utilizan distintos tipos de modelos (lineales, basados en árboles y otros) y distintas implementaciones (R, keras, xgboost) para mostrar que estos problemas se presentan independientemente de la elección de herramientas particulares. Los cuadernillos utilizan notación con punto decimal para mantener consistencia con los paquetes que así lo usan. Se utiliza el lenguaje de programación R, y los siguientes paquetes: Wickham (2017), Max Kuhn and Wickham (2020), Hvitfeldt (2020), Max Kuhn, Chow, and Wickham (2020), Max Kuhn and Vaughan (2020a), Max Kuhn and Vaughan (2020b), Vaughan (2020), Max Kuhn (2020), Xie (2019), Pedersen (2019). Todo el material es reproducible según instrucciones en este repositorio. El repositorio contiene un archivo Dockerfile que describe las dependencias de infraestructura para su replicación. library(tidyverse) library(recipes) library(themis) library(rsample) library(parsnip) library(yardstick) library(workflows) library(tune) library(knitr) library(patchwork) Mala correspondencia de métrica y objetivos Usar modelos que predicen la métrica incorrecta puede llevar a tomar decisiones incorrectas. A veces el problema es claro, cuando la métrica sustituto tiene deficiencias obvias, y en otras puede ser más sutil. En el ejemplo que se muestra a continuación se busca predecir la demanda de cierto producto (pensemos en vacunas o alguna medicina) para poder tomar decisiones de abastecimiento. Se cuenta con datos históricos de inventario (80 semanas), ventas y una variable predictor asociada a ventas (en el caso de las vacunas podría ser temperatura) y otra de agotamiento del inventario. Separamos los datos en entrenamiento y prueba, ajustando el modelo con el subconjunto de datos de entrenamiento. En este caso se utiliza un modelo lineal con variable dependiente ventas y covariables de semana y la covariable predictor. entrena &lt;- ventas %&gt;% filter(semana &lt; 60) prueba &lt;- ventas %&gt;% filter(semana &gt;= 60, semana &lt;= 80) entrena %&gt;% select(-demanda) %&gt;% head() %&gt;% kable() semana inventario ventas predictor agotamiento 1 153 110 -27.7014124 0 2 170 148 0.7664636 0 3 158 130 -15.2606032 0 4 162 142 4.2461227 0 5 159 159 28.5107593 1 6 162 162 14.8895964 1 mod_lineal &lt;- lm(ventas ~ semana + predictor, data = ventas) mod_lineal ## ## Call: ## lm(formula = ventas ~ semana + predictor, data = ventas) ## ## Coefficients: ## (Intercept) semana predictor ## 140.9935 0.8166 0.5535 Evaluamos el error de predicción. preds &lt;- predict(mod_lineal, newdata = prueba) round(mean(abs(preds - prueba$ventas))/mean(prueba$ventas), 3) ## [1] 0.04 El error porcentual es bajo. Los datos ajustados y predicciones se ven como sigue: preds &lt;- predict(mod_lineal, newdata = ventas) ventas_larga &lt;- ventas %&gt;% mutate(pred = preds) %&gt;% pivot_longer(cols = all_of(c(&quot;ventas&quot;,&quot;pred&quot;)), names_to = &quot;tipo&quot;, values_to = &quot;unidades&quot;) ggplot(ventas_larga %&gt;% mutate(unidades = ifelse(tipo==&quot;ventas&quot; &amp; semana &gt; 80, NA, unidades)), aes(x = semana, y = unidades, group = tipo, colour = tipo)) + geom_line() + geom_vline(xintercept = 80) + geom_vline(xintercept = 60) + annotate(&quot;text&quot;, x = 25, y=105, label = &quot;entrena&quot;) + annotate(&quot;text&quot;, x = 69, y=105, label = &quot;prueba&quot;) ## Warning: Removed 20 row(s) containing missing values (geom_path). Pero tomar decisiones de demanda o inventario es equivocado. La razón es que existe una diferencia entre la variable ideal (demanda real de medicinas) y la variable observada (venta de medicinas). La diferencia radica en que existen agotamientos de inventario, es decir periodos en los que, aunque existía demanda no había suficiente inventario para todos los compradores. Se ve marcado esto con rojo en la siguiente gráfica. preds &lt;- predict(mod_lineal, newdata = ventas) ventas_larga &lt;- ventas %&gt;% mutate(pred = preds) %&gt;% pivot_longer(cols = all_of(c(&quot;ventas&quot;,&quot;pred&quot;)), names_to = &quot;tipo&quot;, values_to = &quot;unidades&quot;) ggplot(ventas_larga %&gt;% mutate(unidades = ifelse(tipo==&quot;ventas&quot; &amp; semana &gt; 80, NA, unidades)), aes(x = semana)) + geom_line(aes(group = tipo, colour = tipo, y = unidades)) + geom_point(data = filter(ventas, agotamiento==1, semana &lt; 80), aes(y = ventas), colour = &quot;red&quot;) + geom_vline(xintercept = 80) + geom_vline(xintercept = 60) + annotate(&quot;text&quot;, x = 25, y=105, label = &quot;entrena&quot;) + annotate(&quot;text&quot;, x = 69, y=105, label = &quot;prueba&quot;) ## Warning: Removed 20 row(s) containing missing values (geom_path). Si usáramos la política sugerida por las predicciones (por ejemplo 5% más), veríamos las ventas de la primera gráfica a continuación. Sin embargo, si usáramos una política de inventario con 280 unidades, observaríamos: preds &lt;- predict(mod_lineal, newdata = ventas) ventas_obs &lt;- ventas %&gt;% mutate(pred = preds) %&gt;% mutate(inventario = 1.05 * pred) %&gt;% mutate(ventas = ifelse(semana &gt; 80, pmin(inventario, demanda), ventas)) ventas_larga &lt;- ventas_obs %&gt;% pivot_longer(cols = all_of(c(&quot;ventas&quot;,&quot;pred&quot;)), names_to = &quot;tipo&quot;, values_to = &quot;unidades&quot;) g1 &lt;- ggplot(ventas_larga, aes(x = semana)) + geom_line(aes(group = tipo, colour = tipo, y = unidades)) + geom_point(data = filter(ventas_obs, ventas == inventario, semana &gt; 80), aes(y = ventas), colour = &quot;red&quot;) + geom_vline(xintercept = 80) + labs(subtitle = &quot;Inventario: Predicciones + 5%&quot;) preds &lt;- predict(mod_lineal, newdata = ventas) ventas_obs &lt;- ventas %&gt;% mutate(pred = preds) %&gt;% mutate(inventario = 280) %&gt;% mutate(ventas = ifelse(semana &gt; 80, pmin(inventario, demanda), ventas)) ventas_larga &lt;- ventas_obs %&gt;% pivot_longer(cols = all_of(c(&quot;ventas&quot;,&quot;pred&quot;)), names_to = &quot;tipo&quot;, values_to = &quot;unidades&quot;) g2 &lt;- ggplot(ventas_larga, aes(x = semana)) + geom_line(aes(group = tipo, colour = tipo, y = unidades)) + geom_point(data = filter(ventas_obs, ventas == inventario, semana &gt; 80), aes(y = ventas), colour = &quot;red&quot;) + geom_vline(xintercept = 80)+ labs(subtitle = &quot;Inventario: 300 unidades cte&quot;) g1 / g2 Entonces, La política basada en las predicciones exacerba el problema de los agotamientos. Un uso no pensado de datos sin considerar el proceso generador de los mismos puede producir errores grandes en las decisiones En este caso, la confusión proviene de no separar los conceptos de demanda y ventas. Otros indicadores de demanda o modelos más adecuados ayudarían a resolver el problema. Soluciones simplistas como sólo tomar los datos donde no ocurren agotamientos pueden empeorar aún más la situación: incrementan el sesgo (seleccionamos semanas donde las ventas tienden a ser bajas) y reducen la precisión (usamos menos datos). Muestras naturales y sesgo Cuando las muestras de entrenamiento son diferentes a las poblaciones, donde se van a aplicar los modelos, existen dificultades en validar correctamente las predicciones. 5.3.1 Muestras naturales: mala representatividad Para este ejemplo utilizaremos datos de la encuesta nacional de ingresos y gastos en hogares de México (INEGI 2014), para simular un escenario que queremos ejemplificar. set.seed(128) encuesta_ingreso &lt;- read_csv(&quot;datos/enigh-ejemplo.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## FOLIOVIV = col_character(), ## UBICA_GEO..5 = col_character(), ## EST_DIS..134 = col_character(), ## UPM..135 = col_character(), ## TENEN_NR1 = col_character(), ## TENEN_NR2 = col_character(), ## VEHI1_A = col_character(), ## VEHI2_A = col_character(), ## VEHI3_A = col_character(), ## VEHI4_A = col_character(), ## VEHI5_A = col_character(), ## VEHI6_A = col_character(), ## VEHI7_A = col_character(), ## VEHI8_A = col_logical(), ## VEHI9_A = col_character(), ## EQH1_A = col_character(), ## EQH2_A = col_character(), ## EQH3_A = col_character(), ## EQH4_A = col_character(), ## EQH5_A = col_character() ## # ... with 26 more columns ## ) ## See spec(...) for full column specifications. ## Warning: 2 parsing failures. ## row col expected actual file ## 1192 VEHI8_A 1/0/T/F/TRUE/FALSE 07 &#39;datos/enigh-ejemplo.csv&#39; ## 1398 VEHI8_A 1/0/T/F/TRUE/FALSE 07 &#39;datos/enigh-ejemplo.csv&#39; datos_ingreso &lt;- encuesta_ingreso %&gt;% mutate(num_focos = FOCOS) %&gt;% mutate(ingreso_miles = (INGCOR / 1000)) %&gt;% mutate(tel_celular = ifelse(SERV_2 == 1, &quot;Sí&quot;, &quot;No&quot;)) %&gt;% mutate(piso_firme = ifelse(PISOS != 1 | is.na(PISOS), &quot;Sí&quot;, &quot;No&quot;)) %&gt;% mutate(lavadora = ifelse(LAVAD != 1 | is.na(LAVAD), &quot;Sí&quot;, &quot;No&quot;)) %&gt;% mutate(automovil = VEHI1_N &gt; 0) %&gt;% mutate(marginacion = fct_reorder(marginación, ingreso_miles, median)) %&gt;% rename(ocupadas = PEROCU) %&gt;% rename(educacion_jef = NIVELAPROB) %&gt;% select(ingreso_miles, num_focos, tel_celular, marginacion, ocupadas, piso_firme, lavadora, automovil, educacion_jef) ingreso_split &lt;- initial_split(datos_ingreso, prop = 0.7) entrena &lt;- training(ingreso_split) prueba &lt;- testing(ingreso_split) Supóngase que interesa estimar el ingreso de los hogares, para ello se usa una encuesta por teléfono celular, más aún, supóngase que solo se accede a zonas que no tienen marginación muy alta. muestra_sesgada &lt;- filter(entrena, tel_celular == &quot;Sí&quot;, marginacion==&quot;Muy bajo&quot;) sesgados_split &lt;- initial_split(muestra_sesgada) entrena_sesgo &lt;- training(sesgados_split) validacion_sesgo &lt;- testing(sesgados_split) Se construye un modelo lineal para el logaritmo de ingresos con los datos disponibles. library(splines) formula &lt;- as.formula(&quot;log(ingreso_miles) ~ ns(num_focos, 3) + ns(ocupadas, 3) + lavadora + automovil + piso_firme + ns(educacion_jef, 3)&quot;) mod_sesgo &lt;- lm(formula, data = entrena_sesgo) # tomamos una muestra representativa para comparar, del mismo tamaño que la sesgada mod_representativa &lt;- lm(formula, data = sample_n(entrena, nrow(entrena_sesgo))) Y se evalúa el error en una muestra de prueba construida con datos con las mismas características sesgadas que los datos de entrenamiento (hogares con teléfono celular y grado de marginación muy bajo). preds_val &lt;- predict(mod_sesgo, newdata = validacion_sesgo) mean(abs(preds_val - log(1 + validacion_sesgo$ingreso_miles))) %&gt;% round(2) ## [1] 0.37 El error en una muestra más similar a la población que se pretende aplicar el algoritmo es mayor: preds_prueba_sesgo &lt;- predict(mod_sesgo, newdata = prueba) preds_prueba &lt;- predict(mod_representativa, newdata = prueba) prueba$pred_sesgada &lt;- preds_prueba_sesgo prueba$pred_rep &lt;- preds_prueba mean(abs(preds_prueba_sesgo - log(1 + prueba$ingreso_miles))) %&gt;% round(2) ## [1] 0.42 Sin embargo, el principal problema se refleja en la siguiente gráfica, donde usamos escalas logarítmicas para hacer comparaciones multiplicativas, que nos interesan por la naturaleza del ingreso. Cada punto representa un hogar, la muestra es más similar a la población donde se aplicará la metodología, y en el eje horizontal graficamos la predicción de los hogares utilizando el modelo, mientras que el eje vertical corresponde al ingreso de cada hogar. Como referencia agregamos la recta \\(y = x\\), y un suavizador (ver por ejemplo aquí). Nos concetramos en vel el desempeño para los hogares de ingresos relativamente bajos (menos de 10 mil pesos al mes): breaks_y &lt;- c(3, 5, 10, 20, 40, 80) g_sesgo &lt;- ggplot(prueba %&gt;% filter(pred_sesgada &lt; log(30)), aes(x = exp(pred_sesgada), y = ingreso_miles)) + geom_point(alpha = 0.5) + geom_abline() + geom_smooth(method = &quot;loess&quot;, span = 1) + scale_x_log10(limits=c(5, 30)) + scale_y_log10(breaks = breaks_y) + xlab(&quot;Predicción (miles al trimestre)&quot;) + ylab(&quot;Ingreso corriente (miles al trimestre)&quot;) + labs(subtitle = &quot;Desempeño en prueba \\ncon sesgo en entrenamiento&quot;) g_representativa &lt;- ggplot(prueba %&gt;% filter(pred_sesgada &lt; log(30)), aes(x = exp(pred_rep), y = ingreso_miles)) + geom_point(alpha = 0.5) + geom_abline() + geom_smooth(method = &quot;loess&quot;, span = 1) + scale_x_log10(limits = c(5, 30)) + scale_y_log10(breaks = breaks_y) + xlab(&quot;Predicción (miles al trimestre)&quot;) + ylab(&quot;Ingreso corriente (miles al trimestre)&quot;) + labs(subtitle = &quot;Desempeño en prueba \\ncon muestra representativa en entrenamiento&quot;) g_sesgo + g_representativa ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 3 rows containing non-finite values (stat_smooth). ## Warning: Removed 3 rows containing missing values (geom_point). Aunque comunmente esperamos sobrepredecir valores observados relativamente bajos, y lo contrario para valores relativamente altos, para los que tienen ingresos de menos de 10 mil pesos mensuales, el modelo sesgado sobrepredice el ingreso verdadero por alrededor del 40%: prueba_bajo &lt;- prueba %&gt;% filter(ingreso_miles &lt; 3*10) sesgo &lt;- mean(exp(prueba_bajo$pred_sesgada))/mean(prueba_bajo$ingreso_miles) - 1 round(sesgo,3) ## [1] 0.412 Al compararse con el mismo modelo entrenado con una muestra representativa, donde el efecto es considerablemente menor: prueba_bajo &lt;- prueba %&gt;% filter(ingreso_miles &lt; 3*10) sesgo &lt;- mean(exp(prueba_bajo$pred_rep))/mean(prueba_bajo$ingreso_miles) - 1 round(sesgo,3) ## [1] 0.152 Se tienen entonces dos problemas: El sesgo produce un error considerablemente más grande en la implementación que en la validación. Peor aún, el sesgo es mayor para hogares de menores ingresos (las predicciones son altas), lo cual puede producir una focalización mediocre si buscamos identificar hogares de menores ingresos. 5.3.2 Muestras naturales: comparaciones causales Este ejemplo está tomado de (Hastie, Tibshirani, and Friedman (2017) y Rossouw et al. (1983)). Consideramos los siguientes datos donde queremos predecir enfermedad del corazón (chd)15: sa_heart &lt;- read_csv(&quot;datos/sa-heart.csv&quot;) ## Parsed with column specification: ## cols( ## sbp = col_double(), ## tobacco = col_double(), ## ldl = col_double(), ## adiposity = col_double(), ## famhist = col_character(), ## typea = col_double(), ## obesity = col_double(), ## alcohol = col_double(), ## age = col_double(), ## chd = col_double() ## ) sa_heart &lt;- sa_heart %&gt;% rename(presion_arterial = sbp, tabaco = tobacco, colesterol_ldl = ldl, adiposidad = adiposity, historia_fam = famhist, tipo_a = typea, obesidad = obesity, edad = age, enf_coronaria = chd) sa_heart ## # A tibble: 462 x 10 ## presion_arterial tabaco colesterol_ldl adiposidad historia_fam tipo_a ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 160 12 5.73 23.1 Present 49 ## 2 144 0.01 4.41 28.6 Absent 55 ## 3 118 0.08 3.48 32.3 Present 52 ## 4 170 7.5 6.41 38.0 Present 51 ## 5 134 13.6 3.5 27.8 Present 60 ## 6 132 6.2 6.47 36.2 Present 62 ## 7 142 4.05 3.38 16.2 Absent 59 ## 8 114 4.08 4.59 14.6 Present 62 ## 9 114 0 3.83 19.4 Present 49 ## 10 132 0 5.8 31.0 Present 69 ## # … with 452 more rows, and 4 more variables: obesidad &lt;dbl&gt;, alcohol &lt;dbl&gt;, ## # edad &lt;dbl&gt;, enf_coronaria &lt;dbl&gt; library(recipes) set.seed(125) sa_split &lt;- rsample::initial_split(sa_heart, prop = 0.75) sa_split ## &lt;Analysis/Assess/Total&gt; ## &lt;347/115/462&gt; receta_sa &lt;- training(sa_split) %&gt;% recipe(enf_coronaria ~ .) %&gt;% step_dummy(historia_fam) %&gt;% step_mutate(enf_coronaria = factor(enf_coronaria)) %&gt;% prep() sa_entrena &lt;- receta_sa %&gt;% juice sa_boosted &lt;- boost_tree(trees = 3000, mode = &quot;classification&quot;, learn_rate = 0.001, tree_depth = 2, sample_size = 0.5) %&gt;% set_engine(&quot;xgboost&quot;) %&gt;% fit(enf_coronaria ~ ., data = sa_entrena) Se puede evaluar este modelo y afinar parámetros también. Aquí interesa interpretar el efecto de las variables en este modelo. Para eso se considera la gráfica de dependencia parcial de la prevalencia de enfermedad de corazón y la variable obesidad library(pdp) ## ## Attaching package: &#39;pdp&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## partial pdp_ob &lt;- pdp::partial(sa_boosted$fit, pred.var = &quot;presion_arterial&quot;, plot = TRUE, plot.engine = &quot;ggplot2&quot;, prob = TRUE, train = sa_entrena %&gt;% dplyr::select(-enf_coronaria)) pdp_ob + xlab(&quot;Presión arterial sistólica&quot;) + ylab(&quot;Predicción promedio&quot;) ## Warning: Use of `object[[1L]]` is discouraged. Use `.data[[1L]]` instead. ## Warning: Use of `object[[&quot;yhat&quot;]]` is discouraged. Use `.data[[&quot;yhat&quot;]]` ## instead. La interpretación correcta de este gráfica de dependencia parcial (ver Hastie, Tibshirani, and Friedman (2017)) depende del hecho de que este es un estudio retrospectivo, donde algunos pacientes con riesgo de enfermedad de corazón sufrieron intervenciones para reducir su riesgo, entre los que está tomar medicinas para reducir la presión. Una interpretación causal de reducciones de la presión arterial como promotora de enfermedades del corazón es incorrecta y potencialmente peligrosa (ver más en Hastie, Tibshirani, and Friedman (2017)). Desarrollo de los modelos Fugas Entrenamiento Validación Se presentarán varios ejemplos de cómo las fugas de entrenamiento a validación producen estimaciones sesgadas del desempeño de predictores. 5.3.3 Selección de variables antes de dividir los datos Cualquier paso de preprocesamiento debe hacerse sin usar datos de validación. Esto incluye cuando usamos métodos como validación cruzada Este ejemplo es originalmente de (Hastie, Tibshirani, and Friedman 2017), y se utilizará datos sintéticos, generados con el siguiente proceso: Simulando variables respuesta \\(y\\) con distribución binomial, Simulando 1000 covariables independientes, cada una con distribución normal estándar. simular &lt;- function(n = 100, p = 500, prob = 0.5){ datos &lt;- map(1:p, ~ rnorm(n)) %&gt;% bind_cols() datos$y &lt;- rbinom(n, 1, prob) datos } set.seed(8234) datos_entrena &lt;- simular(n = 200, p = 1000) datos_prueba &lt;- simular(n = 2000, p = 1000) dim(datos_entrena) ## [1] 200 1001 datos_entrena %&gt;% group_by(y) %&gt;% tally() %&gt;% kable() y n 0 113 1 87 La selección de variables está dada por la siguiente función. Esta función selecciona las variables más correlacionadas con la variable objetivo. seleccionar &lt;- function(datos, num_var = 10){ correlaciones &lt;- datos %&gt;% pivot_longer(cols = matches(&quot;V&quot;), names_to = &quot;variable&quot;, values_to = &quot;x&quot;) %&gt;% group_by(variable) %&gt;% summarise(corr = abs(cor(y, x))) %&gt;% arrange(desc(corr)) # seleccionar seleccionadas &lt;- correlaciones %&gt;% top_n(num_var, wt = corr) %&gt;% pull(variable) datos %&gt;% select(one_of(c(&quot;y&quot;, seleccionadas))) } Método erróneo Aquí se ven las 10 variables que fueron seleccionadas, Por sí solo este método no es incorrecto, pero cuando se ejecuta sobre los datos que se usarán en validación (validación cruzada), entonces la estimación de desempeño es optimista: datos_filtrados &lt;- seleccionar(datos_entrena) datos_filtrados %&gt;% head %&gt;% mutate_if(is.numeric, round, 3) %&gt;% kable() y V337 V464 V984 V461 V525 V732 V39 V774 V491 V682 0 1.592 -0.587 1.763 -0.847 0.452 -0.604 -0.400 -1.146 -0.938 0.136 0 1.782 0.604 0.739 -0.533 1.752 0.945 1.142 -0.638 -0.342 -1.308 0 1.528 0.635 -0.326 0.734 -0.207 -0.974 1.574 2.401 0.428 0.176 0 0.799 -1.436 0.724 0.366 1.680 0.476 0.376 -1.673 -0.683 0.161 0 0.759 -0.208 -0.373 0.208 -1.009 -0.028 -1.209 0.759 2.038 1.402 1 -0.377 -1.044 1.358 -0.223 0.469 1.221 0.582 0.378 -0.116 0.173 Para cualquier corte de validación que se haga (ya sea que se separan un conjunto de datos, o hacer validación cruzada), el porcentaje de aciertos parece ser mayor a 0.5: corte_validacion &lt;- datos_filtrados %&gt;% sample_frac(0.7) valida &lt;- anti_join(datos_filtrados, corte_validacion) modelo_1 &lt;- glm(y ~ ., corte_validacion, family = &quot;binomial&quot;) mean(as.numeric(predict(modelo_1, valida) &gt; 0) == valida$y) %&gt;% round(2) ## [1] 0.73 Sin embargo, el desempeño real del modelo será: mean(as.numeric(predict(modelo_1, datos_prueba) &gt; 0) == datos_prueba$y) %&gt;% round(2) ## [1] 0.49 Método correcto La selección de variables debe hacerse en cada vuelta de validación cruzada: corte_validacion &lt;- datos_entrena %&gt;% sample_frac(0.7) datos_filtrados_corte &lt;- seleccionar(corte_validacion) valida &lt;- anti_join(datos_entrena, corte_validacion) modelo_1 &lt;- glm(y ~ ., datos_filtrados_corte, family = &quot;binomial&quot;) mean(as.numeric(predict(modelo_1, valida) &gt; 0) == valida$y) %&gt;% round(2) ## [1] 0.52 5.3.4 Sobremuestrear antes de particionar Una de las formas de resolver problemas de desbalance de clases es la técnica de sobremuestreo, sin embargo, se tiene que ser muy cuidadoso para evitar errores de fuga de información al aplicar estas técnicas. En este ejemplo se verá que sobremuestrear una clase chica antes de separar datos de validación o hacer validación cruzada puede producir estimaciones demasiado optimistas del error de predicción. Supóngase que tenemos desbalance severo entre nuestras dos clases: set.seed(99134) datos_desbalance &lt;- simular(n = 500, p = 20, prob = 0.1) %&gt;% mutate(y = factor(y, levels = c(1, 0))) datos_desbalance %&gt;% group_by(y) %&gt;% tally() %&gt;% kable() y n 1 41 0 459 Manera incorrecta Supóngase que primero aplicamos (SMOTE)(Chawla et al. 2002) para intentar balancear los datos: receta_balance &lt;- recipe(y ~ ., datos_desbalance) %&gt;% step_smote(y) %&gt;% prep() datos_smote &lt;- juice(receta_balance) Obteniendo así, datos_smote %&gt;% group_by(y) %&gt;% tally() %&gt;% kable() y n 1 459 0 459 Ahora se separará entrenamiento y validación sep_datos_smote &lt;- initial_split(datos_smote) entrena_smote &lt;- training(sep_datos_smote) prueba_smote &lt;- testing(sep_datos_smote) Y se genera un método de clasificación usando un bosque aleatorio de árboles de decisión: metricas &lt;- metric_set(accuracy, recall, precision) bosque &lt;- rand_forest(trees = 500, mtry = 20, mode = &quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% fit(y ~ ., data = entrena_smote) bosque %&gt;% predict(prueba_smote) %&gt;% bind_cols(prueba_smote) %&gt;% metricas(truth = y, estimate = .pred_class) %&gt;% mutate_if(is.numeric, round, 3) %&gt;% kable .metric .estimator .estimate accuracy binary 0.926 recall binary 0.973 precision binary 0.887 En primera instancia parece ser que el desempeño es muy bueno. Se sabe que esto es ficticio, pues no hay relación de \\(y\\) con el resto de las covariables. Manera correcta Antes de hacer el rebalanceo de clases se separa en entrenamiento y validación. Si se quiere, esta parte puede hacerse usando muestreo estratificado, por ejemplo, pero aquí la construimos con muestreo aleatorio simple: sep_datos &lt;- initial_split(datos_desbalance, prop = 0.5) entrena &lt;- training(sep_datos) prueba &lt;- testing(sep_datos) receta_balance &lt;- recipe(y ~ ., data = entrena) %&gt;% step_smote(y) %&gt;% prep() entrena_balanceado &lt;- juice(receta_balance) bosque_1 &lt;- rand_forest(trees = 500, mtry = 20, mode = &quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% fit(y ~ ., data = entrena_balanceado) bosque_1 %&gt;% predict(prueba) %&gt;% bind_cols(prueba) %&gt;% metricas(truth = y, estimate=.pred_class) %&gt;% mutate_if(is.numeric, round, 3) %&gt;% kable() .metric .estimator .estimate accuracy binary 0.828 recall binary 0.000 precision binary 0.000 Aunque el accuracy parece alto, la precisión y la sensibilidad son cero. Un clasificador trivial que siempre predice la clase dominante puede tener mejor exactitud que el que hemos construido. Fugas en implementación 5.3.5 Variables no disponibles al momento de predicción En este caso se muestra un ejemplo donde se utiliza erróneamente una variable que no estará disponible al momento de hacer las predicciones (datos de (Greene 2003)). credito &lt;- read_csv(&quot;datos/AER_credit_card_data.csv&quot;) %&gt;% rename(gasto = expenditure, dependientes = dependents, ingreso = income, edad = age, propietario = owner) %&gt;% mutate(propietario = fct_recode(propietario, c(si = &quot;yes&quot;))) ## Parsed with column specification: ## cols( ## card = col_character(), ## reports = col_double(), ## age = col_double(), ## income = col_double(), ## share = col_double(), ## expenditure = col_double(), ## owner = col_character(), ## selfemp = col_character(), ## dependents = col_double(), ## months = col_double(), ## majorcards = col_double(), ## active = col_double() ## ) credito %&gt;% head %&gt;% mutate_if(is.numeric, round, 1) %&gt;% kable() card reports edad ingreso share gasto propietario selfemp dependientes months majorcards active yes 0 37.7 4.5 0.0 125.0 si no 3 54 1 12 yes 0 33.2 2.4 0.0 9.9 no no 3 34 1 13 yes 0 33.7 4.5 0.0 15.0 si no 4 58 1 5 yes 0 30.5 2.5 0.1 137.9 no no 0 25 1 7 yes 0 32.2 9.8 0.1 546.5 si no 2 64 1 5 yes 0 23.2 2.5 0.0 92.0 no no 0 54 1 1 Se quiere construir un modelo para predecir que solicitudes fueron aceptadas y automatizar el proceso de selección. Se usa una regresión logística con Keras y penalización L2: set.seed(823) credito_split &lt;- initial_split(credito) entrena &lt;- training(credito_split) prueba &lt;- testing(credito_split) # preparacion de datos credito_receta &lt;- recipe(card ~ ., credito) %&gt;% step_normalize(all_numeric()) %&gt;% step_dummy(all_nominal(), -card) # modelo modelo_regularizado &lt;- logistic_reg(penalty = 1) %&gt;% set_engine(&quot;keras&quot;, epochs = 500, verbose = FALSE) %&gt;% set_mode(&quot;classification&quot;) # ajustar parametros de preprocesamiento receta_prep &lt;- credito_receta %&gt;% prep(entrena) # preprocesar datos entrena_prep &lt;- bake(receta_prep, entrena) prueba_prep &lt;- bake(receta_prep, prueba) # ajustar modelo ajuste &lt;- modelo_regularizado %&gt;% fit(card~ gasto + dependientes + ingreso + edad + propietario_si, data = entrena_prep) # evaluar metricas &lt;- metric_set(accuracy, recall, precision) ajuste %&gt;% predict(prueba_prep) %&gt;% bind_cols(prueba) %&gt;% metricas(truth = factor(card), estimate = .pred_class) %&gt;% mutate_if(is.numeric, round, 3) %&gt;% kable() .metric .estimator .estimate accuracy binary 0.906 recall binary 0.667 precision binary 0.949 Y parece tener un desempeño razonable. Si quitamos la variable expenditure se degrada totalmente el desempeño del modelo: ajuste_2 &lt;- modelo_regularizado %&gt;% fit(card~ dependientes + ingreso + edad + propietario_si, data = entrena_prep) ajuste_2 %&gt;% predict(prueba_prep) %&gt;% bind_cols(prueba) %&gt;% metricas(truth = factor(card), estimate = .pred_class) %&gt;% mutate_if(is.numeric, round, 3) %&gt;% kable() ## Warning: While computing binary `precision()`, no predicted events were detected (i.e. `true_positive + false_positive = 0`). ## Precision is undefined in this case, and `NA` will be returned. ## Note that 84 true event(s) actually occured for the problematic event level, &#39;no&#39;. .metric .estimator .estimate accuracy binary 0.745 recall binary 0.000 precision binary NA La sensibilidad es muy mala y la precisión no se puede calcular pues el modelo no hace predicciones positivas para el conjunto de prueba. La razón de esta degradación en el desempeño es que gasto se refiere a uso de tarjetas de crédito. Esto incluye la tarjeta para la que queremos hacer predicción de aceptación: entrena %&gt;% mutate(algun_gasto = gasto &gt; 0) %&gt;% group_by(algun_gasto, card) %&gt;% tally() %&gt;% kable() algun_gasto card n FALSE no 212 FALSE yes 19 TRUE yes 759 Lo que indica que algún gasto probablemente incluye el gasto en la tarjeta actual, y la variable gasto es medida posteriormente a la entrega de la tarjeta: El desempeño de este modelo para nuevas aplicaciones será muy malo, pues la variable gasto, en el momento de la aplicación, evidentemente no cuenta cuánto va a gastar cada cliente en el futuro. Evaluación de punto de corte Las mejores decisiones de punto de corte pueden hacerse con análisis de costo beneficio, con curvas tipo lift basadas en ganancias y pérdidas de cada decisión. Aunque esta información muchas veces no está disponible, es la situación ideal para evaluar cómo ayuda el modelo y cuánto valen las acciones que pretendemos tomar. Es posible hacer este análisis con valores inciertos de costo beneficio. Supóngase que estamos pensando en un tratamiento para retener estudiantes en algún programa de entrenamiento o mejora. El tratamiento de retención cuesta 5000 pesos por alumno, Estimamos mediante experimentos o algún análisis externo que nuestro tratamiento reduce la probabilidad de abandono en un 60%, Tenemos algún tipo de valuación del valor social de que un alumno persista en el programa. Podemos evaluar a nuestro modelo en el contexto del problema de las siguiente forma: Suponemos que trataremos a un porcentaje de los estudiantes con mayor probabilidad de rotar. Calculamos el costos esperado si tratamos a un porcentaje de los estudiantes: simulamos reduciendo su probabilidad de abandono por el tratamiento y sumamos los costos de tratarlos. Comparamos contra el escenario de no aplicar ningún tratamiento No es necesario usar medidas muy técnicas para dar un resumen de cómo nos puede ayudar el tratamiento y modelo para mantener el valor de la cartera: ggplot(filter(perdidas_sim, tipo==&quot;Tratamiento modelo&quot;), aes(x = factor(corte), y = - perdida / 1e6)) + geom_boxplot() + ylab(&quot;Ganancia incremental (millones)&quot;) + xlab(&quot;Corte inferior de tratamiento (probabilidad)&quot;) + labs(subtitle = &quot;Ganancia vs ninguna acción&quot;) + theme_minimal() Se puede escoger un punto de corte entre 0.2 y 0.3, por ejemplo, o hacer más simulaciones para refinar la elección. Si se quiere separar el efecto del tratamiento con el efecto del tratamiento aplicado según el modelo, se puede comparar con la acción que consiste en tratar a los estudiantes al azar: ggplot(perdidas_sim, aes(x = factor(corte), y = - perdida / 1e6, group = interaction(tipo, corte), colour = tipo)) + geom_boxplot() + ylab(&quot;Ganancia incremental (millones)&quot;) + xlab(&quot;Corte inferior de tratamiento (probabilidad)&quot;) + labs(subtitle = &quot;Ganancia vs ninguna acción&quot;) + theme_minimal() La conclusión es que el modelo ayuda considerablemente a la focalización del programa (el área entre las dos curvas mostradas arriba). Desbalance de clases Cuando se tiene un desbalance severo de clases podemos enfrentar dos problemas: existen en términos absolutos muy pocos elementos de una clase para poder discriminarla de manera efectiva (aún cuando se tengan los atributos o features correctos), o métodos usuales de evaluación de predicción son deficientes para evaluar el desempeño de predicciones. Considérense los siguientes datos (del paquete de James et al. (2017)): “Los datos contienen 5.822 registros de clientes reales. Cada registro consta de 86 variables, que contienen datos sociodemográficos (variables 1-43) y de propiedad del producto (variables 44-86). Los datos sociodemográficos se derivan de los códigos postales. Todos los clientes que viven en zonas con el mismo código postal tienen los mismos atributos sociodemográficos. La variable 86 (Compra) indica si el cliente adquirió una póliza de seguro de caravanas” (James et al. (2013))16 Se quiere predecir la variable Purchase: caravan &lt;- read_csv(&quot;datos/caravan.csv&quot;) %&gt;% mutate(MOSTYPE = factor(MOSTYPE), MOSHOOFD = factor(MOSHOOFD)) %&gt;% mutate(Compra = fct_recode(Purchase, si = &quot;Yes&quot;, no = &quot;No&quot;)) %&gt;% mutate(Compra = fct_rev(Compra)) %&gt;% select(-Purchase) ## Parsed with column specification: ## cols( ## .default = col_double(), ## Purchase = col_character() ## ) ## See spec(...) for full column specifications. nrow(caravan) ## [1] 5822 caravan %&gt;% count(Compra) %&gt;% mutate(pct = 100 * n / sum(n)) %&gt;% mutate(pct = round(pct, 2)) ## # A tibble: 2 x 3 ## Compra n pct ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 si 348 5.98 ## 2 no 5474 94.0 Esta es la distribución natural de respuesta que se ve en los datos, y se tiene relativamente pocos datos en la categoría “Si”. Se usará muestreo estratificado para obtener proporciones similares en conjuntos de entrenamiento y prueba: set.seed(823) caravan_split = initial_split(caravan, strata = Compra, prop = 0.9) caravan_split ## &lt;Analysis/Assess/Total&gt; ## &lt;5240/582/5822&gt; entrena &lt;- training(caravan_split) prueba &lt;- testing(caravan_split) Y se usará regresión logística (lo mismo aplica para otros métodos que produzcan probabilidades de clase, como boosting, árboles aleatorios o redes neuronales): library(tune) # preparacion de datos caravan_receta &lt;- recipe(Compra ~ ., entrena) %&gt;% step_dummy(all_nominal(), -Compra) caravan_receta_prep &lt;- caravan_receta %&gt;% prep # modelo modelo_log &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) %&gt;% fit(Compra ~ ., data = caravan_receta_prep %&gt;% juice) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Análisis incorrecto La matriz de confusión de entrenamiento es predictions_ent_glm &lt;- modelo_log %&gt;% predict(new_data = juice(caravan_receta_prep)) %&gt;% bind_cols(juice(caravan_receta_prep) %&gt;% select(Compra)) predictions_ent_glm %&gt;% conf_mat(Compra, .pred_class) ## Truth ## Prediction si no ## si 6 9 ## no 299 4926 Y los de prueba: prueba_procesado &lt;- bake(caravan_receta_prep, prueba) predictions_glm &lt;- modelo_log %&gt;% predict(new_data = prueba_procesado) %&gt;% bind_cols(prueba_procesado %&gt;% select(Compra)) predictions_glm %&gt;% conf_mat(Compra, .pred_class) ## Truth ## Prediction si no ## si 0 4 ## no 43 535 Y se obtiene un desempeño pobre según esta matriz de confusión (prueba y entrenamiento). La sensibilidad es muy baja, aunque la especificidad (tasa de correctos negativos) sea alta. Una conclusión típica es que el modelo no tiene valor predictivo, o que es necesario sobre muestrear la clase de ocurrencia baja. Análisis correcto En lugar de empezar con sobre/sub muestreo, que modifica lsas proporciones naturales de las categorías en los datos, podemos trabajar con probabilidades en lugar de predicciones de clase con punto de corte de 0.5. Por ejemplo, podemos visualizar con una curva ROC (o curva lift, precisión-recall, o alguna otra similar que tome en cuenta probabilidades): predictions_prob &lt;- modelo_log %&gt;% predict(new_data = prueba_procesado, type = &quot;prob&quot;) %&gt;% bind_cols(prueba_procesado %&gt;% select(Compra)) %&gt;% select(.pred_si, Compra) ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : ## prediction from a rank-deficient fit may be misleading datos_roc &lt;- roc_curve(predictions_prob, Compra, .pred_si) autoplot(datos_roc) + xlab(&quot;1 - especificidad&quot;) + ylab(&quot;sensibilidad&quot;) Donde se ve que es posible alcanzar buenos niveles de sensibilidad si se acepta alguna degradación en la especificidad, que originalmente es muy alta. Por ejemplo, cortando en 0.05 se puede obtener especificidad y sensibilidad que posiblemente sean adecuadas para el problema: datos_roc %&gt;% filter(abs(.threshold - 0.04) &lt; 1e-4) %&gt;% round(4) ## # A tibble: 2 x 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0399 0.553 0.744 ## 2 0.0399 0.555 0.744 ¿Qué pasa si hacemos sub/sobremuestreo? Sobremuestreamos: caravan_receta_smote &lt;- recipe(Compra ~ ., entrena) %&gt;% step_dummy(MOSTYPE, MOSHOOFD) %&gt;% step_smote(Compra) smote_prep &lt;- prep(caravan_receta_smote) # modelo entrena_1 &lt;- juice(smote_prep) entrena_1 %&gt;% count(Compra) ## # A tibble: 2 x 2 ## Compra n ## &lt;fct&gt; &lt;int&gt; ## 1 si 4935 ## 2 no 4935 modelo_log_smote &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) %&gt;% fit(Compra ~ ., data = entrena_1) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred En entrenamiento la matriz de confusión es aparentemente mejor: predictions_ent_glm &lt;- modelo_log_smote %&gt;% predict(new_data = entrena_1) %&gt;% bind_cols(entrena_1 %&gt;% select(Compra)) predictions_ent_glm %&gt;% conf_mat(Compra, .pred_class) ## Truth ## Prediction si no ## si 3854 1271 ## no 1081 3664 Pero en prueba los resultados son muy similares. Se agrega también el modelo construido submuestreando la clase dominante: entrena_sub &lt;- caravan_receta %&gt;% step_downsample(Compra) %&gt;% prep() %&gt;% juice modelo_log_sub &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) %&gt;% fit(Compra ~ ., data = entrena_sub) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred predictions_prob &lt;- modelo_log_smote %&gt;% predict(new_data = prueba_procesado, type = &quot;prob&quot;) %&gt;% bind_cols(prueba_procesado %&gt;% select(Compra)) %&gt;% select(.pred_si, Compra) ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : ## prediction from a rank-deficient fit may be misleading predictions_prob_sub &lt;- modelo_log_sub %&gt;% predict(new_data = prueba_procesado, type = &quot;prob&quot;) %&gt;% bind_cols(prueba_procesado %&gt;% select(Compra)) %&gt;% select(.pred_si, Compra) ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : ## prediction from a rank-deficient fit may be misleading datos_roc_smote &lt;- roc_curve(predictions_prob, Compra, .pred_si) datos_roc_sub &lt;- roc_curve(predictions_prob_sub, Compra, .pred_si) datos_roc_comp &lt;- bind_rows(datos_roc %&gt;% mutate(tipo = &quot;natural&quot;), datos_roc_smote %&gt;% mutate(tipo = &quot;con sobre muestreo&quot;), datos_roc_sub %&gt;% mutate(tipo = &quot;con sub muestreo&quot;) ) ggplot(datos_roc_comp, aes(x = 1 - specificity, y = sensitivity, colour = tipo)) + geom_path() + geom_abline(lty = 3) + coord_equal() + theme_bw() El problema original no era que el ajuste no funcionaba, sino que evaluamos el punto de corte incorrecto. Un punto de corte de 0.5 con SMOTE equivale a uno mucho más chico sin SMOTE. Peor aún, las probabilidades del modelo construido con sobremuestreo no reflejan las tasas de ocurrencia de la respuesta que nos interesa, lo cual puede producir resúmenes engañosos de las tasas de respuesta que esperamos observar en producción. Equidad con atributos protegidos El siguiente ejemplo es derivado de (Hardt, Price, and Srebro (2016)). Supongamos que tenemos un atributo protegido \\(A\\) que tiene dos valores: azul y naranja. Naranja es el grupo minoritario desaventajado. Usaremos datos simulados como sigue: el atributo score está asociado al atributo protegido: inv_logit &lt;- function(x){ 1 / (1 + exp(-x)) } simular_datos &lt;- function(n = c(10000, 2000)){ score_azul &lt;- pmax(rnorm(n[1], 50, 10), 0) score_naranja &lt;- pmax(rnorm(n[2], 40, 10), 0) azul &lt;- tibble(tipo = &quot;azul&quot;, score = score_azul) naranja &lt;- tibble(tipo = &quot;naranja&quot;, score = score_naranja) datos &lt;- bind_rows(azul, naranja) %&gt;% mutate(coef_0 = ifelse(tipo == &quot;azul&quot;, 0.0, 0), prob_real_pos = inv_logit(-1 + coef_0 + 0.1 * (score-40))) %&gt;% mutate(atr_1 = rpois(nrow(.), 3)) datos %&gt;% select(-coef_0) %&gt;% mutate(paga = map_dbl(prob_real_pos, ~ rbinom(1, 1, .x))) %&gt;% select(-prob_real_pos) } set.seed(1221) tbl_datos &lt;- simular_datos() Una gráfica de conteos para el score se obtiene un grupo minoritario con valores de la variable score más baja: ggplot(tbl_datos, aes(x = score, fill = tipo)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Se ajusta un modelo simple de regresión logística: reg_log &lt;- glm(paga ~ score + atr_1 + tipo, tbl_datos, family = &quot;binomial&quot;) tbl_datos &lt;- tbl_datos %&gt;% mutate(prob_pos = predict(reg_log, type = &quot;response&quot;)) Las tasas reales de cumplimiento son iguales para los dos grupos. En primer lugar, se considera una estrategia donde se aplica el mismo punto de corte para todos los grupos resultado_cortes &lt;- function(tbl_datos, cortes){ resultado &lt;- tbl_datos %&gt;% mutate(recibe = ifelse(tipo == &quot;azul&quot;, prob_pos &gt; cortes[1], prob_pos &gt; cortes[2]), decision = ifelse(recibe, &quot;Aceptado&quot;, &quot;Rechazado&quot;)) resultado %&gt;% group_by(tipo, decision, paga) %&gt;% count() %&gt;% ungroup() } resultados_conteo &lt;- resultado_cortes(tbl_datos, c(0.6, 0.6)) resultados_conteo ## # A tibble: 8 x 4 ## tipo decision paga n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 azul Aceptado 0 905 ## 2 azul Aceptado 1 2400 ## 3 azul Rechazado 0 4149 ## 4 azul Rechazado 1 2546 ## 5 naranja Aceptado 0 47 ## 6 naranja Aceptado 1 101 ## 7 naranja Rechazado 0 1353 ## 8 naranja Rechazado 1 499 resultados_conteo %&gt;% group_by(tipo, decision) %&gt;% summarise(n = sum(n)) %&gt;% mutate(total = sum(n)) %&gt;% mutate(prop = n / total) %&gt;% filter(decision == &quot;Aceptado&quot;) ## # A tibble: 2 x 5 ## # Groups: tipo [2] ## tipo decision n total prop ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 azul Aceptado 3305 10000 0.330 ## 2 naranja Aceptado 148 2000 0.074 Nótese que el grupo naranja ha recibido considerablemente menos aceptaciones que el grupo azul, tanto en total como en proporción. Más aún, con la precisión o tasa de verdaderos positivos podemos evaluar qué proporción de los que cumplirían si fueran aceptados fueron aceptados según nuestro punto de corte: resultados_conteo %&gt;% filter(paga == 1) %&gt;% group_by(tipo) %&gt;% mutate(tvp = n / sum(n)) %&gt;% filter(decision == &quot;Aceptado&quot;) ## # A tibble: 2 x 5 ## # Groups: tipo [2] ## tipo decision paga n tvp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 azul Aceptado 1 2400 0.485 ## 2 naranja Aceptado 1 101 0.168 y se ve que el grupo naranja también está en desventaja, pues entre los que cumplen hay menos decisiones de aceptación. El siguiente paso es considerar paridad demográfica. En este caso, decidimos dar el mismo número de préstamos a cada grupo, dependiendo de su tamaño. calcular_puntos_paridad &lt;- function(tbl_datos, prop){ tbl_datos %&gt;% group_by(tipo) %&gt;% summarise(corte = quantile(prob_pos, 1 - prop)) } cortes_paridad_tbl &lt;- calcular_puntos_paridad(tbl_datos, 0.45) cortes_paridad_tbl ## # A tibble: 2 x 2 ## tipo corte ## &lt;chr&gt; &lt;dbl&gt; ## 1 azul 0.521 ## 2 naranja 0.297 El corte para azul es más exigente que para naranja. En sí eso no es un problema pero observamos: cortes_paridad &lt;- cortes_paridad_tbl %&gt;% pull(corte) resultados_conteo &lt;- resultado_cortes(tbl_datos, cortes_paridad) resultados_conteo %&gt;% filter(paga == 1) %&gt;% group_by(tipo) %&gt;% mutate(tvp = n / sum(n)) %&gt;% filter(decision == &quot;Aceptado&quot;) ## # A tibble: 2 x 5 ## # Groups: tipo [2] ## tipo decision paga n tvp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 azul Aceptado 1 3094 0.626 ## 2 naranja Aceptado 1 410 0.683 Y así que además de ser más exigente con el grupo azul, a los que cumplen del grupo azul también se les otorga menos decisiones de aceptación. Adicionalmente, se aceptan considerablemente menos personas de la población. La solución de igual de de oportunidad es cortar de forma que la tasa de aceptación dentro del grupo de los que pagan sea similar para ambas poblaciones, lo que ocurre aproximadamente en 0.35: calcular_cortes_oportunidad &lt;- function(tbl_datos, prop){ tbl_datos %&gt;% filter(paga==1) %&gt;% group_by(tipo) %&gt;% mutate(rank_p = rank(prob_pos) / length(prob_pos) ) %&gt;% filter(rank_p &lt; prop) %&gt;% top_n(1, rank_p) %&gt;% select(tipo, corte = prob_pos) } cortes_op &lt;- calcular_cortes_oportunidad(tbl_datos, 0.35) resultados_conteo &lt;- resultado_cortes(tbl_datos, cortes_op %&gt;% pull(corte)) resultados_conteo %&gt;% filter(paga == 1) %&gt;% group_by(tipo) %&gt;% mutate(tvp = n / sum(n)) %&gt;% filter(decision == &quot;Aceptado&quot;) ## # A tibble: 2 x 5 ## # Groups: tipo [2] ## tipo decision paga n tvp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 azul Aceptado 1 3215 0.650 ## 2 naranja Aceptado 1 391 0.652 Nota: es importante notar que si la variable de resultado positivo es injustamente asignada, entonces este método no resuelve el problema. En este caso es relevante entender cuáles son los criterios con los que se considera un resultado existoso dependiendo de el grupo del atributo protegido (por ejemplo, si a un segmento particular se le permite mayores atrasos en los pagos y a otro menos, o un grupo se considera un delicuente reincidente por una ofensa mucho menor que otros grupos). Interpretabilidad Se pueden usar medidas como importancia de permutaciones para examinar modelos. En este ejemplo, se regresa al ejercicio de predicción de aceptación de solicitudes de crédito, y consideramos la importancia basada en permutaciones (Molnar (2019)): set.seed(823) credito_split &lt;- initial_split(credito) entrena &lt;- training(credito_split) prueba &lt;- testing(credito_split) # preparacion de datos credito_receta &lt;- recipe(card ~ ., credito) %&gt;% step_normalize(all_numeric()) %&gt;% step_dummy(all_nominal(), -card) # modelo modelo_regularizado &lt;- logistic_reg(penalty = 1) %&gt;% set_engine(&quot;keras&quot;, epochs = 500, verbose = FALSE) %&gt;% set_mode(&quot;classification&quot;) # ajustar parametros de preprocesamiento receta_prep &lt;- credito_receta %&gt;% prep(entrena) # preprocesar datos entrena_prep &lt;- bake(receta_prep, entrena) prueba_prep &lt;- bake(receta_prep, prueba) # ajustar modelo ajuste &lt;- modelo_regularizado %&gt;% fit(card~ gasto + dependientes + ingreso + edad + propietario_si, data = entrena_prep) library(iml) modelo &lt;- ajuste$fit entrena_x &lt;- entrena_prep %&gt;% dplyr::select(gasto, dependientes, ingreso, edad, propietario_si) predictor &lt;- Predictor$new(modelo, data = entrena_x, y = ifelse(entrena_prep$card == &quot;yes&quot;,2,1) , type = &quot;prob&quot;) imp &lt;- FeatureImp$new(predictor, loss = &quot;ce&quot;, compare = &quot;difference&quot;) plot(imp) + theme_minimal() Se ve que, para esta red sin capas ocultas, la importancia se concentra en un solo predictor, gasto, que como vimos representa una fuga de información. Este diagnóstico es útil en general, y aunque no tan dramático como este ejemplo, puede señalar cuáles variables es importante considerar con cuidado. Es importante considerar también el efecto de variables asociadas a grupos protegidos, y de ser necesario, examinar con cuidado cómo afectan las predicciones. Modelos parsimoniosos, que usan menos atributos, facilitan el análisis, el mantenimiento del flujo de datos, y reducen exponernos a problemas de fugas o efectos indeseables. Explicación de predicciones Para explicar predicciones individuales se pueden usar los valores de Shapley (Molnar (2019)), (Lundberg and Lee (2017)). Estas gráficas indican la contribución asignada de cada atributo a una predicción individual, bajo la idea de considerar efectos marginales sobre la predicción dependiendo de la presencia o ausencia de otros atributos. Las contribuciones obtenidas suman la diferencia que hay entre la predicción particular y la predicción promedio. Pueden examinarse también promedios a lo largo de grupos de interés. Considérese el ejemplo de factores para detectar una enfermedad del corazón en el estudio de Sudáfrica que vimos en este ejemplo modelo_sa &lt;- sa_boosted$fit sa_entrena_x &lt;- sa_entrena %&gt;% dplyr::select(-enf_coronaria) predict_fun &lt;- function(object, newdata){ new_data_x = xgb.DMatrix(data.matrix(newdata), missing = NA) results&lt;-predict(modelo_sa, new_data_x) return(results) } predictor &lt;- Predictor$new(modelo_sa, data = sa_entrena_x, y = sa_entrena$chd , type = &quot;prob&quot;, predict.function = predict_fun) ## Warning: Unknown or uninitialised column: `chd`. # el caso de interés es el caso 15 valores_shapley &lt;- Shapley$new(predictor, x.interest = (sa_entrena_x[15, ])) valores_shapley$plot() + theme_minimal() En este caso, varias medidas contribuyen positivamente a la probabilidad de enfermedad del corazon, como es uso de tabaco, edad, y mediciones de colesterol. Estas constribuciones explican la probabiidad tan alta de este individuo particular. En contraste, la siguiente persona está cerca del promedio, aumentando positivamente la probabiidad la edad y medida de colesterol, pero negativamente el no uso de tabaco y ninguna historia familiar de diabetes: # el caso de interés es el caso 24 valores_shapley &lt;- Shapley$new(predictor, x.interest = (sa_entrena_x[24, ])) valores_shapley$plot() + theme_minimal() Observación: igual que en el modelo y en las gráficas de dependencia parcial que se discutieron anteriormente, estos coeficientes no deben interpretarse de manera causal (por ejemplo: es necesario bajar el colesterol para estos dos individuos). Esta es la información que usa el modelo para construir la predicción a partir de la predicción promedio sobre la población. Se pueden calcular los valores de shapley para dos grupos de edad, por ejemplo. Datos accesibles en http://archive.ics.uci.edu/ml/datasets/heart+Disease↩ Datos y más información accesible en  http://www.liacs.nl/~putten/library/cc2000/data.html.↩ "],
["referencias.html", "Referencias", " Referencias "]
]
