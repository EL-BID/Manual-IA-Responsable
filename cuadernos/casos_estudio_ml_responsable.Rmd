---
title: "Casos de estudio"
link-citations: yes
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: '3'
    includes:
      in_header: ../documento/preamble.tex
    latex_engine: xelatex
    citation_package: natbib
  html_document:
    css: ./css/style.css
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
bibliography:
- ../documento/referencias.bib
- ../documento/packages.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

En este cuaderno mostraremos varios ejemplos de los retos y soluciones explicadas
en el documento principal. Usamos distintos tipos de modelos (lineales, basados en árboles y otros) y distintas implementaciones (R, keras, xgboost) para mostrar
que estos problemas se presentan independientemente de la elección de herramientas
particulares.

Se utiliza el lenguaje de programación R, y los siguientes paquetes: @tidyverse, 
@recipes, @themis, @rsample, @parsnip, @yardstick, @workflows, @tune, @knitr, 
@patchwork.

```{r}
library(tidyverse)
library(recipes)
library(themis)
library(rsample)
library(parsnip)
library(yardstick)
library(workflows)
library(tune)
library(knitr)
library(patchwork)
```


# Retos intrínsecos a los datos

## Reto: mala correspondencia de métrica y objetivos

Usar modelos que predicen la métrica incorrecta puede llevar a tomar decisiones incorrectas.
A veces el problema es claro, cuando la métrica sustituto tiene deficiencias obvias, y en otras
puede ser más sutil.

En este ejemplo se busca predecir la demanda de cierto producto, y se cuenta
con datos históricos de inventario, ventas y una covariable *predictor* 
asociada a ventas:

```{r, echo=FALSE, include=FALSE}
set.seed(124)
semana <- 1:100
indicador <- rnorm(100, 0, 20)
inventario <- 100 + 7*sqrt(1:100) + rpois(100, 50) 
demanda <- 30 + 1.2*1:100 + indicador + rpois(100, 100) 
ventas <- tibble(
    semana = 1:100,
    inventario = round(inventario), 
    demanda = round(demanda), 
    ventas = pmin(inventario, demanda),
    predictor = indicador, 
    agotamiento = ifelse(ventas==inventario, 1, 0))
ventas %>% select(-demanda)
```


Se cuenta con datos históricos de inventario (80 semanas), ventas y una variable *predictor* asociada a ventas y otra de agotamiento del inventario. Separamos los datos en entrenamiento y prueba, ajustando el modelo con el 
subconjunto de datos de entrenamiento. En este caso se utiliza un modelo lineal
con variable dependiente ventas y covariables de semana y la covariable *predictor*.

```{r}
entrena <- ventas %>% filter(semana < 60)
prueba <- ventas %>% filter(semana >= 60, semana <= 80)
entrena %>% select(-demanda) %>%  head() %>% kable()
mod_lineal <- lm(ventas ~ semana + predictor, data = ventas)
mod_lineal
```
Evaluamos el error de predicción.

```{r}
preds <- predict(mod_lineal, newdata = prueba)
round(mean(abs(preds - prueba$ventas))/mean(prueba$ventas), 3)
```

El error porcentual es bajo. Los datos ajustados y predicciones se ven como sigue:

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
ggplot(ventas_larga %>% mutate(unidades = ifelse(tipo=="ventas" & semana > 80, NA, unidades)), 
       aes(x = semana, y = unidades, group = tipo, colour = tipo)) + 
  geom_line() +
  geom_vline(xintercept = 80) +
  geom_vline(xintercept = 60) +
  annotate("text", x = 25, y=105, label = "entrena") +
  annotate("text", x = 69, y=105, label = "prueba")
```

Pero tomar decisiones de demanda o inventario es equivocado. La razón es que 
existen agotamientos de inventario, que marcamos con rojo en la siguiente gráfica.

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
ggplot(ventas_larga %>% mutate(unidades = ifelse(tipo=="ventas" & semana > 80, NA, unidades)), aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_point(data = filter(ventas, agotamiento==1, semana < 80), aes(y = ventas), colour = "red") + 
  geom_vline(xintercept = 80) +
  geom_vline(xintercept = 60) +
  annotate("text", x = 25, y=105, label = "entrena") +
  annotate("text", x = 69, y=105, label = "prueba")
```

Si usáramos la política sugerida por las predicciones (por ejemplo 5% más), 
veríamos las ventas de la primera gráfica a continuación. Sin embargo, si 
usáramos una política de inventario con 280 unidades, observaríamos:


```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_obs <- ventas %>% mutate(pred = preds) %>%
  mutate(inventario = 1.05 *  pred) %>% 
  mutate(ventas = ifelse(semana > 80, pmin(inventario, demanda), ventas)) 
ventas_larga <- ventas_obs %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
g1 <- ggplot(ventas_larga, aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_point(data = filter(ventas_obs, ventas == inventario, semana > 80), aes(y = ventas), colour = "red") +
  geom_vline(xintercept = 80) + labs(subtitle = "Inventario: Predicciones + 5%")
```

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_obs <- ventas %>% mutate(pred = preds) %>% 
  mutate(inventario = 280) %>% 
  mutate(ventas = ifelse(semana > 80, pmin(inventario, demanda), ventas)) 
ventas_larga <- ventas_obs %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
g2 <- ggplot(ventas_larga, aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_point(data = filter(ventas_obs, ventas == inventario, semana > 80), aes(y = ventas), colour = "red") +
  geom_vline(xintercept = 80)+ labs(subtitle = "Inventario: 300 unidades cte")
```

```{r}
g1 / g2
```

Entonces, 

- La política basada en las predicciones **exacerba** el problema de los agotamientos.
- Un uso no pensado de datos sin considerar el proceso generador de los mismos puede producir 
errores grandes en las decisiones
- En este caso, la confusión proviene de no separar los conceptos de demanda y ventas. Otros
indicadores de demanda o modelos más adecuados ayudarían a resolver el problema.
- Soluciones simplistas como sólo tomar los datos donde no ocurren agotamientos pueden empeorar aún más
la situación: incrementan el sesgo (seleccionamos semanas donde las ventas tienden a ser bajas) y reducen
la precisión (usamos menos datos).

## Muestras naturales y sesgo

Cuando las muestras de entrenamiento son diferentes a las poblaciones, donde
se van a aplicar los modelos, existen dificultades en validar correctamente las
predicciones.

### Muestras naturales: mala representatividad 

Para este ejemplo utilizaremos datos de la encuesta nacional de ingresos y gastos
en hogares de México [@enigh], para simular un escenario que queremos ejemplificar.

```{r}
set.seed(128)
encuesta_ingreso <- read_csv("datos/enigh-ejemplo.csv")
datos_ingreso <- encuesta_ingreso %>% 
  mutate(num_focos = FOCOS) %>%
  mutate(ingreso_miles = (INGCOR / 1000)) %>% 
  mutate(tel_celular = ifelse(SERV_2 == 1, "Sí", "No")) %>% 
  mutate(piso_firme = ifelse(PISOS != 1 | is.na(PISOS), "Sí", "No")) %>% 
  mutate(lavadora = ifelse(LAVAD != 1 | is.na(LAVAD), "Sí", "No")) %>% 
  mutate(automovil = VEHI1_N > 0) %>% 
  mutate(marginacion = fct_reorder(marginación, ingreso_miles, median)) %>% 
  rename(ocupadas = PEROCU) %>% 
  rename(educacion_jef = NIVELAPROB) %>% 
  select(ingreso_miles, num_focos, tel_celular, 
         marginacion, ocupadas, piso_firme, lavadora, automovil, educacion_jef)
```

```{r}
ingreso_split <- initial_split(datos_ingreso, prop = 0.7)
entrena <- training(ingreso_split)
prueba <- testing(ingreso_split)
```

Supongamos que nos interesa estimar el ingreso de los hogares, para ello usamos 
una encuesta por teléfono celular, más aún, supongamos que solo accedimos a 
zonas que no tienen marginación muy alta.

```{r}
muestra_sesgada <- filter(entrena, 
                          tel_celular == "Sí", 
                          marginacion=="Muy bajo")
sesgados_split <- initial_split(muestra_sesgada)
entrena_sesgo <- training(sesgados_split)
validacion_sesgo <- testing(sesgados_split)
```

Construimos un modelo lineal para el logaritmo de ingresos con los datos disponibles.

```{r}
library(splines)
formula <- as.formula("log(ingreso_miles) ~ ns(num_focos, 3) + 
              ns(ocupadas, 3) +  lavadora + automovil + piso_firme +
              ns(educacion_jef, 3)")
mod_sesgo <- lm(formula, data = entrena_sesgo)
# tomamos una muestra representativa para comparar, del mismo tamaño que la sesgada
mod_representativa <- lm(formula, data = sample_n(entrena, nrow(entrena_sesgo)))
```

Y evaluamos el error en una muestra de prueba construida con datos con las mismas
características sesgadas que los datos de entrenamiento (hogares con teléfono celular y 
grado de marginación muy bajo).

```{r}
preds_val <- predict(mod_sesgo, newdata = validacion_sesgo)
mean(abs(preds_val - log(1 + validacion_sesgo$ingreso_miles))) %>% round(2)
```

El error en una muestra más similar a la población que se pretende aplicar el 
algoritmo es mayor:

```{r}
preds_prueba_sesgo <- predict(mod_sesgo, newdata = prueba)
preds_prueba <- predict(mod_representativa, newdata = prueba)
prueba$pred_sesgada <- preds_prueba_sesgo
prueba$pred_rep <- preds_prueba
mean(abs(preds_prueba_sesgo - log(1 + prueba$ingreso_miles))) %>% round(2)
```

Sin embargo, el principal problema se refleja en la siguiente gráfica, donde
usamos escalas logarítmicas para hacer comparaciones multiplicativas, que nos interesan
por la naturaleza del ingreso. Cada punto 
representa un hogar, la muestra es más similar a la población donde se aplicará la
metodología, y en el 
eje horizontal graficamos la predicción de los hogares utilizando el modelo,
mientras que el eje vertical corresponde al ingreso de cada hogar. Como referencia
agregamos la recta $y = x$, y un suavizador (ver por ejemplo [aquí](https://en.wikipedia.org/wiki/Local_regression)). Nos concetramos
en vel el desempeño para los hogares de ingresos relativamente bajos
(menos de 10 mil pesos al mes):

```{r, fig.width = 9, fig.height = 4}
breaks_y <- c(3, 5, 10, 20, 40, 80)
g_sesgo <- ggplot(prueba %>% filter(pred_sesgada < log(30)), 
  aes(x = exp(pred_sesgada), y = ingreso_miles)) + 
  geom_point(alpha = 0.5) +
  geom_abline() + geom_smooth(method = "loess", span = 1) +
  scale_x_log10(limits=c(5, 30)) + scale_y_log10(breaks = breaks_y) +
  xlab("Predicción (miles al trimestre)") +
  ylab("Ingreso corriente (miles al trimestre)") +
  labs(subtitle = "Desempeño en prueba \ncon sesgo en entrenamiento") 
g_representativa <- ggplot(prueba %>% filter(pred_sesgada < log(30)), 
  aes(x = exp(pred_rep), y = ingreso_miles)) + 
  geom_point(alpha = 0.5) +
  geom_abline() + geom_smooth(method = "loess", span = 1) +
  scale_x_log10(limits = c(5, 30)) + scale_y_log10(breaks = breaks_y) +
  xlab("Predicción (miles al trimestre)") +
  ylab("Ingreso corriente (miles al trimestre)") +
  labs(subtitle = "Desempeño en prueba \ncon muestra representativa en entrenamiento") 
g_sesgo + g_representativa
```

Aunque comunmente esperamos sobrepredecir valores observados relativamente bajos, y lo
contrario para valores relativamente altos, 
para los que tienen ingresos de menos de 10 mil pesos mensuales,
el modelo sesgado sobrepredice el ingreso verdadero por alrededor del 40%:

```{r}
prueba_bajo <- prueba %>% filter(ingreso_miles < 3*10)
sesgo <- mean(exp(prueba_bajo$pred_sesgada))/mean(prueba_bajo$ingreso_miles) - 1
round(sesgo,3)
```

Que comparamos con el mismo modelo entrenado con una muestra representativa, donde
el efecto es considerablemente menor:

```{r}
prueba_bajo <- prueba %>% filter(ingreso_miles < 3*10)
sesgo <- mean(exp(prueba_bajo$pred_rep))/mean(prueba_bajo$ingreso_miles) - 1
round(sesgo,3)
```


Tenemos entonces dos problemas:

1. El sesgo produce un error considerablemente más grande en la implementación que en la validación.
2. Peor aún, el sesgo es mayor para hogares de menores ingresos (las predicciones son altas), 
lo cual puede producir una focalización mediocre si buscamos identificar hogares de menores ingresos.

### Muestras naturales: comparaciones causales {#natural}

Este ejemplo está tomado de (@ESL y @saheart). Consideramos los siguientes datos donde queremos predecir enfermedad del corazón (chd) : 
(@saheart):

```{r}
sa_heart <- read_csv("datos/sa-heart.csv")
sa_heart <- sa_heart %>% 
  rename(presion_arterial = sbp, tabaco = tobacco, colesterol_ldl = ldl, 
         adiposidad = adiposity, historia_fam = famhist, tipo_a = typea, obesidad = obesity, 
         edad = age, enf_coronaria = chd)
sa_heart
```

```{r}
library(recipes)
set.seed(125)
sa_split <- rsample::initial_split(sa_heart, prop = 0.75)
sa_split
receta_sa <- training(sa_split) %>%
  recipe(enf_coronaria ~ .) %>%
  step_dummy(historia_fam) %>% 
  step_mutate(enf_coronaria = factor(enf_coronaria)) %>%  
  prep()
```



```{r}
sa_entrena <- receta_sa %>% juice
sa_boosted <- boost_tree(trees = 3000, mode = "classification", 
                        learn_rate = 0.001, tree_depth = 2, 
                        sample_size = 0.5) %>%
  set_engine("xgboost") %>% 
  fit(enf_coronaria ~ ., data = sa_entrena)
```

Podemos evaluar este modelo y afinar parámetros también. Nos interesa
aqui interpretar el efecto de las variables en este modelo. Para eso
consideramos la gráfica de dependencia parcial de la 
prevalencia de enfermedad de corazón y la
variable obesidad

```{r, fig.width=5, fig.height=3.5}
library(pdp)
pdp_ob <- pdp::partial(sa_boosted$fit,  pred.var = "presion_arterial",
  plot = TRUE, plot.engine = "ggplot2", prob = TRUE,
  train = sa_entrena %>% dplyr::select(-enf_coronaria))
pdp_ob + xlab("Presión arterial sistólica") + ylab("Predicción promedio")
```

La interpretación correcta de este gráfica de **dependencia parcial** (ver @ESL) 
depende del hecho de que
este es un estudio retrospectivo, donde **algunos pacientes con riesgo de enfermedad
de corazón sufrieron intervenciones para reducir su riesgo**, entre los
que está tomar medicinas para reducir la presión. Una interpretación
causal de reducciones de la presión arterial como promotora de 
enfermedades del corazón es incorrecta y potencialmente peligrosa (ver más en @ESL).


# Retos en construcción y desarrollo de predictores

## Fugas Entrenamiento Validación

Veremos varios ejemplos de cómo las fugas de entrenamiento a validación producen estimaciones
sesgadas del desempeño de predictores.

### Selección de variables antes de dividir los datos


```{block2, type='resumen'}
Cualquier paso de preprocesamiento debe hacerse sin usar datos de validación. Esto incluye
cuando usamos métodos como validación cruzada
```


Este ejemplo es originalmente de [@ESL], y utilizaremos datos sintéticos, 
generados con el siguiente proceso:

1. Simulamos variables respuesta $y$ con distribución binomial,  
2. Simulamos 1000 covariables independientes, cada una con distribución normal
estándar.

```{r, message = FALSE}
simular <- function(n = 100, p = 500, prob = 0.5){
  datos <- map(1:p, ~ rnorm(n)) %>% 
    bind_cols()
  datos$y <- rbinom(n, 1, prob)
  datos
}
set.seed(8234)
datos_entrena <- simular(n = 200, p = 1000)
datos_prueba <- simular(n = 2000, p = 1000)
dim(datos_entrena)
datos_entrena %>% group_by(y) %>% tally() %>% kable()
```

Nuestra selección de variables está dada por la siguiente función. Esta función *selecciona las
variables más correlacionadas con la respuesta**. Por sí solo este método no es incorrecto, pero cuando
se ejecuta sobre los datos que se usarán en validación (validación cruzada), entonces la estimación
de desempeño es optimista:

```{r}
seleccionar <- function(datos, num_var = 10){
  correlaciones <- datos %>% 
    pivot_longer(cols = matches("V"), names_to = "variable", values_to = "x") %>% 
    group_by(variable) %>% 
    summarise(corr = abs(cor(y, x))) %>% 
    arrange(desc(corr)) 
  # seleccionar 
  seleccionadas <- correlaciones %>% 
    top_n(num_var, wt = corr) %>% 
    pull(variable)
  datos %>% select(one_of(c("y", seleccionadas)))
}

```

#### Método erróneo {-}

Aquí vemos las variables que fueron seleccionadas:

```{r}
datos_filtrados <- seleccionar(datos_entrena)
datos_filtrados %>% head %>% 
  mutate_if(is.numeric, round, 3) %>% kable()
```

Para cualquier corte de validación que hagamos (ya sea que separemos un conjunto de datos, o hagamos
validación cruzada), el porcentaje de aciertos parece ser mayor a 0.5:

```{r, message = FALSE}
corte_validacion <- datos_filtrados %>% sample_frac(0.7)
valida <- anti_join(datos_filtrados, corte_validacion)
modelo_1 <- glm(y ~ ., corte_validacion, family = "binomial")
mean(as.numeric(predict(modelo_1, valida) > 0) == valida$y) %>% round(2)
```

Sin embargo, el desempeño real del modelo será:

```{r}
mean(as.numeric(predict(modelo_1, datos_prueba) > 0) == datos_prueba$y) %>% round(2)
```

#### Método correcto {-}

La selección de variables debe hacerse en cada vuelta de validación cruzada:

```{r, message=FALSE}
corte_validacion <- datos_entrena %>% sample_frac(0.7)
datos_filtrados_corte <- seleccionar(corte_validacion)
valida <- anti_join(datos_entrena, corte_validacion)
modelo_1 <- glm(y ~ ., datos_filtrados_corte, family = "binomial")
mean(as.numeric(predict(modelo_1, valida) > 0) == valida$y) %>% round(2)
```


### Sobremuestrear antes de particionar

En este ejemplo veremos que sobremuestrear una clase chica antes de separar 
datos de validación o hacer validación cruzada puede producir estimaciones 
demasiado optimistas del error de predicción.

Supongamos que tenemos desbalance severo entre nuestras dos clases:

```{r}
set.seed(99134)
datos_desbalance <- simular(n = 500, p = 20, prob = 0.1) %>% 
  mutate(y = factor(y, levels = c(1, 0)))
datos_desbalance %>% group_by(y) %>% tally() %>% kable()
```


#### Manera incorrecta {-}

Supongamos que primero aplicamos (SMOTE)[@chawla] para intentar balancear los 
datos:

```{r}
receta_balance <- recipe(y ~ ., datos_desbalance) %>%
  step_smote(y) %>%
  prep()
datos_smote <- juice(receta_balance) 
```

Obteniendo así,

```{r}
datos_smote %>% group_by(y) %>% tally() %>% kable()
```

Ahora separamos entrenamiento y validación

```{r}
sep_datos_smote <- initial_split(datos_smote)
entrena_smote <- training(sep_datos_smote)
prueba_smote <- testing(sep_datos_smote)
```

Y ajustamos un bosque aleatorio para clasificación:

```{r}
metricas <- metric_set(accuracy, recall, precision)
bosque <- rand_forest(trees = 500, mtry = 20, mode = "classification") %>%
  set_engine("ranger") %>%
  fit(y ~ ., data = entrena_smote)
bosque %>%
  predict(prueba_smote) %>%
  bind_cols(prueba_smote) %>% 
  metricas(truth = y, estimate = .pred_class) %>%
  mutate_if(is.numeric, round, 3) %>% kable
```

En primera instancia parece ser que el desempeño es muy bueno. Sabemos que esto 
es ficticio, pues no hay relación de $y$ con el resto de las covariables.


#### Manera correcta {-}

Antes de hacer el rebalanceo de clases separamos entrenamiento y validación. Si se
quiere, esta parte puede hacerse usando muestreo estratificado, por ejemplo, pero aquí
la construimos con muestreo aleatorio simple:

```{r}
sep_datos <- initial_split(datos_desbalance, prop = 0.5)
entrena <- training(sep_datos)
prueba <- testing(sep_datos)
```

```{r}
receta_balance <- recipe(y ~ ., data = entrena) %>%
  step_smote(y) %>%
  prep()
entrena_balanceado <- juice(receta_balance)
```

```{r}
bosque_1 <- rand_forest(trees = 500, mtry = 20, mode = "classification") %>%
  set_engine("ranger") %>%
  fit(y ~ ., data = entrena_balanceado)
bosque_1 %>%
  predict(prueba) %>%
  bind_cols(prueba) %>% 
  metricas(truth = y, estimate=.pred_class) %>% 
  mutate_if(is.numeric, round, 3) %>% 
  kable()
```

Aunque el accuracy parece alto, la precisión y la sensibilidad son cero. Un clasificador trivial
que siempre predice la clase dominante puede tener mejor exactitud que el que hemos construido.

## Fugas en implementación

### Variables no disponibles al momento de predicción

En este caso mostramos un ejemplo donde se utiliza erróneamente una variable que
no estará disponible al momento de hacer las predicciones (datos de [@greene]).

```{r}
credito <- read_csv("datos/AER_credit_card_data.csv") %>% 
  rename(gasto = expenditure, dependientes = dependents, ingreso = income,
         edad = age, propietario = owner) %>% 
  mutate(propietario = fct_recode(propietario, c(si = "yes")))
credito %>% head %>% 
  mutate_if(is.numeric, round, 1) %>% kable()
```

Queremos construir un modelo para predecir que solicitudes fueron aceptadas y automatizar
el proceso de selección. Usamos regresión logística con keras y penalización L2:

```{r}
set.seed(823)
credito_split <- initial_split(credito)
entrena <- training(credito_split)
prueba <- testing(credito_split)
```

```{r}
# preparacion de datos
credito_receta <- recipe(card ~ ., credito) %>%
  step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal(), -card) 
# modelo
modelo_regularizado <- 
  logistic_reg(penalty = 1) %>% 
  set_engine("keras", epochs = 500, verbose = FALSE)  %>% 
  set_mode("classification") 
```

```{r, message=FALSE, warning = FALSE}
# ajustar parametros de preprocesamiento
receta_prep <- credito_receta %>% prep(entrena)
# preprocesar datos
entrena_prep <- bake(receta_prep, entrena)
prueba_prep <- bake(receta_prep, prueba)
# ajustar modelo
ajuste <- modelo_regularizado %>% 
  fit(card~ gasto + dependientes + ingreso + edad + propietario_si, data = entrena_prep)
```

```{r}
# evaluar
metricas <- metric_set(accuracy, recall, precision)
ajuste %>% predict(prueba_prep) %>%
  bind_cols(prueba) %>% 
  metricas(truth = factor(card), estimate = .pred_class) %>% 
  mutate_if(is.numeric, round, 3) %>% 
  kable()
```

Y parece tener un desempeño razonable. Si quitamos la variable *expenditure* se
degrada totalmente el desempeño del modelo:

```{r}
ajuste_2 <- modelo_regularizado %>% 
  fit(card~ dependientes + ingreso + edad + propietario_si, data = entrena_prep)
ajuste_2 %>% predict(prueba_prep) %>%
  bind_cols(prueba) %>% 
  metricas(truth = factor(card), estimate = .pred_class) %>% 
  mutate_if(is.numeric, round, 3) %>% 
  kable()
```

La sensibilidad es muy mala y la precisión no se puede calcular pues el modelo no hace predicciones positivas para el conjunto de prueba. 

La razón de esta degradación en el desempeño es que *gasto* se refiere a uso de tarjetas de crédito. Esto incluye
la tarjeta para la que queremos hacer predicción de aceptación:

```{r}
entrena %>% 
  mutate(algun_gasto = gasto > 0) %>% 
  group_by(algun_gasto, card) %>% 
  tally() %>% 
  kable()
```

Lo que nos indica que algún gasto probablemente incluye el gasto en la tarjeta actual,
y la variable *gasto* es medida posteriormente a la entrega de la tarjeta:

- El desempeño de este modelo para nuevas aplicaciones será muy malo, pues la variable
*gasto*, en el momento de la aplicación, evidentemente no cuenta cuánto va a gastar
cada cliente en el futuro.

## Desbalance de clases

Cuando tenemos desbalance severo de clases podemos enfrentar dos problemas:
existen en términos absolutos muy pocos elementos de una clase para poder
discriminarla de manera efectiva (aún cuando tengamos los atributos
o *features* correctos), o métodos usuales de evaluación de predicción 
son deficientes para evaluar el desempeño de predicciones.  

Consideremos los siguientes datos (del paquete de @ISLR):

The data contains 5822 real customer records. Each record consists of 86 variables, containing sociodemographic data (variables 1-43) and product ownership (variables 44-86). The sociodemographic data is derived from zip codes. All customers living in areas with the same zip code have the same sociodemographic attributes. Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy. Further information on the individual variables can be obtained at http://www.liacs.nl/~putten/library/cc2000/data.html


Queremos predecir la variable *Purchase*:

```{r}
caravan <- read_csv("datos/caravan.csv") %>% 
  mutate(MOSTYPE = factor(MOSTYPE),
         MOSHOOFD = factor(MOSHOOFD)) %>% 
  mutate(Compra = fct_recode(Purchase, si = "Yes", no = "No")) %>%
  mutate(Compra = fct_rev(Compra)) %>% 
  select(-Purchase)
nrow(caravan)
caravan %>% count(Compra) %>% 
  mutate(pct = 100 * n / sum(n)) %>% 
  mutate(pct = round(pct, 2))
```

Esta es la distribución natural de respuesta que vemos en los datos, y tenemos relativamente pocos
datos en la categoría "Si".

Usaremos muestreo estratificado para obtener proporciones similares en conjuntos de entrenamiento y prueba

```{r}
set.seed(823)
caravan_split = initial_split(caravan, strata = Compra, prop = 0.9)
caravan_split
entrena <- training(caravan_split)
prueba <- testing(caravan_split)
```

Y usaremos regresión logística (lo mismo aplica para otros método que produzcan probabilidades
de clase, como boosting, árboles aleatorios o redes neuronales):

```{r}
library(tune)
# preparacion de datos
caravan_receta <- recipe(Compra ~ ., entrena) %>%
  step_dummy(all_nominal(), -Compra)
caravan_receta_prep <- caravan_receta %>% prep
# modelo
modelo_log <- 
  logistic_reg() %>% 
  set_engine("glm")  %>% 
  set_mode("classification") %>% 
  fit(Compra ~ ., data = caravan_receta_prep %>% juice)
```

#### Análisis incorrecto {-}

La matriz de confusión de prueba es:

```{r, message=FALSE, warning = FALSE}
prueba_procesado <- bake(caravan_receta_prep, prueba)
predictions_glm <- modelo_log %>%
  predict(new_data = prueba_procesado) %>%
  bind_cols(prueba_procesado %>% select(Compra))
predictions_glm %>%
  conf_mat(Compra, .pred_class)
```

Y la de entrenamiento:

```{r, message=FALSE, warning = FALSE}
predictions_ent_glm <- modelo_log %>%
  predict(new_data = juice(caravan_receta_prep)) %>%
  bind_cols(juice(caravan_receta_prep) %>% select(Compra))
predictions_ent_glm %>%
  conf_mat(Compra, .pred_class)
```


Y obtenemos desempeño pobre según esta matriz de confusión (prueba y entrenamiento).
La sensibilidad es muy baja, aunque
la especificidad (tasa de correctos negativos) sea alta. Una conclusión típica es que 
*el modelo no tiene valor predictivo*, o que
*es necesario sobre muestrear la
clase de ocurrencia baja*.

#### Análisis correcto {-}

En lugar de empezar con sobre/sub muestreo,
que modifica lsas proporciones naturales de las categorías en los datos, podemos trabajar con probabilidades
en lugar de predicciones de clase con punto de corte de 0.5.

Por ejemplo, podemos visualizar con una curva ROC (o curva lift, precisión-recall, o alguna otra similar que
tome en cuenta probabilidades):

```{r, fig.width = 5, fig.height = 4}
predictions_prob <- modelo_log %>%
  predict(new_data = prueba_procesado, type = "prob") %>%
  bind_cols(prueba_procesado %>% select(Compra)) %>% 
  select(.pred_si, Compra)
datos_roc <- roc_curve(predictions_prob, Compra, .pred_si)
autoplot(datos_roc) +
  xlab("1 - especificidad") + ylab("sensibilidad")
```

Donde vemos que podemos alcanzar buenos niveles de sensibilidad si aceptamos alguna degradación en la especificidad,
que originalmente es muy alta. Por ejemplo, cortando en 0.05 podemos obtener especificidad y sensibilidad
que posiblemente sean adecuadas para el problema:

```{r}
datos_roc %>% filter(abs(.threshold - 0.04) < 1e-4) %>% round(4)
```

**¿Qué pasa si hacemos sub/sobremuestreo? **

Sobremuestreamos: 
```{r}
caravan_receta_smote <- recipe(Compra ~ ., entrena) %>%
  step_dummy(MOSTYPE, MOSHOOFD) %>% 
  step_smote(Compra)
smote_prep <- prep(caravan_receta_smote)
# modelo
entrena_1 <- juice(smote_prep)
entrena_1 %>% count(Compra)
modelo_log_smote <- 
  logistic_reg() %>% 
  set_engine("glm")  %>% 
  set_mode("classification") %>% 
  fit(Compra ~ ., data = entrena_1)
```

En entrenamiento la matriz de confusión es *aparentemente* mejor:
```{r, message=FALSE, warning = FALSE}
predictions_ent_glm <- modelo_log_smote %>%
  predict(new_data = entrena_1) %>%
  bind_cols(entrena_1 %>% select(Compra))
predictions_ent_glm %>%
  conf_mat(Compra, .pred_class)
```


Pero en prueba los resultados son muy similares. Agregamos también el 
modelo construido submuestreando la clase dominante:

```{r}
entrena_sub <- caravan_receta %>% step_downsample(Compra) %>% prep() %>% juice
modelo_log_sub <- 
  logistic_reg() %>% 
  set_engine("glm")  %>% 
  set_mode("classification") %>% 
  fit(Compra ~ ., data = entrena_sub)
```


```{r}
predictions_prob <- modelo_log_smote %>%
  predict(new_data = prueba_procesado, type = "prob") %>%
  bind_cols(prueba_procesado %>% select(Compra)) %>% 
  select(.pred_si, Compra)
predictions_prob_sub <- modelo_log_sub %>% 
  predict(new_data = prueba_procesado, type = "prob") %>%
  bind_cols(prueba_procesado %>% select(Compra)) %>% 
  select(.pred_si, Compra)
datos_roc_smote <- roc_curve(predictions_prob, Compra, .pred_si)
datos_roc_sub <- roc_curve(predictions_prob_sub, Compra, .pred_si)
datos_roc_comp <- bind_rows(datos_roc %>% mutate(tipo = "natural"),
                            datos_roc_smote %>% mutate(tipo = "con sobre muestreo"),
                            datos_roc_sub %>% mutate(tipo = "con sub muestreo")
)
ggplot(datos_roc_comp,
       aes(x = 1 - specificity, y = sensitivity, colour = tipo)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()
```

- **El problema original no era que el ajuste no funcionaba, sino que evaluamos el punto 
de corte incorrecto**. Un punto de corte de 0.5 con SMOTE equivale a uno mucho más chico sin
SMOTE.

- Peor aún, las **probabilidades del modelo construido con sobremuestreo no reflejan las tasas
de ocurrencia de la respuesta que nos interesa**, lo cual puede producir resúmenes engañosos de las
tasas de respuesta que esperamos observar en producción.

## Evaluación de punto de corte 

Las mejores decisiones de punto de corte pueden hacerse con análisis de costo beneficio,
con curvas tipo *lift* basadas en ganancias y pérdidas de cada decisión. 
Aunque esta información muchas veces no está disponible, es la
situación ideal para evaluar cómo ayuda el modelo y cuánto valen las acciones que pretendemos
tomar. Es posible hacer este análisis con valores inciertos de costo beneficio.

Supongamos que estamos pensando en un tratamiento para retener estudiantes en algún programa
de entrenamiento o mejora.

- El tratamiento de retención cuesta 5000 pesos por alumno,
- Estimamos mediante experimentos o algún análisis externo que nuestro tratamiento reduce la probabilidad de abandono
en un 60\%,
- Tenemos algún tipo de valuación del valor social de que un alumno persista en el programa.

Podemos evaluar a nuestro modelo en el contexto del problema de las siguiente 
forma:

- Suponemos que trataremos a un porcentaje de los estudiantes con mayor probabilidad de rotar.
- Calculamos el costos esperado si tratamos a un porcentaje de los estudiantes: 
simulamos reduciendo su probabilidad de abandono por el tratamiento y sumamos los costos de tratarlos.
- Comparamos contra el escenario de no aplicar ningún tratamiento

No es necesario usar medidas muy técnicas para dar un resumen de cómo
nos puede ayudar el tratamiento y modelo para mantener el valor de la cartera:

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
clientes <- tibble(id = 1:20000, valor = 50000) %>% 
    mutate(prob_pred = rbeta(length(valor), 1, 2)) 
```

```{r echo = FALSE, fig.width = 8, fig.height = 5}
calc_perdida_aleatorio <- function(corte, factor_ret, costo){
    n_tratados <- filter(clientes, prob_pred >= corte) %>% nrow()
    p_tratados <- n_tratados / nrow(clientes)
    clientes <- clientes %>%
        mutate(aleatorio = rbinom(length(prob_pred), 1 , p_tratados))
    perdida_sin_accion <- filter(clientes, aleatorio == 1) %>%
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    perdida_aleatorio <- filter(clientes, aleatorio==1) %>%
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, 
          prob_pred*(aleatorio*factor_ret + (1-aleatorio))) == 1, valor, 0)) %>%
        summarise(total = sum(costo)) %>% 
        pull(total)
    total <- perdida_aleatorio - (perdida_sin_accion)  +
      costo*nrow(filter(clientes, prob_pred >= corte))
    total
}
calc_perdida_modelo <- function(corte, factor_ret, costo){
    perdida_no_trata <- filter(clientes, prob_pred < corte) %>% 
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    perdida_trata <- filter(clientes, prob_pred >= corte) %>% 
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred*factor_ret) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    perdida_cf <- filter(clientes, prob_pred >= corte) %>%  
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    total <- perdida_no_trata +  perdida_trata - (perdida_no_trata + perdida_cf) +
      costo*nrow(filter(clientes, prob_pred >= corte)) 
    total
}
perdidas_sim_aleatorio <- map_dfr(rep(seq(0,1, 0.1), 50), 
    function(x){
      perdida_sim <- calc_perdida_aleatorio(x, 0.6, 5000)
      tibble(perdida = perdida_sim, corte = x)
    }) %>% bind_rows %>% mutate(tipo = "Tratamiento aleatorio")
perdidas_sim_modelo <-  map_dfr(rep(seq(0,1, 0.1), 50), 
    function(x){
      perdida_sim <- calc_perdida_modelo(x, 0.6, 5000)
      tibble(perdida = perdida_sim, corte = x)
    }) %>% bind_rows %>% mutate(tipo = "Tratamiento modelo")
perdidas_sim <- bind_rows(perdidas_sim_aleatorio, perdidas_sim_modelo)
```

```{r}
ggplot(filter(perdidas_sim, tipo=="Tratamiento modelo"),
       aes(x = factor(corte), y = - perdida / 1e6)) + 
  geom_boxplot() + ylab("Ganancia incremental (millones)") +
  xlab("Corte inferior de tratamiento (probabilidad)") +
  labs(subtitle = "Ganancia vs ninguna acción") + theme_minimal()
```

Podemos escoger un punto de corte entre 0.2 y 0.3, por ejemplo, o hacer más simulaciones
para refinar la elección. 

Si queremos separar el efecto del tratamiento con el efecto del tratamiento aplicado
según el modelo, podemos comparar con la acción que consiste en tratar a los estudiantes
al azar:

```{r}
ggplot(perdidas_sim, aes(x = factor(corte), y = - perdida / 1e6, 
                         group = interaction(tipo, corte), colour = tipo)) + 
  geom_boxplot() + ylab("Ganancia incremental (millones)") +
  xlab("Corte inferior de tratamiento (probabilidad)") +
  labs(subtitle = "Ganancia vs ninguna acción") + theme_minimal()
```

- La conclusión es que el modelo **ayuda considerablemente a la focalización del programa** (el área entre
las dos curvas mostradas arriba).

## Equidad con atributos protegidos

El siguiente ejemplo es derivado de (@Hardt). Supongamos que tenemos un atributo protegido $A$ que
tiene dos valores: azul y naranja. Naranja es el grupo minoritario  desaventajado.
Usaremos datos simulados como sigue: el atributo *score* está asociado
al atributo protegido:


```{r}
inv_logit <- function(x){
  1 / (1 + exp(-x))
}
simular_datos <- function(n = c(10000, 2000)){
  score_azul <- pmax(rnorm(n[1], 50, 10), 0)
  score_naranja <- pmax(rnorm(n[2], 40, 10), 0)
  azul <- tibble(tipo = "azul", score = score_azul)
  naranja <- tibble(tipo = "naranja", score = score_naranja)
  datos <- bind_rows(azul, naranja) %>% 
    mutate(coef_0 = ifelse(tipo == "azul", 0.0, 0),
           prob_real_pos = inv_logit(-1 + coef_0 + 0.1 * (score-40))) %>% 
    mutate(atr_1 = rpois(nrow(.), 3))
  datos %>% select(-coef_0) %>% 
    mutate(paga = map_dbl(prob_real_pos, ~ rbinom(1, 1, .x))) %>% 
    select(-prob_real_pos)
}
set.seed(1221)
tbl_datos <- simular_datos()
```

Una gráfica de conteos para el score obtenemos un grupo minoritario con valores de la variable score
más baja:

```{r, fig.width = 4, fig.height = 2}
ggplot(tbl_datos, aes(x = score, fill = tipo)) + geom_histogram()
```

Ajustamos un modelo simple de regresión logística:

```{r}
reg_log <- glm(paga ~  score + atr_1 + tipo, tbl_datos, family = "binomial")
tbl_datos <- tbl_datos %>% mutate(prob_pos = predict(reg_log, type = "response"))
```



Las tasas reales de cumplimiento son iguales para los dos grupos. En primer lugar, consideremos
una estrategia donde aplicamos el mismo punto de corte para todos los grupos

```{r}
resultado_cortes <- function(tbl_datos, cortes){
  resultado <- tbl_datos %>% 
    mutate(recibe = ifelse(tipo == "azul", prob_pos > cortes[1], prob_pos > cortes[2]),
           decision = ifelse(recibe, "Aceptado", "Rechazado")) 
  resultado %>% group_by(tipo, decision, paga) %>% count() %>% 
    ungroup()
}
resultados_conteo <- resultado_cortes(tbl_datos, c(0.6, 0.6)) 
resultados_conteo
```


```{r}
resultados_conteo %>%
  group_by(tipo, decision) %>%
  summarise(n = sum(n)) %>% 
  mutate(total = sum(n)) %>% 
  mutate(prop = n / total) %>% 
  filter(decision == "Aceptado")
```

Nótese que el grupo naranja ha recibido considerablemente menos aceptaciones que el grupo azul, tanto
en total como en proporción. Más aún, con la precisión o tasa de verdaderos positivos
podemos evaluar qué proporción de los que cumplirían si fueran aceptados fueron 
aceptados según nuestro punto de corte:

```{r}
resultados_conteo %>% 
  filter(paga == 1) %>% 
  group_by(tipo) %>% 
  mutate(tvp = n / sum(n)) %>% 
  filter(decision == "Aceptado")
```

y vemos que el grupo naranja también está en desventaja, pues entre los que cumplen hay menos decisiones
de aceptación.

El siguiente paso es considerar **paridad demográfica**. En este caso, decidimos dar el mismo número
de préstamos a cada grupo, dependiendo de su tamaño.

```{r}
calcular_puntos_paridad <- function(tbl_datos, prop){
  tbl_datos %>% group_by(tipo) %>% 
    summarise(corte = quantile(prob_pos, 1 - prop))
}
cortes_paridad_tbl <- calcular_puntos_paridad(tbl_datos, 0.45)
cortes_paridad_tbl
```

El corte para azul es más exigente que para naranja. En sí eso no es un problema pero observamos:

```{r}
cortes_paridad <- cortes_paridad_tbl %>% pull(corte)
resultados_conteo <- resultado_cortes(tbl_datos, cortes_paridad) 
resultados_conteo %>% 
  filter(paga == 1) %>% 
  group_by(tipo) %>% 
  mutate(tvp = n / sum(n)) %>% 
  filter(decision == "Aceptado")
```

Y así que además de ser más exigente con el grupo azul, a los que cumplen del grupo azul también
se les otorga menos decisiones de aceptación. Adicionalmente, se aceptan considerablemente menos
personas de la población.

La solución de **igual de de oportunidad** es cortar de forma que la tasa de aceptación dentro del grupo
de los que pagan sea similar para ambas poblaciones, lo que ocurre aproximadamente en 0.35:

```{r}
calcular_cortes_oportunidad <- function(tbl_datos, prop){
  tbl_datos %>% 
    filter(paga==1) %>% 
    group_by(tipo) %>% 
    mutate(rank_p = rank(prob_pos) / length(prob_pos) ) %>% 
    filter(rank_p < prop) %>% 
    top_n(1, rank_p) %>% 
    select(tipo, corte = prob_pos)
}
cortes_op <- calcular_cortes_oportunidad(tbl_datos, 0.35)
resultados_conteo <- resultado_cortes(tbl_datos, cortes_op %>% pull(corte)) 
resultados_conteo %>% 
  filter(paga == 1) %>% 
  group_by(tipo) %>% 
  mutate(tvp = n / sum(n)) %>% 
  filter(decision == "Aceptado")
```


**Nota**: es importante notar que si la variable de resultado positivo es injustamente asignada, entonces
este método no resuelve el problema. En este caso es relevante entender cuáles son los criterios con
los que se considera un resultado existoso dependiendo de el grupo del atributo protegido (por ejemplo,
si a un segmento particular se le permite mayores atrasos en los pagos y a otro menos, o un grupo se
considera un delicuente reincidente por una ofensa mucho menor que otros grupos).


# Retos de rendición de cuentas

## Interpretabilidad

Podemos usar medidas como importancia de permutaciones para examinar modelos. En este ejemplo,
regresamos a nuestro ejemplo de predicción de aceptación de solicitudes de crédito, y consideramos
la importancia basada en permutaciones [@molnar2019]:

```{r}
set.seed(823)
credito_split <- initial_split(credito)
entrena <- training(credito_split)
prueba <- testing(credito_split)
# preparacion de datos
credito_receta <- recipe(card ~ ., credito) %>%
  step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal(), -card) 
# modelo
modelo_regularizado <- 
  logistic_reg(penalty = 1) %>% 
  set_engine("keras", epochs = 500, verbose = FALSE)  %>% 
  set_mode("classification") 
```

```{r, message=FALSE, warning = FALSE}
# ajustar parametros de preprocesamiento
receta_prep <- credito_receta %>% prep(entrena)
# preprocesar datos
entrena_prep <- bake(receta_prep, entrena)
prueba_prep <- bake(receta_prep, prueba)
# ajustar modelo
ajuste <- modelo_regularizado %>% 
  fit(card~ gasto + dependientes + ingreso + edad + propietario_si, data = entrena_prep)
```

```{r}
library(iml)
modelo <- ajuste$fit
entrena_x <- entrena_prep %>% dplyr::select(gasto, dependientes, ingreso, edad, propietario_si)
predictor <- Predictor$new(modelo, data = entrena_x, y = ifelse(entrena_prep$card == "yes",2,1) ,
                           type = "prob")
imp <- FeatureImp$new(predictor, loss = "ce", compare = "difference")
plot(imp) + theme_minimal()
```

- Vemos claramente que para esta red sin capas ocultas, la importancia se concentra en un solo
predictor, *gasto*, que como vimos representa una fuga de información. Este diagnóstico es útil en general,
y aunque no tan dramático como este ejemplo, puede señalar cuáles variables es importante considerar con cuidado.

- Es importante considerar también el efecto de variables asociadas a grupos protegidos, y de ser necesario, examinar
con cuidado cómo afectan las predicciones.

- Modelos parsimoniosos, que usan menos atributos, facilitan el análisis, el mantenimiento
del flujo de datos, y reducen
exponernos a problemas de fugas o efectos indeseables.

## Explicación de predicciones

Para explicar predicciones individuales podemos usar los valores de Shapley (ver [@molnar2019], [@shapley]). Estas gráficas indican
la contribución asignada de cada atributo a una predicción individual, bajo la idea de considerar efectos marginales sobre la predicción
dependiendo de la presencia o ausencia de otros atributos. **Las contribuciones obtenidas** suman la diferencia que
hay entre la predicción particular y la predicción promedio.

Pueden examinarse también promedios a lo largo de grupos de interés.


Consideramos el ejemplo de factores para detectar una enfermedad del corazón en el estudio de Sudáfrica que vimos
en este [ejemplo](#natural)

```{r}
modelo_sa <- sa_boosted$fit
sa_entrena_x <- sa_entrena %>% dplyr::select(-enf_coronaria)
predict_fun <- function(object, newdata){
  new_data_x = xgb.DMatrix(data.matrix(newdata), missing = NA)
  results<-predict(modelo_sa, new_data_x)
  return(results)
}
predictor <- Predictor$new(modelo_sa, data = sa_entrena_x, y = sa_entrena$chd ,
                           type = "prob", predict.function = predict_fun)

# el caso de interés es el caso 15
valores_shapley <- Shapley$new(predictor, x.interest = (sa_entrena_x[15, ]))
valores_shapley$plot() + theme_minimal()
```

En este caso, varias medidas contribuyen positivamente a la probabilidad de enfermedad del corazon, como
es uso de tabaco, edad, y mediciones de colesterol. Estas constribuciones explican la probabiidad tan alta de este individuo particular.

En contraste, la siguiente persona está cerca del promedio, aumentando positivamente la probabiidad la edad y medida
de colesterol, pero negativamente el no uso de tabaco y ninguna historia familiar de diabetes:

```{r}
# el caso de interés es el caso 24
valores_shapley <- Shapley$new(predictor, x.interest = (sa_entrena_x[24, ]))
valores_shapley$plot() + theme_minimal()
```

**Observación**: igual que en el modelo y en las gráficas de dependencia parcial que discutimos arriba, 
estos coeficientes **no** deben interpretarse de manera causal (por ejemplo: es necesario bajar el colesterol
para estos dos individuos). Esta es la información que usa el modelo para construir la predicción a partir de la predicción
promedio sobre la población.


Podemos calcular los valores de shapley para dos grupos de edad, por ejemplo.


# Referencias
