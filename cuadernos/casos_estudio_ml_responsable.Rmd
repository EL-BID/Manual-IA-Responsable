---
title: "Casos de estudio"
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: cerulean
    css: [ "./css/style.css"]
link-citations: yes
bibliography:
- ../documento/referencias.bib
- ../documento/packages.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

En este cuaderno mostraremos varios ejemplos de los retos y soluciones explicadas
en el documento principal. Usamos distintos tipos de modelos (lineales, basados en árboles y otros) y distintas implementaciones (R, keras, xgboost) para mostrar
que estos problemas se presentan independientemente de la elección de herramientas
particulares.

```{r}
library(tidyverse)
library(recipes)
library(themis)
library(rsample)
library(parsnip)
library(yardstick)
library(workflows)
library(tune)
library(knitr)
library(patchwork)
```


# Retos intrínsecos a los datos

## Reto: mala correspondencia de métrica y objetivos

Usar modelos que predicen la métrica incorrecta puede llevar a tomar decisiones incorrectas.
A veces el problema es claro, cuando la métrica sustituto tiene deficiencias obvias, y en otras
puede ser más sutil.

En este ejemplo se busca predecir la demanda de cierto producto, y se cuenta
con datos históricos de inventario, ventas y una covariable indicadora de ventas:

```{r, echo=FALSE, include=FALSE}
set.seed(124)
semana <- 1:100
indicador <- rnorm(100, 0, 20)
inventario <- 100 + 7*sqrt(1:100) + rpois(100, 50)
demanda <- 30 + 1.2*1:100 + indicador + rpois(100, 100)
ventas <- tibble(
    semana = 1:100,
    inventario = inventario, 
    demanda = demanda, 
    ventas = pmin(inventario, demanda),
    indicador = indicador, 
    agotamiento = ifelse(ventas==inventario, 1, 0))
ventas %>% select(-demanda)
```

Separamos los datos en entrenamiento y prueba, ajustando el modelo con el 
subconjunto de datos de entrenamiento. En este caso se utiliza un modelo lineal
con variable dependiente ventas y covariables de semana y una variable indicador.

```{r}
entrena <- ventas %>% filter(semana < 80)
prueba <- ventas %>% filter(semana >= 80)
entrena %>% select(-demanda) %>%  head() %>% kable()
mod_lineal <- lm(ventas ~ semana + indicador, data = ventas)
mod_lineal
```
Evaluamos el error de predicción.

```{r}
preds <- predict(mod_lineal, newdata = prueba)
mean(abs(preds - prueba$ventas))/mean(prueba$ventas)
```

El error porcentual es bajo. Los datos ajustados y predicciones se ven como sigue:

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
ggplot(ventas_larga %>% mutate(unidades = ifelse(tipo=="ventas" & semana > 80, NA, unidades)), 
       aes(x = semana, y = unidades, group = tipo, colour = tipo)) + 
  geom_line() +
  geom_vline(xintercept = 80)
```

Pero tomar decisiones de demanda o inventario es equivocado. La razón es que 
existen agotamientos de inventario, que marcamos con rojo en la siguiente gráfica.

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
ggplot(ventas_larga %>% mutate(unidades = ifelse(tipo=="ventas" & semana > 80, NA, unidades)), aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_point(data = filter(ventas, agotamiento==1, semana < 80), aes(y = ventas), colour = "red") + 
  geom_vline(xintercept = 80)
```

Si usáramos la política sugerida por las predicciones (por ejemplo 5% más), 
veríamos las ventas de la primera gráfica a continuación. Sin embargo, si 
usáramos una política de inventario con 250 unidades, observaríamos:


```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  mutate(ventas = ifelse(semana > 80, pmin(1.02*pred, demanda), ventas)) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
g1 <- ggplot(ventas_larga, aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_vline(xintercept = 80) + labs(subtitle = "Inventario: Predicciones + 2%")
```

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  mutate(ventas = ifelse(semana > 80, pmin(300, demanda), ventas)) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
g2 <- ggplot(ventas_larga, aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_vline(xintercept = 80)+ labs(subtitle = "Inventario: 300 unidades cte")
```

```{r}
g1 / g2
```

Entonces, 

- La política basada en las predicciones **exacerba** el problema de los agotamientos.
- Un uso no pensado de datos sin considerar el proceso generador de los mismos puede producir 
errores grandes en la decisión de políticas
- En este caso, la confusión proviene de no separar los conceptos de demanda y ventas. Otros
indicadores de demanda o modelos más adecuados ayudarían a resolver el problema.


## Muestras naturales y sesgo

Cuando las muestras de entrenamiento son diferentes a las poblaciones, donde
se van a aplicar los modelos, existen dificultades en validar correctamente las
predicciones.

### Muestras naturales: mala representatividad 

Para este ejemplo utilizaremos datos de la encuesta nacional de ingresos y gastos
en hogares de México [@enigh], para simular un escenario que queremos ejemplificar.

```{r}
set.seed(128)
encuesta_ingreso <- read_csv("datos/enigh-ejemplo.csv")
datos_ingreso <- encuesta_ingreso %>% 
  mutate(num_focos = FOCOS) %>%
  mutate(ingreso_miles = (INGCOR / 1000)) %>% 
  mutate(tel_celular = ifelse(SERV_2 == 1, "Sí", "No")) %>% 
  mutate(marginacion = fct_reorder(marginación, ingreso_miles, median)) %>% 
  rename(ocupadas = PEROCU) %>% 
  select(ingreso_miles, num_focos, tel_celular, marginacion, ocupadas)
```

```{r}
ingreso_split <- initial_split(datos_ingreso, prop = 0.7)
entrena <- training(ingreso_split)
prueba <- testing(ingreso_split)
```

Supongamos que nos interesa estimar el ingreso de los hogares, para ello usamos 
una encuesta por teléfono celular, más aún, supongamos que solo accedimos a 
zonas que no tienen marginación muy alta.

```{r}
entrena_sesgada <- filter(entrena, tel_celular == "Sí", marginacion=="Muy bajo")
sesgados_split <- initial_split(entrena_sesgada)
entrena_sesgo <- training(sesgados_split)
validacion <- testing(sesgados_split)
```

Construimos nuestro modelo de ingreos con los datos disponibles.

```{r}
mod_1 <- lm(log(1 + ingreso_miles) ~ num_focos + ocupadas, data = entrena_sesgo)
```

Y evaluamos el error en una muestra de prueba construida con datos con las mismas
características que los datos de entrenamiento (hogares con teléfono celular y 
grado de marginación no muy alto).

```{r}
preds_val <- predict(mod_1, newdata = validacion)
mean(abs(preds_val - log(1 + validacion$ingreso_miles))) %>% round(2)
```

El error en una muestra más similar a la población que se pretende aplicar el 
algoritmo es mayor:

```{r}
preds_prueba <- predict(mod_1, newdata = prueba)
mean(abs(preds_prueba - log(1 + prueba$ingreso_miles))) %>% round(2)
prueba$pred <- preds_prueba
```

Sin embargo, el principal problema se refleja en la siguiente gráfica: cada punto 
representa un hogar, la muestra es más similar a la población donde se aplicará la
metodología, y en el 
eje horizontal graficamos la predicción de los hogares utilizando el modelo,
mientras que el eje vertical corresponde al ingreso de cada hogar. Como referencia
agregamos la recta $y = x$, y un suavizador.

```{r, fig.width = 5, fig.height = 4}
ggplot(prueba %>% filter(pred < log(100)), 
  aes(x = exp(pred) - 1, y = ingreso_miles, group=1)) + geom_jitter() +
  geom_abline() + geom_smooth(method = "loess", se = FALSE) +
  scale_x_log10() + scale_y_log10() + xlab("Predicción (miles al trimestre)") +
  ylab("Ingreso corriente (miles al trimestre)") 
```

Tenemos entonces dos problemas:

1. El sesgo produce un error considerablemente más grande en la implementación que en la validación
2. Peor aún, el sesgo es mayor para hogares de menores ingresos, lo cual puede producir
una focalización mediocre si buscamos identificar hogares de menores ingresos.

### Muestras naturales: comparaciones causales

Este ejemplo está tomado de (@ESL y @saheart). Consideramos los siguientes datos donde queremos predecir enfermedad del corazón (chd) : 
(@saheart):

```{r}
sa_heart <- read_csv("datos/sa-heart.csv")
sa_heart
```

```{r}
library(recipes)
set.seed(125)
sa_split <- rsample::initial_split(sa_heart, prop = 0.75)
sa_split
receta_sa <- training(sa_split) %>%
  recipe(chd ~ .) %>%
  step_dummy(famhist) %>% 
  step_mutate(chd = factor(chd)) %>%  
  prep()
```



```{r}
sa_entrena <- receta_sa %>% juice
sa_boosted <- boost_tree(trees = 3000, mode = "classification", 
                        learn_rate = 0.001, tree_depth = 2, 
                        sample_size = 0.5) %>%
  set_engine("xgboost") %>% 
  fit(chd ~ ., data = sa_entrena)
```

Podemos evaluar este modelo y afinar parámetros también. Nos interesa
aqui interpretar el efecto de las variables en este modelo. Para eso
consideramos la gráfica de dependencia parcial de la 
prevalencia de enfermedad de corazón y la
variable obesidad

```{r}
library(pdp)
pdp_ob <- pdp::partial(sa_boosted$fit,  pred.var = "sbp",
  plot = TRUE, plot.engine = "ggplot2", prob = TRUE,
  train = sa_entrena %>% dplyr::select(-chd))
pdp_ob
```

La interpretación correcta de este gráfica proviene del hecho de que
este es un estudio retrospectivo, donde **algunos pacientes con riesgo de enfermedad
de corazón sufrieron intervenciones para reducir su riesgo**, entre los
que está tomar medicinas para reducir la presión. Una interpretación
causal de reducciones de la presión arterial como promotora de 
enfermedades del corazón es incorrecta y potencialmente peligrosa (ver más en @ESL).


# Retos en construcción y desarrollo de predictores

## Fugas Entrenamiento Validación

Veremos varios ejemplos de cómo las fugas de entrenamiento a validación producen estimaciones
sesgadas del desempeño de predictores.

### Selección de variables antes de particionar


```{block2, type='resumen'}
Cualquier paso de preprocesamiento debe hacerse sin usar datos de validación. Esto incluye
cuando usamos métodos como validación cruzada
```


Este ejemplo es originalmente de [@ESL], y utilizaremos datos sintéticos, 
generados con el siguiente proceso:

1. Simulamos variables respuesta $y$ con distribución binomial,  
2. Simulamos 1000 covariables independientes, cada una con distribución normal
estándar.

```{r, message = FALSE}
simular <- function(n = 100, p = 500, prob = 0.5){
  datos <- map(1:p, ~ rnorm(n)) %>% 
    bind_cols()
  datos$y <- rbinom(n, 1, prob)
  datos
}
set.seed(8234)
datos_entrena <- simular(n = 200, p = 1000)
datos_prueba <- simular(n = 2000, p = 1000)
dim(datos_entrena)
datos_entrena %>% group_by(y) %>% tally() %>% kable()
```

Nuestra selección de variables está dada por la siguiente función. Esta función *selecciona las
variables más correlacionadas con la respuesta**. Por sí solo este método no es incorrecto, pero cuando
se ejecuta sobre los datos que se usarán en validación (validación cruzada), entonces la estimación
de desempeño es optimista:

```{r}
seleccionar <- function(datos, num_var = 10){
  correlaciones <- datos %>% 
    pivot_longer(cols = matches("V"), names_to = "variable", values_to = "x") %>% 
    group_by(variable) %>% 
    summarise(corr = abs(cor(y, x))) %>% 
    arrange(desc(corr)) 
  # seleccionar 
  seleccionadas <- correlaciones %>% 
    top_n(num_var, wt = corr) %>% 
    pull(variable)
  datos %>% select(one_of(c("y", seleccionadas)))
}

```

#### Método erróneo {-}

Para cualquier corte de validación cruzada el error parece ser menor a 0.5:

```{r}
datos_filtrados <- seleccionar(datos_entrena)
datos_filtrados %>% head %>% kable()
```


```{r, message = FALSE}
corte_validacion <- datos_filtrados %>% sample_frac(0.7)
valida <- anti_join(datos_filtrados, corte_validacion)
modelo_1 <- glm(y ~ ., corte_validacion, family = "binomial")
mean(as.numeric(predict(modelo_1, valida) > 0) == valida$y) %>% round(2)
```

Sin embargo, el desempeño real del modelo será:

```{r}
mean(as.numeric(predict(modelo_1, datos_prueba) > 0) == datos_prueba$y) %>% round(2)
```

#### Método correcto {-}

La selección de variables debe hacerse en cada vuelta de validación cruzada:

```{r, message=FALSE}
corte_validacion <- datos_entrena %>% sample_frac(0.7)
datos_filtrados_corte <- seleccionar(corte_validacion)
valida <- anti_join(datos_entrena, corte_validacion)
modelo_1 <- glm(y ~ ., datos_filtrados_corte, family = "binomial")
mean(as.numeric(predict(modelo_1, valida) > 0) == valida$y) %>% round(2)
```


### Sobremuestrear antes de particionar

En este ejemplo veremos que sobremuestrear una clase chica antes de separar 
datos de validación o hacer validación cruzada puede producir estimaciones 
demasiado optimistas del error de predicción.

Supongamos que tenemos desbalance severo entre nuestras dos clases:

```{r}
set.seed(99134)
datos_desbalance <- simular(n = 500, p = 20, prob = 0.1) %>% 
  mutate(y = factor(y, levels = c(1, 0)))
datos_desbalance %>% group_by(y) %>% tally() %>% kable()
```


#### Manera incorrecta {-}

Supongamos que primero aplicamos (SMOTE)[@chawla] para intentar balancear los 
datos:

```{r}
receta_balance <- recipe(y ~ ., datos_desbalance) %>%
  step_smote(y) %>%
  prep()
datos_smote <- juice(receta_balance) 
```

Obteniendo así,

```{r}
datos_smote %>% group_by(y) %>% tally() %>% kable()
```

Ahora separamos entrenamiento y validación

```{r}
sep_datos_smote <- initial_split(datos_smote)
entrena_smote <- training(sep_datos_smote)
prueba_smote <- testing(sep_datos_smote)
```

Y usamos nuestro método de clasificación:

```{r}
class_metrics <- metric_set(accuracy, recall, precision, kap)
bosque <- rand_forest(trees = 500, mtry = 20, mode = "classification") %>%
  set_engine("ranger") %>%
  fit(y ~ ., data = entrena_smote)
bosque %>%
  predict(prueba_smote) %>%
  bind_cols(prueba_smote) %>% 
  class_metrics(truth = y, estimate = .pred_class) %>% 
  kable()
```

En primera instancia parece ser que el desempeño es muy bueno. Sabemos que esto 
es fictico, pues no hay relación de $y$ con el resto de las covariables.


#### Manera correcta {-}

Antes de hacer el rebalance de clases separamos entrenamiento y validación. Es 
posible hacer el muestro estratificado, por ejemplo:

```{r}
sep_datos <- initial_split(datos_desbalance, prop = 0.5)
entrena <- training(sep_datos)
prueba <- testing(sep_datos)
```

```{r}
receta_balance <- recipe(y ~ ., data = entrena) %>%
  step_smote(y) %>%
  prep()
entrena_balanceado <- juice(receta_balance)
```

```{r}
bosque_1 <- rand_forest(trees = 500, mtry = 20, mode = "classification") %>%
  set_engine("ranger") %>%
  fit(y ~ ., data = entrena_balanceado)
bosque_1 %>%
  predict(prueba) %>%
  bind_cols(prueba) %>% 
  class_metrics(truth = y, estimate=.pred_class) %>% 
  kable()
```

Aunque el accuracy parece alto, la precisión y la exhaustividad son cero.

## Fugas en implementación

### Variables no disponibles al momento de predicción

En este caso mostramos un ejemplo donde se utiliza erróneamente una variable que
no estará disponible al momento de hacer las predicciones (datos de [@greene]).

```{r}
credito <- read_csv("datos/AER_credit_card_data.csv") 
credito %>% head %>% kable()
```

Queremos construir un modelo para predecir que solicitudes fueron aceptadas y automatizar
el proceso de selección. Usamos regresión logística con keras y penalización L2:

```{r}
set.seed(823)
credito_split <- initial_split(credito)
entrena <- training(credito_split)
prueba <- testing(credito_split)
```

```{r}
# preparacion de datos
credito_receta <- recipe(card ~ ., credito) %>%
  step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal(), -card) 
# modelo
modelo_regularizado <- 
  logistic_reg(penalty = 1) %>% 
  set_engine("keras", epochs = 500, verbose = FALSE)  %>% 
  set_mode("classification") 
```

```{r, message=FALSE, warning = FALSE}
# ajustar parametros de preprocesamiento
receta_prep <- credito_receta %>% prep(entrena)
# preprocesar datos
entrena_prep <- bake(receta_prep, entrena)
prueba_prep <- bake(receta_prep, prueba)
# ajustar modelo
ajuste <- modelo_regularizado %>% 
  fit(card~ expenditure + dependents + income + age + owner_yes, data = entrena_prep)
```

```{r}
# evaluar
metricas <- metric_set(accuracy, recall, precision)
ajuste %>% predict(prueba_prep) %>%
  bind_cols(prueba) %>% 
  metricas(truth = factor(card), estimate = .pred_class)
```

Y parece tener un desempeño razonable. Si quitamos la variable *expenditure* se
degrada totalmente el desempeño del modelo:

```{r}
ajuste_2 <- modelo_regularizado %>% 
  fit(card~ dependents + income + age + owner_yes, data = entrena_prep)
ajuste_2 %>% predict(prueba_prep) %>%
  bind_cols(prueba) %>% 
  metricas(truth = factor(card), estimate = .pred_class) %>% 
  kable()
```

La razón es que *expenditure* es gasto en tarjetas de crédito. Esto incluye
la tarjeta para la que queremos hacer predicción de aceptación:

```{r}
entrena %>% 
  mutate(algun_gasto = expenditure > 0) %>% 
  group_by(algun_gasto, card) %>% 
  tally() %>% 
  kable()
```

Lo que nos indica que algún gasto probablemente incluye el gasto en la tarjeta actual,
y la variable *expenditure* es medida posteriormente a la entrega de la tarjeta:

- El desempeño de este modelo para nuevas aplicaciones será muy malo, pues la variable
*expenditure*, en el momento de la aplicación, evidentemente no cuenta cuánto va a gastar
cada cliente en el futuro.

## Desbalance de clases

Cuando tenemos desbalance severo de clases podemos enfrentar dos problemas:
existen en términos absolutos muy pocos elementos de una clase para poder
discriminarla de manera efectiva (aún cuando tengamos los atributos
o *features* correctas), o métodos usuales de evaluación de predicción 
son deficientes para evaluar el desempeño de predicciones. 

Consideremos los siguientes datos (del paquete @ISLR):

The data contains 5822 real customer records. Each record consists of 86 variables, containing sociodemographic data (variables 1-43) and product ownership (variables 44-86). The sociodemographic data is derived from zip codes. All customers living in areas with the same zip code have the same sociodemographic attributes. Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy. Further information on the individual variables can be obtained at http://www.liacs.nl/~putten/library/cc2000/data.html


Queremos predecir la variable *Purchase*:

```{r}
caravan <- read_csv("datos/caravan.csv") %>% 
  mutate(MOSTYPE = factor(MOSTYPE),
         MOSHOOFD = factor(MOSHOOFD))
nrow(caravan)
caravan %>% count(Purchase) %>% 
  mutate(pct = 100 * n / sum(n)) %>% 
  mutate(pct = round(pct, 2))
```

Esta es la distribución natural de respuesta que vemos en los datos, y tenemos relativamente pocos
datos en la categoría "Yes".

Usaremos muestreo estratificado para obtener proporciones similares en conjuntos de entrenamiento y prueba

```{r}
set.seed(823)
caravan_split = initial_split(caravan, strata = Purchase, prop = 0.8)
caravan_split
```

Y usaremos regresión logística (podemos usar bosques, xgboost, keras, etc.)

```{r}
library(tune)
# preparacion de datos
caravan_receta <- recipe(Purchase ~ ., caravan) %>%
  step_dummy(all_nominal(), -Purchase) %>% 
  prep()
# modelo
modelo_log <- 
  logistic_reg() %>% 
  set_engine("glm")  %>% 
  set_mode("classification") %>% 
  fit(Purchase ~ ., data = caravan_receta %>% juice)
```

#### Análisis incorrecto {-}

La matriz de confusión de prueba es:

```{r, message=FALSE, warning = FALSE}
prueba_procesado <- bake(caravan_receta, testing(caravan_split))
predictions_glm <- modelo_log %>%
  predict(new_data = prueba_procesado) %>%
  bind_cols(prueba_procesado %>% select(Purchase))
predictions_glm %>%
  conf_mat(Purchase, .pred_class)
```

Y la de entrenamiento:

```{r, message=FALSE, warning = FALSE}
predictions_ent_glm <- modelo_log %>%
  predict(new_data = juice(caravan_receta)) %>%
  bind_cols(juice(caravan_receta) %>% select(Purchase))
predictions_ent_glm %>%
  conf_mat(Purchase, .pred_class)
```


Y obtenemos desempeño pobre según esta matriz de confusión (prueba y entrenamiento).
La sensibilidad es muy baja, aunque
la especificidad sea alta. Una conclusión típica es que 
*el modelo no tiene valor predictivo*, o que
*es necesario sobre muestrear la
clase de ocurrencia baja*.

#### Análisis correcto {-}

En lugar de empezar con sobre/sub muestreo,
que modifica lsas proporciones naturales de las categorías en los datos, podemos trabajar con probabilidades
en lugar de predicciones de clase con punto de corte de 0.5.

Por ejemplo, podemos visualizar con una curva ROC (o curva lift, precisión recall, o alguna otra similar que
tome en cuenta probabilidades):

```{r, fig.width = 5, fig.height = 4}
library(yardstick)
predictions_prob <- modelo_log %>%
  predict(new_data = prueba_procesado, type = "prob") %>%
  bind_cols(prueba_procesado %>% select(Purchase)) %>% 
  select(.pred_Yes, Purchase)
datos_roc <- roc_curve(predictions_prob, Purchase, .pred_Yes)
autoplot(datos_roc)
```

Donde vemos que podemos alcanzar buenos niveles de sensibilidad si aceptamos alguna degradación en la especificidad,
que originalmente es muy alta. Por ejemplo, cortando en 0.05 podemos obtener especificidad y sensibilidad
que posiblemente sean adecuadas para el problema:

```{r}
datos_roc %>% filter(abs(.threshold - 0.05) < 1e-4) %>% round(3)
```

**¿Qué pasa si hacemos sub/sobremuestreo? **

Sobremuetreamos:

```{r}
caravan_receta <- recipe(Purchase ~ ., caravan) %>%
  step_dummy(all_nominal(), -Purchase) 
# modelo
entrena <- caravan_receta %>% step_smote(Purchase) %>% prep() %>% juice
entrena %>% count(Purchase)
modelo_log_smote <- 
  logistic_reg() %>% 
  set_engine("glm")  %>% 
  set_mode("classification") %>% 
  fit(Purchase ~ ., data = entrena)
```

En entrenamiento la matriz de confusión es *aparentemente* mejor:
```{r, message=FALSE, warning = FALSE}
predictions_ent_glm <- modelo_log_smote %>%
  predict(new_data = entrena) %>%
  bind_cols(entrena %>% select(Purchase))
predictions_ent_glm %>%
  conf_mat(Purchase, .pred_class)
```


Pero en prueba los resultados son muy similares:

```{r}
predictions_prob <- modelo_log_smote %>%
  predict(new_data = prueba_procesado, type = "prob") %>%
  bind_cols(prueba_procesado %>% select(Purchase)) %>% 
  select(.pred_Yes, Purchase)
datos_roc_smote <- roc_curve(predictions_prob, Purchase, .pred_Yes)
datos_roc_comp <- bind_rows(datos_roc %>% mutate(tipo = "natural"),
                            datos_roc_smote %>% mutate(tipo = "con sub-sobre muestreo")
)
ggplot(datos_roc_comp,
       aes(x = 1 - specificity, y = sensitivity, colour = tipo)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()
```

- **El problema original no era que el ajuste no funcionaba, sino que evaluamos el punto 
de corte incorrecto**. Un punto de corte de 0.5 con SMOTE equivale a uno mucho más chico sin
SMOTE.

- Peor aún, las **probabilidades del modelo construido con sobremuestreo no reflejan las tasas
de ocurrencia de la respuesta que nos interesa**, lo cual puede producir resúmenes engañosos de las
tasas de respuesta que esperamos observar en producción.

## Evaluación de punto de corte 

Las mejores decisiones de punto de corte pueden hacerse con análisis de costo beneficio,
con curvas tipo *lift*. Aunque esta información muchas veces no está disponible, es la
situación ideal para evaluar cómo ayuda el modelo y cuánto valen las acciones que pretendemos
tomar. Es posible hacer este análisis con valores inciertos de costo beneficio.

Supongamos que estamos pensando en un tratamiento para retener estudiantes en algún programa
de entrenamiento o mejora.

- El tratamiento de retención cuesta 5000 pesos por alumno,
- Estimamos mediante experimentos o algún análisis externo que nuestro tratamiento reduce la probabilidad de abandono
en un 60\%,
- Tenemos algún tipo de valuación del valor social de que un alumno persista en el programa.

Podemos evaluar a nuestro modelo en el contexto del problema de las siguiente 
forma:

- Suponemos que trataremos a un u % de los estudiantes con mayor probabilidad de rotar.
- Calculamos el costos esperado si tratamos a u % de los estudiantes: 
simulamos reduciendo su probabilidad de abandono por el tratamiento y sumamos los costos de tratarlos.
- Comparamos contra el escenario de no aplicar ningún tratamiento

No es necesario usar medidas muy técnicas para dar un resumen de cómo
nos puede ayudar el tratamiento y modelo para mantener el valor de la cartera:

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
clientes <- tibble(id = 1:20000, valor = 50000) %>% 
    mutate(prob_pred = rbeta(length(valor), 1, 2)) 
```

```{r echo = FALSE, fig.width = 8, fig.height = 5}
calc_perdida_aleatorio <- function(corte, factor_ret, costo){
    n_tratados <- filter(clientes, prob_pred >= corte) %>% nrow()
    p_tratados <- n_tratados / nrow(clientes)
    clientes <- clientes %>%
        mutate(aleatorio = rbinom(length(prob_pred), 1 , p_tratados))
    perdida_sin_accion <- filter(clientes, aleatorio == 1) %>%
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    perdida_aleatorio <- filter(clientes, aleatorio==1) %>%
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, 
          prob_pred*(aleatorio*factor_ret + (1-aleatorio))) == 1, valor, 0)) %>%
        summarise(total = sum(costo)) %>% 
        pull(total)
    total <- perdida_aleatorio - (perdida_sin_accion)  +
      costo*nrow(filter(clientes, prob_pred >= corte))
    total
}
calc_perdida_modelo <- function(corte, factor_ret, costo){
    perdida_no_trata <- filter(clientes, prob_pred < corte) %>% 
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    perdida_trata <- filter(clientes, prob_pred >= corte) %>% 
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred*factor_ret) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    perdida_cf <- filter(clientes, prob_pred >= corte) %>%  
        mutate(costo = ifelse(rbinom(length(prob_pred), 1, prob_pred) == 1, valor, 0)) %>% 
        summarise(total = sum(costo)) %>% 
        pull(total)
    total <- perdida_no_trata +  perdida_trata - (perdida_no_trata + perdida_cf) +
      costo*nrow(filter(clientes, prob_pred >= corte)) 
    total
}
perdidas_sim_aleatorio <- map_dfr(rep(seq(0,1, 0.1), 50), 
    function(x){
      perdida_sim <- calc_perdida_aleatorio(x, 0.6, 5000)
      tibble(perdida = perdida_sim, corte = x)
    }) %>% bind_rows %>% mutate(tipo = "Tratamiento aleatorio")
perdidas_sim_modelo <-  map_dfr(rep(seq(0,1, 0.1), 50), 
    function(x){
      perdida_sim <- calc_perdida_modelo(x, 0.6, 5000)
      tibble(perdida = perdida_sim, corte = x)
    }) %>% bind_rows %>% mutate(tipo = "Tratamiento modelo")
perdidas_sim <- bind_rows(perdidas_sim_aleatorio, perdidas_sim_modelo)
```

```{r}
ggplot(filter(perdidas_sim, tipo=="Tratamiento modelo"),
       aes(x = factor(corte), y = - perdida / 1e6)) + 
  geom_boxplot() + ylab("Ganancia incremental (millones)") +
  xlab("Corte inferior de tratamiento (probabilidad)") +
  labs(subtitle = "Ganancia vs ninguna acción") + theme_minimal()
```

¿Dónde habría que hacer el punto de corte para tratar a los estudiantes?

Si queremos separar el efecto del tratamiento con el efecto del tratamiento aplicado
según el modelo, podemos comparar con la acción que consiste en tratar a los estudiantes
al azar:

```{r}
ggplot(perdidas_sim, aes(x = factor(corte), y = - perdida / 1e6, 
                         group = interaction(tipo, corte), colour = tipo)) + 
  geom_boxplot() + ylab("Ganancia incremental (millones)") +
  xlab("Corte inferior de tratamiento (probabilidad)") +
  labs(subtitle = "Ganancia vs ninguna acción") + theme_minimal()
```

- La conclusión es que el modelo **ayuda considerablemente a la focalización del programa** (el área entre
las dos curvas mostradas arriba).

## Equidad con atributos protegidos

El siguiente ejemplo es derivado de (@Hardt). Supongamos que tenemos un atributo protegido $A$ que
tiene dos valores: azul y naranja. Naranja es el grupo minoritario  desaventajado.
Usaremos datos simulados como sigue: el atributo *score* está asociado
al atributo protegido:


```{r}
inv_logit <- function(x){
  1 / (1 + exp(-x))
}
simular_datos <- function(n = c(10000, 2000)){
  score_azul <- pmax(rnorm(n[1], 50, 10), 0)
  score_naranja <- pmax(rnorm(n[2], 40, 10), 0)
  azul <- tibble(tipo = "azul", score = score_azul)
  naranja <- tibble(tipo = "naranja", score = score_naranja)
  datos <- bind_rows(azul, naranja) %>% 
    mutate(coef_0 = ifelse(tipo == "azul", 0.0, 0),
           prob_real_pos = inv_logit(-1 + coef_0 + 0.1 * (score-40))) %>% 
    mutate(atr_1 = rpois(nrow(.), 3))
  datos %>% select(-coef_0) %>% 
    mutate(paga = map_dbl(prob_real_pos, ~ rbinom(1, 1, .x))) %>% 
    select(-prob_real_pos)
}
set.seed(1221)
tbl_datos <- simular_datos()
```

Una gráfica de conteos para el score obtenemos un grupo minoritario con valores de la variable score
más baja:

```{r, fig.width = 4, fig.height = 2}
ggplot(tbl_datos, aes(x = score, fill = tipo)) + geom_histogram()
```

Ajustamos un modelo simple de regresión logística:

```{r}
reg_log <- glm(paga ~  score + atr_1 + tipo, tbl_datos, family = "binomial")
tbl_datos <- tbl_datos %>% mutate(prob_pos = predict(reg_log, type = "response"))
```



Las tasas reales de cumplimiento son iguales para los dos grupos. En primer lugar, consideremos
una estrategia donde aplicamos el mismo punto de corte para todos los grupos

```{r}
resultado_cortes <- function(tbl_datos, cortes){
  resultado <- tbl_datos %>% 
    mutate(recibe = ifelse(tipo == "azul", prob_pos > cortes[1], prob_pos > cortes[2]),
           decision = ifelse(recibe, "Aceptado", "Rechazado")) 
  resultado %>% group_by(tipo, decision, paga) %>% count() %>% 
    ungroup()
}
resultados_conteo <- resultado_cortes(tbl_datos, c(0.45, 0.45)) 
resultados_conteo
```


```{r}
resultados_conteo %>%
  group_by(tipo, decision) %>%
  summarise(n = sum(n)) %>% 
  mutate(total = sum(n)) %>% 
  mutate(prop = n / total) %>% 
  filter(decision == "Aceptado")
```

Nótese que el grupo naranja ha recibido considerablemente menos aceptaciones que el grupo azul, tanto
en total como en proporción. Más aún, con la precisión o tasa de verdaderos positivos
podemos evaluar qué proporción de los pagarían fueron aceptados:

```{r}
resultados_conteo %>% 
  filter(paga == 1) %>% 
  group_by(tipo) %>% 
  mutate(tvp = n / sum(n)) %>% 
  filter(decision == "Aceptado")
```

y vemos que el grupo naranja también está en desventaja, pues entre los que cumplen hay menos decisiones
de aceptación.

El siguiente paso es considerar **paridad demográfica**. En este caso, decidimos dar el mismo número
de préstamos a cada grupo, dependiendo de su tamaño.

```{r}
calcular_puntos_paridad <- function(tbl_datos, prop){
  tbl_datos %>% group_by(tipo) %>% 
    summarise(corte = quantile(prob_pos, 1 - prop))
}
cortes_paridad_tbl <- calcular_puntos_paridad(tbl_datos, 0.40)
cortes_paridad_tbl
```

El corte para azul es más exigente que para naranja. En sí eso no es un problema pero observamos:

```{r}
cortes_paridad <- cortes_paridad_tbl %>% pull(corte)
resultados_conteo <- resultado_cortes(tbl_datos, cortes_paridad) 
resultados_conteo %>% 
  filter(paga == 1) %>% 
  group_by(tipo) %>% 
  mutate(tvp = n / sum(n)) %>% 
  filter(decision == "Aceptado")
```

Y así que además de ser más exigente con el grupo azul, a los que cumplen del grupo azul también
se les otorga menos decisiones de aceptación. Adicionalmente, se aceptan considerablemente menos
personas de la población.

La solución de **igual de de oportunidad** es cortar de forma que la tasa de aceptación dentro del grupo
de los que pagan sea similar para ambas poblaciones.

```{r}
calcular_cortes_oportunidad <- function(tbl_datos, prop){
  tbl_datos %>% 
    filter(paga==1) %>% 
    group_by(tipo) %>% 
    mutate(rank_p = rank(prob_pos) / length(prob_pos) ) %>% 
    filter(rank_p < prop) %>% 
    top_n(1, rank_p) %>% 
    select(tipo, corte = prob_pos)
}
cortes_op <- calcular_cortes_oportunidad(tbl_datos, 0.40)
resultados_conteo <- resultado_cortes(tbl_datos, cortes_op %>% pull(corte)) 
resultados_conteo %>% 
  filter(paga == 1) %>% 
  group_by(tipo) %>% 
  mutate(tvp = n / sum(n)) %>% 
  filter(decision == "Aceptado")
```




# Retos de rendición de cuentas

## Interpretabilidad

## Explicación de predicciones


# Referencias
