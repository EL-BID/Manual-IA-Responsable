---
title: "Casos de estudio"
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: cerulean
    css: [ "./css/style.css"]
link-citations: yes
bibliography:
- ../documento/referencias.bib
- ../documento/packages.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

En este cuaderno mostraremos varios ejemplos de los retos y soluciones explicadas
en el documento principal.

```{r}
library(tidyverse)
library(recipes)
library(themis)
library(rsample)
library(parsnip)
library(yardstick)
library(workflows)
library(tune)
library(knitr)
library(patchwork)
```


# Retos intrínsecos a los datos

## Reto: mala correspondencia de métrica y objetivos

Usar modelos que predicen la métrica incorrecta puede llevar a tomar decisiones incorrectas.
A veces el problema es claro, cuando la métrica sustituto tiene deficiencias obvias, y en otras
puede ser más sutil.

En este ejemplo se busca predecir la demanda de cierto producto, y se cuenta
con datos históricos de inventario, ventas y una covariable indicadora de ventas:

```{r, echo=FALSE, include=FALSE}
set.seed(124)
semana <- 1:100
indicador <- rnorm(100, 0, 20)
inventario <- 100 + 7*sqrt(1:100) + rpois(100, 50)
demanda <- 30 + 1.2*1:100 + indicador + rpois(100, 100)
ventas <- tibble(
    semana = 1:100,
    inventario = inventario, 
    demanda = demanda, 
    ventas = pmin(inventario, demanda),
    indicador = indicador, 
    agotamiento = ifelse(ventas==inventario, 1, 0))
ventas %>% select(-demanda)
```

Separamos los datos en entrenamiento y prueba, ajustando el modelo con el 
subconjunto de datos de entrenamiento. En este caso se utiliza un modelo lineal
con variable dependiente ventas y covariables de semana y una variable indicador.

```{r}
entrena <- ventas %>% filter(semana < 80)
prueba <- ventas %>% filter(semana >= 80)
entrena %>% select(-demanda) %>%  head() %>% kable()
mod_lineal <- lm(ventas ~ semana + indicador, data = ventas)
mod_lineal
```
Evaluamos el error de predicción.

```{r}
preds <- predict(mod_lineal, newdata = prueba)
mean(abs(preds - prueba$ventas))/mean(prueba$ventas)
```

El error porcentual es bajo. Los datos ajustados y predicciones se ven como sigue:

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
ggplot(ventas_larga %>% mutate(unidades = ifelse(tipo=="ventas" & semana > 80, NA, unidades)), 
       aes(x = semana, y = unidades, group = tipo, colour = tipo)) + 
  geom_line() +
  geom_vline(xintercept = 80)
```

Pero tomar decisiones de demanda o inventario es equivocado. La razón es que 
existen agotamientos de inventario, que marcamos con rojo en la siguiente gráfica.

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
ggplot(ventas_larga %>% mutate(unidades = ifelse(tipo=="ventas" & semana > 80, NA, unidades)), aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_point(data = filter(ventas, agotamiento==1, semana < 80), aes(y = ventas), colour = "red") + 
  geom_vline(xintercept = 80)
```

Si usáramos la política sugerida por las predicciones (por ejemplo 5% más), 
veríamos las ventas de la primera gráfica a continuación. Sin embargo, si 
usáramos una política de inventario con 250 unidades, observaríamos:


```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  mutate(ventas = ifelse(semana > 80, pmin(1.02*pred, demanda), ventas)) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
g1 <- ggplot(ventas_larga, aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_vline(xintercept = 80) + labs(subtitle = "Inventario: Predicciones + 2%")
```

```{r, fig.width = 5, fig.height=2.5}
preds <- predict(mod_lineal, newdata = ventas)
ventas_larga <- ventas %>% mutate(pred = preds) %>% 
  mutate(ventas = ifelse(semana > 80, pmin(300, demanda), ventas)) %>% 
  pivot_longer(cols = all_of(c("ventas","pred")), names_to = "tipo", values_to = "unidades")
g2 <- ggplot(ventas_larga, aes(x = semana)) + 
  geom_line(aes(group = tipo, colour = tipo, y = unidades)) +
  geom_vline(xintercept = 80)+ labs(subtitle = "Inventario: 300 unidades cte")
```

```{r}
g1 / g2
```

Entonces, 

- La política basada en las predicciones **exacerba** el problema de los agotamientos.
- Un uso no pensado de datos sin considerar el proceso generador de los mismos puede producir 
errores grandes en la decisión de políticas
- En este caso, la confusión proviene de no separar los conceptos de demanda y ventas. Otros
indicadores de demanda o modelos más adecuados ayudarían a resolver el problema.


## Muestras naturales y sesgo

Cuando las muestras de entrenamiento son diferentes a las poblaciones, donde
se van a aplicar los modelos, existen dificultades en validar correctamente las
predicciones.

### Muestras naturales: mala representatividad 

Para este ejemplo utilizaremos datos de la encuesta nacional de ingresos y gastos
en hogares de México [@enigh], para simular un escenario que queremos ejemplificar.

```{r}
set.seed(128)
encuesta_ingreso <- read_csv("datos/enigh-ejemplo.csv")
datos_ingreso <- encuesta_ingreso %>% 
  mutate(num_focos = FOCOS) %>%
  mutate(ingreso_miles = (INGCOR / 1000)) %>% 
  mutate(tel_celular = ifelse(SERV_2 == 1, "Sí", "No")) %>% 
  mutate(marginacion = fct_reorder(marginación, ingreso_miles, median)) %>% 
  rename(ocupadas = PEROCU) %>% 
  select(ingreso_miles, num_focos, tel_celular, marginacion, ocupadas)
```

```{r}
ingreso_split <- initial_split(datos_ingreso, prop = 0.7)
entrena <- training(ingreso_split)
prueba <- testing(ingreso_split)
```

Supongamos que nos interesa estimar el ingreso de los hogares, para ello usamos 
una encuesta por teléfono celular, más aún, supongamos que solo accedimos a 
zonas que no tienen marginación muy alta.

```{r}
entrena_sesgada <- filter(entrena, tel_celular == "Sí", marginacion=="Muy bajo")
sesgados_split <- initial_split(entrena_sesgada)
entrena_sesgo <- training(sesgados_split)
validacion <- testing(sesgados_split)
```

Construimos nuestro modelo de ingreos con los datos disponibles.

```{r}
mod_1 <- lm(log(1 + ingreso_miles) ~ num_focos + ocupadas, data = entrena_sesgo)
```

Y evaluamos el error en una muestra de prueba construida con datos con las mismas
características que los datos de entrenamiento (hogares con teléfono celular y 
grado de marginación no muy alto).

```{r}
preds_val <- predict(mod_1, newdata = validacion)
mean(abs(preds_val - log(1 + validacion$ingreso_miles))) %>% round(2)
```

El error en una muestra más similar a la población que se pretende aplicar el 
algoritmo es mayor:

```{r}
preds_prueba <- predict(mod_1, newdata = prueba)
mean(abs(preds_prueba - log(1 + prueba$ingreso_miles))) %>% round(2)
prueba$pred <- preds_prueba
```

Sin embargo, el principal problema se refleja en la siguiente gráfica: cada punto 
representa un hogar, la muestra es más similar a la población donde se aplicará la
metodología, y en el 
eje horizontal graficamos la predicción de los hogares utilizando el modelo,
mientras que el eje vertical corresponde al ingreso de cada hogar. Como referencia
agregamos la recta $y = x$, y un suavizador.

```{r, fig.width = 5, fig.height = 4}
ggplot(prueba %>% filter(pred < log(100)), 
  aes(x = exp(pred) - 1, y = ingreso_miles, group=1)) + geom_jitter() +
  geom_abline() + geom_smooth(method = "loess", se = FALSE) +
  scale_x_log10() + scale_y_log10() + xlab("Predicción (miles al trimestre)") +
  ylab("Ingreso corriente (miles al trimestre)") 
```

Tenemos entonces dos problemas:

1. El sesgo produce un error considerablemente más grande en la implementación que en la validación
2. Peor aún, el sesgo es mayor para hogares de menores ingresos, lo cual puede producir
una focalización mediocre si buscamos identificar hogares de menores ingresos.

### Muestras naturales: comparaciones causales

Supongamos que quisiéramos evaluar el efecto que tiene ir a una
escuela pública vs una escuela privada en las calificaciones de un examen 
estandarizado. Consideramos datos agregados por escuela

```{r}
escuelas <- read_csv("datos/enlace.csv")
escuelas
```

En este caso particular tenemos pocas covariables, pero haremos un primer
modelo considerando estado y tipo de escuela. La respuesta es la calificación
en matemáticas

```{r}
library(recipes)
escuelas_split <- rsample::initial_split(escuelas, prop = 0.5)
escuelas_split
receta_escuelas <- training(escuelas_split) %>%
  recipe(mate_6 ~ estado + tipo + num_evaluados_total) %>%
  step_dummy(all_nominal()) %>% 
  prep()
```



```{r}
escuelas_entrena <- receta_escuelas %>% juice
escuelas_prueba <- receta_escuelas %>% bake(testing(escuelas_split)) 
esc_ranger <- rand_forest(trees = 250, mode = "regression") %>%
  set_engine("ranger") %>%
  fit(mate_6 ~ ., data = escuelas_entrena)
```

Podemos comparar por ejemplo las predicciones por estado para cada tipo de
escuela:

```{r}
grafica_datos <- training(escuelas_split) %>% 
  dplyr::select(estado, tipo) %>% unique %>% 
  crossing(tibble(num_evaluados_total = c(10, 20, 50, 100)))
grafica_baked <- receta_escuelas %>% bake(grafica_datos)
preds <- predict(esc_ranger, grafica_baked) %>% pull(.pred)
graf_prom <- grafica_datos %>% mutate(prediccion = preds) %>% 
  group_by(tipo, num_evaluados_total) %>% 
  summarise(prediccion = mean(prediccion))
ggplot(graf_prom, 
       aes(x = num_evaluados_total, y = prediccion, colour=tipo)) +
  geom_line() + geom_point()
```

Estas diferencias predictivas difícilmente pueden interpetarse como 
"efectos de la calidad de la escuela". Podemos ver eso cuando
consideramos incluír también nivel de marginación donde está el municipio.
En este caso, obtrenemos

```{r}
receta_escuelas <- training(escuelas_split) %>%
  recipe(mate_6 ~ estado + tipo + num_evaluados_total + marginacion) %>%
  step_dummy(all_nominal()) %>% 
  prep()
escuelas_entrena <- receta_escuelas %>% juice
esc_ranger <- rand_forest(trees = 250, mode = "regression") %>%
  set_engine("ranger") %>%
  fit(mate_6 ~ ., data = escuelas_entrena)
grafica_datos <- training(escuelas_split) %>% 
  dplyr::select(estado, tipo, marginacion) %>% unique %>% 
  crossing(tibble(num_evaluados_total = c(10, 20, 50, 100)))
grafica_baked <- receta_escuelas %>% bake(grafica_datos)
preds <- predict(esc_ranger, grafica_baked) %>% pull(.pred)
graf_prom <- grafica_datos %>% mutate(prediccion = preds) %>% 
  group_by(tipo, num_evaluados_total, marginacion) %>% 
  summarise(prediccion = mean(prediccion)) %>% 
  mutate(marginacion = fct_relevel(marginacion, c("MUY BAJO", "BAJO", "MEDIO",
                                                  "ALTO", "MUY ALTO")))
ggplot(graf_prom, 
       aes(x = num_evaluados_total, y = prediccion, colour=tipo)) +
  geom_line() + geom_point() + facet_wrap(~marginacion)

```




# Retos en construcción y desarrollo de predictores

## Fugas Entrenamiento Validación

Veremos varios ejemplos de cómo las fugas de entrenamiento a validación producen estimaciones
sesgadas del desempeño de predictores.

### Selección de variables antes de particionar


```{block2, type='resumen'}
Cualquier paso de preprocesamiento debe hacerse sin usar datos de validación. Esto incluye
cuando usamos métodos como validación cruzada
```


Este ejemplo es originalmente de [@ESL], y utilizaremos datos sintéticos, 
generados con el siguiente proceso:

1. Simulamos variables respuesta $y$ con distribución binomial,  
2. Simulamos 1000 covariables independientes, cada una con distribución normal
estándar.

```{r, message = FALSE}
simular <- function(n = 100, p = 500, prob = 0.5){
  datos <- map(1:p, ~ rnorm(n)) %>% 
    bind_cols()
  datos$y <- rbinom(n, 1, prob)
  datos
}
set.seed(8234)
datos_entrena <- simular(n = 200, p = 1000)
datos_prueba <- simular(n = 2000, p = 1000)
dim(datos_entrena)
datos_entrena %>% group_by(y) %>% tally() %>% kable()
```

Nuestra selección de variables está dada por la siguiente función. Esta función *selecciona las
variables más correlacionadas con la respuesta**. Por sí solo este método no es incorrecto, pero cuando
se ejecuta sobre los datos que se usarán en validación (validación cruzada), entonces la estimación
de desempeño es optimista:

```{r}
seleccionar <- function(datos, num_var = 10){
  correlaciones <- datos %>% 
    pivot_longer(cols = matches("V"), names_to = "variable", values_to = "x") %>% 
    group_by(variable) %>% 
    summarise(corr = abs(cor(y, x))) %>% 
    arrange(desc(corr)) 
  # seleccionar 
  seleccionadas <- correlaciones %>% 
    top_n(num_var, wt = corr) %>% 
    pull(variable)
  datos %>% select(one_of(c("y", seleccionadas)))
}

```

#### Método erróneo {-}

Para cualquier corte de validación cruzada el error parece ser menor a 0.5:

```{r}
datos_filtrados <- seleccionar(datos_entrena)
datos_filtrados %>% head %>% kable()
```


```{r, message = FALSE}
corte_validacion <- datos_filtrados %>% sample_frac(0.7)
valida <- anti_join(datos_filtrados, corte_validacion)
modelo_1 <- glm(y ~ ., corte_validacion, family = "binomial")
mean(as.numeric(predict(modelo_1, valida) > 0) == valida$y) %>% round(2)
```

Sin embargo, el desempeño real del modelo será:

```{r}
mean(as.numeric(predict(modelo_1, datos_prueba) > 0) == datos_prueba$y) %>% round(2)
```

#### Método correcto {-}

La selección de variables debe hacerse en cada vuelta de validación cruzada:

```{r, message=FALSE}
corte_validacion <- datos_entrena %>% sample_frac(0.7)
datos_filtrados_corte <- seleccionar(corte_validacion)
valida <- anti_join(datos_entrena, corte_validacion)
modelo_1 <- glm(y ~ ., datos_filtrados_corte, family = "binomial")
mean(as.numeric(predict(modelo_1, valida) > 0) == valida$y) %>% round(2)
```


### Sobremuestrear antes de particionar

En este ejemplo veremos que sobremuestrear una clase chica antes de separar 
datos de validación o hacer validación cruzada puede producir estimaciones 
demasiado optimistas del error de predicción.

Supongamos que tenemos desbalance severo entre nuestras dos clases:

```{r}
set.seed(99134)
datos_desbalance <- simular(n = 500, p = 20, prob = 0.1) %>% 
  mutate(y = factor(y, levels = c(1, 0)))
datos_desbalance %>% group_by(y) %>% tally() %>% kable()
```


#### Manera incorrecta {-}

Supongamos que primero aplicamos (SMOTE)[@chawla] para intentar balancear los 
datos:

```{r}
receta_balance <- recipe(y ~ ., datos_desbalance) %>%
  step_smote(y) %>%
  prep()
datos_smote <- juice(receta_balance) 
```

Obteniendo así,

```{r}
datos_smote %>% group_by(y) %>% tally() %>% kable()
```

Ahora separamos entrenamiento y validación

```{r}
sep_datos_smote <- initial_split(datos_smote)
entrena_smote <- training(sep_datos_smote)
prueba_smote <- testing(sep_datos_smote)
```

Y usamos nuestro método de clasificación:

```{r}
class_metrics <- metric_set(accuracy, recall, precision, kap)
bosque <- rand_forest(trees = 500, mtry = 20, mode = "classification") %>%
  set_engine("ranger") %>%
  fit(y ~ ., data = entrena_smote)
bosque %>%
  predict(prueba_smote) %>%
  bind_cols(prueba_smote) %>% 
  class_metrics(truth = y, estimate = .pred_class) %>% 
  kable()
```

En primera instancia parece ser que el desempeño es muy bueno. Sabemos que esto 
es fictico, pues no hay relación de $y$ con el resto de las covariables.


#### Manera correcta {-}

Antes de hacer el rebalance de clases separamos entrenamiento y validación. Es 
posible hacer el muestro estratificado, por ejemplo:

```{r}
sep_datos <- initial_split(datos_desbalance, prop = 0.5)
entrena <- training(sep_datos)
prueba <- testing(sep_datos)
```

```{r}
receta_balance <- recipe(y ~ ., data = entrena) %>%
  step_smote(y) %>%
  prep()
entrena_balanceado <- juice(receta_balance)
```

```{r}
bosque_1 <- rand_forest(trees = 500, mtry = 20, mode = "classification") %>%
  set_engine("ranger") %>%
  fit(y ~ ., data = entrena_balanceado)
bosque_1 %>%
  predict(prueba) %>%
  bind_cols(prueba) %>% 
  class_metrics(truth = y, estimate=.pred_class) %>% 
  kable()
```

Aunque el accuracy parece alto, la precisión y la exhaustividad son cero.

## Fugas en implementación

### Variables no disponibles al momento de predicción

En este caso mostramos un ejemplo donde se utiliza erróneamente una variable que
no estará disponible al momento de hacer las predicciones (datos de [@greene]).

```{r}
credito <- read_csv("datos/AER_credit_card_data.csv") 
credito %>% head %>% kable()
```

Queremos construir un modelo para predecir que solicitudes fueron aceptadas y automatizar
el proceso de selección. Usamos regresión logística con keras y penalización L2:

```{r}
set.seed(823)
credito_split <- initial_split(credito)
entrena <- training(credito_split)
prueba <- testing(credito_split)
```

```{r}
# preparacion de datos
credito_receta <- recipe(card ~ ., credito) %>%
  step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal(), -card) 
# modelo
modelo_regularizado <- 
  logistic_reg(penalty = 1) %>% 
  set_engine("keras", epochs = 500, verbose = FALSE)  %>% 
  set_mode("classification") 
```

```{r, message=FALSE, warning = FALSE}
# ajustar parametros de preprocesamiento
receta_prep <- credito_receta %>% prep(entrena)
# preprocesar datos
entrena_prep <- bake(receta_prep, entrena)
prueba_prep <- bake(receta_prep, prueba)
# ajustar modelo
ajuste <- modelo_regularizado %>% 
  fit(card~ expenditure + dependents + income + age + owner_yes, data = entrena_prep)
```

```{r}
# evaluar
metricas <- metric_set(accuracy, recall, precision)
ajuste %>% predict(prueba_prep) %>%
  bind_cols(prueba) %>% 
  metricas(truth = factor(card), estimate = .pred_class)
```

Y parece tener un desempeño razonable. Si quitamos la variable *expenditure* se
degrada totalmente el desempeño del modelo:

```{r}
ajuste_2 <- modelo_regularizado %>% 
  fit(card~ dependents + income + age + owner_yes, data = entrena_prep)
ajuste_2 %>% predict(prueba_prep) %>%
  bind_cols(prueba) %>% 
  metricas(truth = factor(card), estimate = .pred_class) %>% 
  kable()
```

La razón es que *expenditure* es gasto en tarjetas de crédito. Esto incluye
la tarjeta para la que queremos hacer predicción de aceptación:

```{r}
entrena %>% 
  mutate(algun_gasto = expenditure > 0) %>% 
  group_by(algun_gasto, card) %>% 
  tally() %>% 
  kable()
```

Lo que nos indica que algún gasto probablemente incluye el gasto en la tarjeta actual,
y la variable *expenditure* es medida posteriormente a la entrega de la tarjeta:

- El desempeño de este modelo para nuevas aplicaciones será muy malo, pues la variable
*expenditure*, en el momento de la aplicación, evidentemente no cuenta cuánto va a gastar
cada cliente en el futuro.


### Referencias
