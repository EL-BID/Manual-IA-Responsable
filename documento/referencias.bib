@inproceedings{kaufman,
author = {Kaufman, Shachar and Rosset, Saharon and Perlich, Claudia},
year = {2011},
month = {01},
pages = {556-563},
title = {Leakage in Data Mining: Formulation, Detection, and Avoidance},
volume = {6},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2020408.2020496}
}
@book{box78,
  title={Statistics for experimenters: an introduction to design, data analysis, and model building},
  author={Box, G.E.P. and Hunter, W.G. and Hunter, J.S.},
  isbn={9780471093152},
  series={Wiley series in probability and mathematical statistics: Applied probability and statistics},
  year={1978},
  publisher={Wiley}
}
@book{lohr,
  title={Sampling: Design and Analysis},
  author={Lohr, S.L.},
  isbn={9780495105275},
  lccn={99211216},
  series={Advanced (Cengage Learning)},
  url={https://books.google.com.mx/books?id=aSXKXbyNlMQC},
  year={2009},
  publisher={Cengage Learning}
}
@book{GelmanHill,
  added-at = {2011-08-15T12:47:13.000+0200},
  asin = {052168689X},
  author = {Gelman, Andrew and Hill, Jennifer},
  biburl = {https://www.bibsonomy.org/bibtex/201c2497e4ffea441a9835d0f05160dd7/vivion},
  description = {Amazon.com: Data Analysis Using Regression and Multilevel/Hierarchical Models (9780521686891): Andrew Gelman, Jennifer Hill: Books},
  dewey = {519.536},
  ean = {9780521686891},
  edition = 1,
  interhash = {3a8313c400b72653645c195a19c1eb02},
  intrahash = {01c2497e4ffea441a9835d0f05160dd7},
  isbn = {052168689X},
  keywords = {statistics},
  publisher = {Cambridge University Press},
  timestamp = {2011-08-15T12:47:13.000+0200},
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  url = {http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?s=books&ie=UTF8&qid=1313405184&sr=1-1},
  year = 2006
}
@article {king,
	title = {Misunderstandings Among Experimentalists and Observationalists about Causal Inference},
	journal = {Journal of the Royal Statistical Society, Series A},
	volume = {171, part 2},
	year = {2008},
	pages = {481{\textendash}502},
	author = {Kosuke Imai and Gary King and Elizabeth Stuart}
}


@book{kuhn,
  title={Applied Predictive Modeling},
  author={Kuhn, M. and Johnson, K.},
  isbn={9781461468493},
  series={SpringerLink : B{\"u}cher},
  url={https://books.google.com.mx/books?id=xYRDAAAAQBAJ},
  year={2013},
  publisher={Springer New York}
}
@incollection{NIPS2017_7062,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}
@article{Mitchell,
  author    = {Margaret Mitchell and
               Simone Wu and
               Andrew Zaldivar and
               Parker Barnes and
               Lucy Vasserman and
               Ben Hutchinson and
               Elena Spitzer and
               Inioluwa Deborah Raji and
               Timnit Gebru},
  title     = {Model Cards for Model Reporting},
  journal   = {CoRR},
  volume    = {abs/1810.03993},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.03993},
  archivePrefix = {arXiv},
  eprint    = {1810.03993},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-03993.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Suresh2019AFF,
  title={A Framework for Understanding Unintended Consequences of Machine Learning},
  author={Harini Suresh and John V. Guttag},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.10002}
}
@article{hardt,
  author    = {Moritz Hardt and
               Eric Price and
               Nathan Srebro},
  title     = {Equality of Opportunity in Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/1610.02413},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02413},
  archivePrefix = {arXiv},
  eprint    = {1610.02413},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HardtPS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{creditcard,
title	= {Machine Learning: The High Interest Credit Card of Technical Debt},
author	= {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
year	= {2014},
booktitle	= {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)}
}
@article{biasdata,
author = {Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and Kompatsiaris, Ioannis and Kinder-Kurlanda, Katharina and Wagner, Claudia and Karimi, Fariba and Fernandez, Miriam and Alani, Harith and Berendt, Bettina and Kruegel, Tina and Heinze, Christian and Broelemann, Klaus and Kasneci, Gjergji and Tiropanis, Thanassis and Staab, Steffen},
title = {Bias in data-driven artificial intelligence systems—An introductory survey},
journal = {WIREs Data Mining and Knowledge Discovery},
keywords = {fairness, fairness-aware AI, fairness-aware machine learning, interpretability, responsible AI},
doi = {10.1002/widm.1356},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1356},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1356},
year = {2020}
}


@article {ImaKinStu08,
	title = {Misunderstandings Among Experimentalists and Observationalists about Causal Inference},
	journal = {Journal of the Royal Statistical Society, Series A},
	volume = {171, part 2},
	year = {2008},
	pages = {481{\textendash}502},
	author = {Kosuke Imai and Gary King and Elizabeth Stuart}
}

@book{missingrubin,
  author = {Little, R.J.A. and Rubin, D.B.},
  biburl = {https://www.bibsonomy.org/bibtex/2b2fb20df470898e82317fb855c088703/peter.ralph},
  isbn = {9780471183860},
  keywords = {missing_data statistics},
  lccn = {2002027006},
  publisher = {Wiley},
  series = {Wiley series in probability and mathematical statistics. Probability and mathematical statistics},
  timestamp = {2012-09-09T11:27:10.000+0200},
  title = {Statistical analysis with missing data},
  url = {http://books.google.com/books?id=aYPwAAAAMAAJ},
  year = 2002
}

@Article{mice,
    title = {{mice}: Multivariate Imputation by Chained Equations in R},
    author = {Stef {van Buuren} and Karin Groothuis-Oudshoorn},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {45},
    number = {3},
    pages = {1-67},
    url = {https://www.jstatsoft.org/v45/i03/},
}

@Book{ESL,
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  url = {http://web.stanford.edu/~hastie/ElemStatLearn/},
  title = {The Elements of Statistical Learning},
  year = 2017
}


@incollection{NIPS2017_7062,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@article{Suresh,
  title={A Framework for Understanding Unintended Consequences of Machine Learning},
  author={Harini Suresh and John V. Guttag},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.10002}
}

@article{DBLP:journals/corr/HardtPS16,
  author    = {Moritz Hardt and
               Eric Price and
               Nathan Srebro},
  title     = {Equality of Opportunity in Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/1610.02413},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02413},
  archivePrefix = {arXiv},
  eprint    = {1610.02413},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HardtPS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{barocas,
  abstract = {This article addresses the potential for disparate impact in the data mining processes that are taking over modern-day business. Scholars and policymakers had, until recently, focused almost exclusively on data mining’s capacity to hide intentional discrimination, hoping to convince regulators to develop the tools to unmask such discrimination. Recently there has been a noted shift in the policy discussions, where some have begun to recognize that unintentional discrimination is a hidden danger that might be even more worrisome. So far, the recognition of the possibility of unintentional discrimination lacks technical and theoretical foundation, making policy recommendations difficult, where they are not simply misdirected. This article provides the necessary foundation about how data mining can give rise to discrimination and how data mining interacts with anti-discrimination law.<br><br>The article carefully steps through the technical process of data mining and points to different places within the process where a disproportionately adverse impact on protected classes may result from innocent choices on the part of the data miner. From there, the article analyzes these disproportionate impacts under Title VII. The Article concludes that Title VII is largely ill equipped to address the discrimination that results from data mining. Worse, due to problems in the internal logic of data mining as well as political and constitutional constraints, there appears to be no easy way to reform Title VII to fix these inadequacies. The article focuses on Title VII because it is the most well developed anti-discrimination doctrine, but the conclusions apply more broadly because they are based on the general approach to anti-discrimination within American law.},
  added-at = {2014-12-21T17:02:48.000+0100},
  author = {Barocas, Solon and Selbst, Andrew D.},
  biburl = {https://www.bibsonomy.org/bibtex/2a5dabc63a11dec020c6f049486f7ce3d/asmelash},
  description = {Big Data's Disparate Impact by Solon Barocas, Andrew D. Selbst :: SSRN},
  interhash = {847b99093ba878afb2b4cb4819eb9718},
  intrahash = {a5dabc63a11dec020c6f049486f7ce3d},
  journal = {SSRN eLibrary},
  keywords = {bias bigdata datamining discrimination models phdproposal},
  language = {English},
  location = {http://ssrn.com/paper=2477899},
  publisher = {SSRN},
  timestamp = {2014-12-21T17:02:48.000+0100},
  title = {{Big Data's Disparate Impact}},
  type = {Working Paper Series},
  year = 2014
}

@inproceedings{verma,
author = {Verma, Sahil and Rubin, Julia},
title = {Fairness Definitions Explained},
year = {2018},
isbn = {9781450357463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194770.3194776},
doi = {10.1145/3194770.3194776},
booktitle = {Proceedings of the International Workshop on Software Fairness},
pages = {1–7},
numpages = {7},
location = {Gothenburg, Sweden},
series = {FairWare ’18}
}
@article{mehrabi,
  title={A Survey on Bias and Fairness in Machine Learning},
  author={Ninareh Mehrabi and Fred Morstatter and Nripsuta Saxena and Kristina Lerman and Aram Galstyan},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.09635}
}
@InProceedings{boulamwini,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Joy Buolamwini and Timnit Gebru},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Sorelle A. Friedler and Christo Wilson},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, NY, USA},
  month = 	 {23--24 Feb},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {http://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}
@article{bolukbasi,
  title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author={Tolga Bolukbasi and Kai-Wei Chang and James Y. Zou and Venkatesh Saligrama and Adam Tauman Kalai},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06520}
}


@article{miller,
  title={Explanation in Artificial Intelligence: Insights from the Social Sciences},
  author={Tim Miller},
  journal={Artif. Intell.},
  year={2019},
  volume={267},
  pages={1-38}
}

@book{molnar2019,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}
@article{shapley,
  title={A Unified Approach to Interpreting Model Predictions},
  author={Scott Lundberg and Su-In Lee},
  journal={ArXiv},
  year={2017},
  volume={abs/1705.07874}
}
@article{gradient,
  title={Axiomatic Attribution for Deep Networks},
  author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.01365}
}
@article{wachter,
  title={Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR},
  author={Sandra Wachter and Brent D. Mittelstadt and Chris Russell},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.00399}
}
@article{friedman,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}
@inproceedings{fisherperm,
  title={All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously.},
  author={Aaron J. Fisher and Cynthia Rudin and Francesca Dominici},
  year={2019}
}
@article{breimanrandom,
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to },
  added-at = {2015-04-15T08:57:31.000+0200},
  author = {Breiman, Leo},
  biburl = {https://www.bibsonomy.org/bibtex/2b8187107bf870043f2f93669958858f1/kdepublication},
  description = {Random Forests - Springer},
  doi = {10.1023/A:1010933404324},
  interhash = {4450d2e56555e7cb8f3817578e1dd4da},
  intrahash = {b8187107bf870043f2f93669958858f1},
  issn = {0885-6125},
  journal = {Machine Learning},
  keywords = {classification classifier dblp decision ensemble final forest forests imported kde learning machine ml mykopie origin random text-detection the_youtube_social_network thema:exploiting_place_features_in_link_prediction_on_location-based_social_networks trees uw_ss14_web2.0},
  language = {English},
  number = 1,
  pages = {5-32},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2015-04-24T14:37:24.000+0200},
  title = {Random Forests},
  url = {http://dx.doi.org/10.1023/A%3A1010933404324},
  volume = 45,
  year = 2001
}

@techreport{vaver,
title	= {Measuring Ad Effectiveness Using Geo Experiments},
author	= {Jon Vaver and Jim Koehler},
year	= {2011},
URL	= {http://googleresearch.blogspot.com/2011/12/measuring-ad-effectiveness-using-geo.html},
institution	= {Google Inc.}
}


@article {Obermeyer4,
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	number = {6464},
	pages = {447--453},
	year = {2019},
	doi = {10.1126/science.aax2342},
	publisher = {American Association for the Advancement of Science},
	abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.Science, this issue p. 447; see also p. 421Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/366/6464/447},
	eprint = {https://science.sciencemag.org/content/366/6464/447.full.pdf},
	journal = {Science}
}
@article{anemiaracial,
    author = {Williams, D M},
    title = "{Racial differences of hemoglobin concentration: measurements of iron, copper, and zinc}",
    journal = {The American Journal of Clinical Nutrition},
    volume = {34},
    number = {9},
    pages = {1694-1700},
    year = {1981},
    month = {09},
    abstract = "{Nutritional surveys have demonstrated that Hb levels in blacks are consistently lower than in whites. This difference does not appear to be related to socioeconomic dietary differences, or differences of hereditary disorders such as sickle cell disease. We studied 47 black and 63 white subjects drawn from hospital employees. Mean Hb of black men was 0.9 g/dl less than that of white men. Mean Hb of black women was 0.5 g/dl less than that of white women. This difference was not associated with differences of serum iron, iron binding capacity, or transferrin saturation. Ferritin values of white women were significantly less than values observed in white men, but similar differences were not observed between black men and women. The explanation for this is unclear although may be related to sample size or differences of menstrual status. Copper levels were lower in white men than in any other group, and zinc levels were essentially similar between racial groups. The red blood cells of blacks were also smaller than those of whites. This difference could not be explained by disordered Hb synthesis. These observations confirm that Hb concentrations in both black men and women are lower than in their white counterparts. This difference cannot be explained by differences in iron, copper, or zinc nutriture. Further, measurements of Hb in nutritional survey populations reflect iron status as only one of several variables and should not be used as the only assessment of iron nutrition.}",
    issn = {0002-9165},
    doi = {10.1093/ajcn/34.9.1694},
    url = {https://doi.org/10.1093/ajcn/34.9.1694},
    eprint = {https://academic.oup.com/ajcn/article-pdf/34/9/1694/24157148/1694.pdf},
}
@inproceedings{anemiaapp,
author = {Wang, Edward and Li, William and Hawkins, Doug and Gernsheimer, Terry and Norby-Slycord, Colette and Patel, Shwetak},
year = {2016},
month = {09},
pages = {593-604},
title = {HemaApp: noninvasive blood screening of hemoglobin using smartphone cameras},
doi = {10.1145/2971648.2971653}
}
@article{hipertension,
author = {Lackland, Daniel},
year = {2014},
month = {06},
pages = {},
title = {Racial Differences in Hypertension: Implications for High Blood Pressure Management},
volume = {348},
journal = {The American journal of the medical sciences},
doi = {10.1097/MAJ.0000000000000308}
}
@article {dressel,
	author = {Dressel, Julia and Farid, Hany},
	title = {The accuracy, fairness, and limits of predicting recidivism},
	volume = {4},
	number = {1},
	elocation-id = {eaao5580},
	year = {2018},
	doi = {10.1126/sciadv.aao5580},
	publisher = {American Association for the Advancement of Science},
	abstract = {Algorithms for predicting recidivism are commonly used to assess a criminal defendant{\textquoteright}s likelihood of committing a crime. These predictions are used in pretrial, parole, and sentencing decisions. Proponents of these systems argue that big data and advanced machine learning make these analyses more accurate and less biased than humans. We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS{\textquoteright}s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features.},
	URL = {https://advances.sciencemag.org/content/4/1/eaao5580},
	eprint = {https://advances.sciencemag.org/content/4/1/eaao5580.full.pdf},
	journal = {Science Advances}
}
@book{greene,
  title={Econometric Analysis},
  author={Greene, W.H.},
  isbn={9788177586848},
  url={https://books.google.com.mx/books?id=njAcXDlR5U8C},
  year={2003},
  publisher={Pearson Education}
}
@Article{enigh, 
    author = {INEGI}, 
    year = {2014},
    title = {Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH-2014). Diseño muestral},
    url = {http://internet.contenidos.inegi.org.mx/contenidos/Productos/prod_serv/contenidos/espanol/bvinegi/productos/nueva_estruc/702825070359.pdf},
}
}

@ARTICLE{chawla,
    author = {Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer},
    title = {SMOTE: Synthetic Minority Over-sampling Technique},
    journal = {Journal of Artificial Intelligence Research},
    year = {2002},
    volume = {16},
    pages = {321--357}
}