# Uso y Monitoreo

Una vez que los métodos de aprendizaje automático se comienzan utilizar para tomar decisiones, es necesario:

- Monitorear, en general, desempeño y atributos usadas en el tiempo.
- Monitorear en particular resultados indeseables que pueden resultar de la interacción de usuarios con algoritmos.
- Tomar decisiones acerca del proceso generador de datos para mejorar desempeño o evaluar resultados.

## Degradación de desempeño


```{block2, type='rmdnote'}
**Reto: degradación de predicciones en el tiempo**

 El desempeño de un modelo puede degradarse en el tiempo debido a cambios 
en la población o proceso sobre la que se hacen predicciones.
```

Una primera medida de deseable es entender cuáles son los requisitos de actualización de datos para
la aplicación, es decir: para mantener el desempeño, ¿es suficiente actualizar cada año, mes, semana?

```{block2, type='rmdtip'}
**Medidas de mitigación: requisitos de actualización**

- Es deseable estimar desde el proceso de validación cuáles son los requisitos de actualización (tiempos)
para mantener el desempeño estimado, y planear para recolectar datos para reajustar modelos.
```

Durante el despliegue deben monitorearse medidas de error a lo largo de rondas de predicción para alertar
sobre este tipo de degradación

## Cambios de metodología y errores de implementación

Es común que al pasar de validación a despliegue del modelo ocurran errores silenciosos que posiblemente
tengan efectos grandes sobre las predicciones. Razones comunes son:

- Cambios en la metodología de recopilación de datos
- Procesamiento diferente de atributos una vez que se despliega el modelo
- Errores generales en la implementación de producción de los modelos.

```{block2, type='rmdnote'}
El desempeño de un modelo puede degradarse severamente por cambios de metodología en la recolección o procesamiento de datos, o
diferencias entre datos de entremiento/validación y datos que se usan en el despliegue del modelo.
```

Mitigamos este problema instrumentando apropiadamente el sistema para monitorear degradaciones no esperadas
en el desempeño:

```{block2, type='rmdtip'}
**Medidas de mitigación**

- Monitorear cambios en la metodología de levantamiento y procesamiento de datos que pueden reducir calidad de las predicciones.
- Monitorear varias métricas asociadas a las predicciones, en subgrupos definidos con antelación 
(incluyendo variables protegidas).
- Monitorear deriva en distribuciones de características con respecto al conjunto de entrenamiento.
```

## Errores no medidos

Es posible que algunas predicciones tengan problemas o errores particulares que pueden ser difíciles de detectar 
con métricas agregadas (por ejemplo, ciertas búsquedas dan resultados muy pobres, o cierto tipo de personas tienden
a ser descalificadas de algún programa debido a un atributo ruidoso). 

```{block2, type='rmdnote'}
Aunque un modelo tenga desempeño aceptable en las métricas seleccionadas, es posible que produzca
decisiones indeseables o errores grandes en ciertos casos
```

En este caso, 

```{block2, type='rmdtip'}
**Medidas de mitigación: errores no medidos**

- Evaluaciones de expertos de casos particulares y desempeño general puede ser útil para detectar deficiencias.

```



## Evaluación de efectividad

La forma y los datos que se recopilan para mantenimiento de los algoritmos de predicción debe planearse con el objeto de mejorar en lo posible, y entender mejor las consecuencias del uso de los algoritmos

```{block2, type='rmdnote'}
**Reto: evaluación de efectividad**

Las mejoras que esperamos en el proceso pueden ser dificíles de evaluar sin contrafactuales sólidos.

```

Pruebas con diseño experimental pueden planearse (pruebas de tipo A/B, o ver por ejemplo [@vaver]), cuando sea posible, para entender qué cambios particulares, deseables o indeseables, introduce el uso de los algoritmos. Es necesario también, al menos,
considerar efectos a mediano plazo, aunque estos pueden ser difíciles de definir por adelantado.

```{block2, type='rmdtip'}
**Medidas de mitigación: evaluación de efectividad**

- Cuando sea posible, planear asignar bajo diseños experimentales tratamientos al azar (o según el status-quo) a algunas unidades. Hacer comparaciones de
desempeño y comportamiento entre esta muestra y los resultados bajo el régimen algorítmico. 
- Al considerar estos diseños, deben considerarse no sólo mejoras a corto plazo en métricas de desempeño, sino también
comportamiento en otro tipo de medidas relacionadas con los objetivos de los tomadores de decisiones.

```

Por ejemplo, un algoritmo de recomendación que no incorpora nuevos documentos al ser entrenado puede parecer efectivo
en el corto plazo según métricas de desempeño. Sin embargo, esto es deficiente desde el punto de vista de los
tomadores de decisiones, que requieren que nuevo contenido sea mostrado cuando es relevante.
