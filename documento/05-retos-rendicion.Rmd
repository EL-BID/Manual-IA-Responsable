# Retos de rendición de cuentas


## Interpretabilidad y explicación de predicciones.

Es difícil dar una definición técnica de **interpretabilidad** o **explicabilidad**, que en general se refieren 
a hacer inteligible para humanos el funcionamiento de un algoritmo ([@molnar2019], [@miller]). Hay varias razones
por las que tener cierto grado de interpretabilidad en los algoritmos que se usan para tomar decisiones es importantes ([@molnar2019]):

1. Aprendizaje acerca del dominio del problema.
2. Aceptación social.
3. Detección de sesgos potenciales de los algoritmos.
4. Depuración y mejora de modelos.

Los puntos 1 y 2 son más difíciles de definir, y advertimos que cualquier técnica aplicada a interpretar 
los modelos con estos dos fines tienen varias dificultades que superar. El *aprendizaje automático*, como se usa
tipicamente hoy en día, dificilmente se acerca a explicaciones causales o mecanísticas. 

El punto 3 y 4, que veremos más abajo, son más suceptibles de análisis técnico. Los sesgos potenciales pueden ocurrir cuando en el proceso
de aprendizaje se aprenden de características que son irrelevantes, pero caracterizan a los conjuntos de entrenamiento
y validación/prueba que fueron utilizados.

Por otra parte, existe en muchos casos la necesidad de dar **explicaciones individuales** de cómo fueron tomadas ciertas decisiones
(por ejemplo, por qué a una persona no se le otorgó un crédito, o por qué alguien no califica para un programa social). 


## Interpretabilidad: sesgos y depuración

Sesgos potenciales pueden ocurrir cuando se usan características o variables de los datos que aunque válidos para 
un momento y conjunto de datos dado, son suceptibles de cambiar fácilmente con intervenciones en el proceso generador de datos. Ejemplos
pueden ser el uso de variables que están siendo influenciadas activamente por alguna política particular que no continuará en el futuro,
o aprendizaje de características particulares de un conjunto de entrenamiento no exhaustivo (por ejemplo, en reconocimento de 
imágenes reconocer especies animales por el contexto en el que se recolectó la información: zoológico, trampa cámara, etc).

```{block2, type='rmdnote'}
**Reto: sesgos potenciales y depuración**

- Algoritmos o métodos predictivos utilizan atributos poco relevantes, con validez temporal,  más adelante pueden 
dañar el desempeño conforme observamos desplazamiento en datos futuros.

- Algoritmos o métodos predictivos que usan una gran cantidad de atributos poco importantes tienen más riesgo de fallar
tanto explícitamente como de forma silenciosa cuando las fuentes de datos o los procesos generadores de datos cambian. 
Ambas fallas pueden ser difíciles de diagnosticar y depurar. Fallas silenciosas representar riesgos adicionales.

```


Este tipo de sesgo es difícil de detectar, pero principios de parsimonia y conocimiento experto pueden mitigar su riesgo:


```{block2, type='rmdtip'}
**Medidas: sesgos potenciales y depuración**

- (Cualitativa) Incluir todas las características disponibles para construir modelos aumenta el riesgo de
que esto suceda. Las variables a incluirse en el proceso de aprendizaje deben tener algún sustento teórico o explicación 
de por qué pueden ayudar en la tarea de predicción.
- (Cuantitativa) Métodos más parsimoniosos, que usan menos características, son preferibles a modelos que utilizan
muchas características.
- (Cuantitativa) Métodos como gráficas de dependencia parcial ([@friedman]) o  importancia basada en permutaciones ([@breimanrandom], [@molnar2019]) pueden señalar
variables problemáticas que reciben mucho peso en la predicción, en contra de observaciones pasadas o conocimiento experto.

```


#### Ejemplo {-}




## Explicabilidad de predicciones individuales

Explicaciones individuales pueden ser importantes para rendir cuentas a usuarios o poder explicar decisiones tomadas
con aprendizaje automático_

```{block2, type='rmdnote'}
**Reto: explicaciones individuales**

  En muchos casos, es necesario dar explicaciones de por qué se tomó una decisión particular. Métodos predictivos complejos
son en principio poco transparentes en cuanto a cómo se hacen las predicciones subyacentes.

```


Existen varias formas de explicar predicciones [@molnar2019]. Algunas de ellas actualmente en uso y con implementaciones
robustas son:


```{block2, type='rmdtip'}
**Medida: explicaciones individuales**
  
Para explicar las predicciones pueden utilizarse métodos como el de explicaciones contrafactuales ([@wachter]),
valores de Shapley ([@shapley]) o gradientes integrados para redes profundas ([@gradient]).
Para modelos más simples (por ejemplo, lineales), pueden construirse explicaciones ad-hoc.

```

#### Ejemplo {-}

