# Rendición de cuentas

## Interpretabilidad y explicación de predicciones.

Es difícil dar una definición técnica de **interpretabilidad** o **explicabilidad**, que en general se refieren 
a hacer inteligible para humanos el funcionamiento de un algoritmo ([@molnar2019], [@miller]). Hay varias razones
por las que tener cierto grado de interpretabilidad en los algoritmos que se usan para tomar decisiones es importante ([@molnar2019]):

1. Aprendizaje acerca del dominio del problema.
2. Aspectos legales y aceptación social (tomadores de decisiones y afectados).
3. Detección de sesgos potenciales de los algoritmos.
4. Depuración y mejora de modelos.

Los puntos 1 y 2 son más difíciles de definir, y advertimos que cualquier técnica aplicada a interpretar 
los modelos con estos dos fines tienen varias dificultades que superar. El *aprendizaje automático*, como se usa
tipicamente hoy en día, dificilmente se acerca a explicaciones causales o mecanísticas. 

Los puntos 3 y 4, que veremos más abajo, son más suceptibles de análisis técnico. Los sesgos potenciales pueden ocurrir cuando en el proceso
de aprendizaje se aprenden de características que son irrelevantes, pero caracterizan a los conjuntos de entrenamiento
y validación/prueba que fueron utilizados.

Por otra parte, existe en muchos casos la necesidad de dar **explicaciones individuales** de cómo fueron tomadas ciertas decisiones
(por ejemplo, por qué a una persona no se le otorgó un crédito, o por qué alguien no califica para un programa social). 


## Interpretabilidad: sesgos y depuración

Sesgos potenciales pueden ocurrir cuando se usan características o variables de los datos que aunque válidos para 
un momento y conjunto de datos dado, son suceptibles de cambiar fácilmente con intervenciones en el proceso generador de datos. Ejemplos
pueden ser el uso de variables que están siendo influenciadas activamente por alguna política particular que no continuará en el futuro,
o aprendizaje de características particulares de un conjunto de entrenamiento no exhaustivo (por ejemplo, en reconocimento de 
imágenes reconocer especies animales por el contexto en el que se recolectó la información: zoológico, trampa cámara, etc).

```{block2, type='rmdnote'}
**Reto: sesgos potenciales y depuración**

- Algoritmos o métodos predictivos utilizan atributos poco relevantes, con validez temporal,  más adelante pueden 
dañar el desempeño conforme observamos desplazamiento en datos futuros.

- Algoritmos o métodos predictivos que usan una gran cantidad de atributos poco importantes tienen más riesgo de fallar
tanto explícitamente como de forma silenciosa cuando las fuentes de datos o los procesos generadores de datos cambian. 
Ambas fallas pueden ser difíciles de diagnosticar y depurar. Fallas silenciosas representar riesgos adicionales.

```


Este tipo de sesgo es difícil de detectar, pero principios de parsimonia y conocimiento experto pueden mitigar su riesgo:


```{block2, type='rmdtip'}
**Medidas de mitigación: sesgos potenciales y depuración**

- (Cualitativa) Incluir todas las características disponibles para construir modelos aumenta el riesgo de
que esto suceda. Las variables a incluirse en el proceso de aprendizaje deben tener algún sustento teórico o explicación 
de por qué pueden ayudar en la tarea de predicción.
- (Cuantitativa) Métodos más parsimoniosos, que usan menos características, son preferibles a modelos que utilizan
muchas características.
- (Cuantitativa) Métodos como gráficas de dependencia parcial ([@friedman]) o  importancia basada en permutaciones ([@breimanrandom], [@molnar2019]) pueden señalar
variables problemáticas que reciben mucho peso en la predicción, en contra de observaciones pasadas o conocimiento experto.

```



## Explicabilidad de predicciones individuales

Explicaciones individuales pueden ser importantes para rendir cuentas a usuarios o poder explicar decisiones tomadas
con aprendizaje automático:

```{block2, type='rmdnote'}
**Reto: explicaciones individuales**

  En muchos casos, es necesario dar explicaciones de por qué se tomó una decisión particular. Métodos predictivos complejos
son en principio poco transparentes en cuanto a cómo se hacen las predicciones subyacentes.

```


Existen varias formas de explicar predicciones [@molnar2019]. Algunas de ellas actualmente en uso y con implementaciones
robustas son:


```{block2, type='rmdtip'}
**Medidas de mitigación: explicaciones individuales**
  
Para explicar las predicciones pueden utilizarse métodos como el de explicaciones contrafactuales ([@wachter]),
valores de Shapley ([@shapley]) o gradientes integrados para redes profundas ([@gradient]).
Para modelos más simples (por ejemplo, lineales), pueden construirse explicaciones ad-hoc.

```




## Transparencia y auditabilidad

En algunos casos, será necesario transparentar el funcionamiento de tanto datos usados, modelos predictivos particulares,
objetivos de las políticas y la aplicación de las predicciones para alcanzar los objetivos de las políticas. Esto varía
de aplicación a aplicación: por ejemplo, en el caso de acceso a un programa social, puede ser necesario que las 
decisiones basadas en datos sean explicadas y documentadas en su totalidad, o al menos explicar cuáles son los
datos que se están usando para tomar la decisiones acerca del acceso. En otros casos, será necesario explicar
cómo fueron recogidos los datos, pero no necesariamente tienen que compartirse los datos o todos los detalles de procesamiento
por razones de privacidad.

```{block2, type='rmdnote'}
**Reto: decisiones que no son transparentes**

Dependiendo de la aplicación, distintas partes de la construcción y utilización pueden no estar explicadas
y justificadas correctamente, de frente a usuarios, tomadores de decisiones, o población objetivo. Esto produce
poca confianza en cuanto al funcionamiento de sistemas basados en datos.

```



```{block2, type='rmdtip'}
**Medidas de mitigación: transparencia**

Los requisitos de transparencia deben ser definidos claramente y cumplidos con la documentación y datos correspondientes, pero
pueden variar según el tipo de proyecto:

- De ser necesario, los datos deben ser publicados en formatos abiertos, o debe explicarse cómo fueron recolectados
y procesados.
- De ser necesario, el código debe estar documentado y versionado. 
- De ser necesario, deben proveerse herramientas o instrucciones para reproducir resultados con los que se tomaron decisiones.

```



